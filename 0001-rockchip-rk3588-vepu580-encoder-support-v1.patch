diff -urN a/drivers/iommu/iommu.c bb/drivers/iommu/iommu.c
--- a/drivers/iommu/iommu.c	1969-12-31 16:00:00
+++ bb/drivers/iommu/iommu.c	2026-02-08 03:48:58
@@ -0,0 +1,3884 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2007-2008 Advanced Micro Devices, Inc.
+ * Author: Joerg Roedel <jroedel@suse.de>
+ */
+
+#define pr_fmt(fmt)    "iommu: " fmt
+
+#include <linux/amba/bus.h>
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/bits.h>
+#include <linux/bug.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/export.h>
+#include <linux/slab.h>
+#include <linux/errno.h>
+#include <linux/host1x_context_bus.h>
+#include <linux/iommu.h>
+#include <linux/iommufd.h>
+#include <linux/idr.h>
+#include <linux/err.h>
+#include <linux/pci.h>
+#include <linux/pci-ats.h>
+#include <linux/bitops.h>
+#include <linux/platform_device.h>
+#include <linux/property.h>
+#include <linux/fsl/mc.h>
+#include <linux/module.h>
+#include <linux/cc_platform.h>
+#include <linux/cdx/cdx_bus.h>
+#include <trace/events/iommu.h>
+#include <linux/sched/mm.h>
+#include <linux/msi.h>
+#include <uapi/linux/iommufd.h>
+
+#include "dma-iommu.h"
+#include "iommu-priv.h"
+
+static struct kset *iommu_group_kset;
+static DEFINE_IDA(iommu_group_ida);
+static DEFINE_IDA(iommu_global_pasid_ida);
+
+static unsigned int iommu_def_domain_type __read_mostly;
+static bool iommu_dma_strict __read_mostly = IS_ENABLED(CONFIG_IOMMU_DEFAULT_DMA_STRICT);
+static u32 iommu_cmd_line __read_mostly;
+
+/* Tags used with xa_tag_pointer() in group->pasid_array */
+enum { IOMMU_PASID_ARRAY_DOMAIN = 0, IOMMU_PASID_ARRAY_HANDLE = 1 };
+
+struct iommu_group {
+	struct kobject kobj;
+	struct kobject *devices_kobj;
+	struct list_head devices;
+	struct xarray pasid_array;
+	struct mutex mutex;
+	void *iommu_data;
+	void (*iommu_data_release)(void *iommu_data);
+	char *name;
+	int id;
+	struct iommu_domain *default_domain;
+	struct iommu_domain *blocking_domain;
+	struct iommu_domain *domain;
+	struct list_head entry;
+	unsigned int owner_cnt;
+	void *owner;
+};
+
+struct group_device {
+	struct list_head list;
+	struct device *dev;
+	char *name;
+};
+
+/* Iterate over each struct group_device in a struct iommu_group */
+#define for_each_group_device(group, pos) \
+	list_for_each_entry(pos, &(group)->devices, list)
+
+struct iommu_group_attribute {
+	struct attribute attr;
+	ssize_t (*show)(struct iommu_group *group, char *buf);
+	ssize_t (*store)(struct iommu_group *group,
+			 const char *buf, size_t count);
+};
+
+static const char * const iommu_group_resv_type_string[] = {
+	[IOMMU_RESV_DIRECT]			= "direct",
+	[IOMMU_RESV_DIRECT_RELAXABLE]		= "direct-relaxable",
+	[IOMMU_RESV_RESERVED]			= "reserved",
+	[IOMMU_RESV_MSI]			= "msi",
+	[IOMMU_RESV_SW_MSI]			= "msi",
+};
+
+#define IOMMU_CMD_LINE_DMA_API		BIT(0)
+#define IOMMU_CMD_LINE_STRICT		BIT(1)
+
+static int bus_iommu_probe(const struct bus_type *bus);
+static int iommu_bus_notifier(struct notifier_block *nb,
+			      unsigned long action, void *data);
+static void iommu_release_device(struct device *dev);
+static int __iommu_attach_device(struct iommu_domain *domain,
+				 struct device *dev, struct iommu_domain *old);
+static int __iommu_attach_group(struct iommu_domain *domain,
+				struct iommu_group *group);
+static struct iommu_domain *__iommu_paging_domain_alloc_flags(struct device *dev,
+						       unsigned int type,
+						       unsigned int flags);
+
+enum {
+	IOMMU_SET_DOMAIN_MUST_SUCCEED = 1 << 0,
+};
+
+static int __iommu_device_set_domain(struct iommu_group *group,
+				     struct device *dev,
+				     struct iommu_domain *new_domain,
+				     struct iommu_domain *old_domain,
+				     unsigned int flags);
+static int __iommu_group_set_domain_internal(struct iommu_group *group,
+					     struct iommu_domain *new_domain,
+					     unsigned int flags);
+static int __iommu_group_set_domain(struct iommu_group *group,
+				    struct iommu_domain *new_domain)
+{
+	return __iommu_group_set_domain_internal(group, new_domain, 0);
+}
+static void __iommu_group_set_domain_nofail(struct iommu_group *group,
+					    struct iommu_domain *new_domain)
+{
+	WARN_ON(__iommu_group_set_domain_internal(
+		group, new_domain, IOMMU_SET_DOMAIN_MUST_SUCCEED));
+}
+
+static int iommu_setup_default_domain(struct iommu_group *group,
+				      int target_type);
+static int iommu_create_device_direct_mappings(struct iommu_domain *domain,
+					       struct device *dev);
+static ssize_t iommu_group_store_type(struct iommu_group *group,
+				      const char *buf, size_t count);
+static struct group_device *iommu_group_alloc_device(struct iommu_group *group,
+						     struct device *dev);
+static void __iommu_group_free_device(struct iommu_group *group,
+				      struct group_device *grp_dev);
+static void iommu_domain_init(struct iommu_domain *domain, unsigned int type,
+			      const struct iommu_ops *ops);
+
+#define IOMMU_GROUP_ATTR(_name, _mode, _show, _store)		\
+struct iommu_group_attribute iommu_group_attr_##_name =		\
+	__ATTR(_name, _mode, _show, _store)
+
+#define to_iommu_group_attr(_attr)	\
+	container_of(_attr, struct iommu_group_attribute, attr)
+#define to_iommu_group(_kobj)		\
+	container_of(_kobj, struct iommu_group, kobj)
+
+static LIST_HEAD(iommu_device_list);
+static DEFINE_SPINLOCK(iommu_device_lock);
+
+static const struct bus_type * const iommu_buses[] = {
+	&platform_bus_type,
+#ifdef CONFIG_PCI
+	&pci_bus_type,
+#endif
+#ifdef CONFIG_ARM_AMBA
+	&amba_bustype,
+#endif
+#ifdef CONFIG_FSL_MC_BUS
+	&fsl_mc_bus_type,
+#endif
+#ifdef CONFIG_TEGRA_HOST1X_CONTEXT_BUS
+	&host1x_context_device_bus_type,
+#endif
+#ifdef CONFIG_CDX_BUS
+	&cdx_bus_type,
+#endif
+};
+
+/*
+ * Use a function instead of an array here because the domain-type is a
+ * bit-field, so an array would waste memory.
+ */
+static const char *iommu_domain_type_str(unsigned int t)
+{
+	switch (t) {
+	case IOMMU_DOMAIN_BLOCKED:
+		return "Blocked";
+	case IOMMU_DOMAIN_IDENTITY:
+		return "Passthrough";
+	case IOMMU_DOMAIN_UNMANAGED:
+		return "Unmanaged";
+	case IOMMU_DOMAIN_DMA:
+	case IOMMU_DOMAIN_DMA_FQ:
+		return "Translated";
+	case IOMMU_DOMAIN_PLATFORM:
+		return "Platform";
+	default:
+		return "Unknown";
+	}
+}
+
+static int __init iommu_subsys_init(void)
+{
+	struct notifier_block *nb;
+
+	if (!(iommu_cmd_line & IOMMU_CMD_LINE_DMA_API)) {
+		if (IS_ENABLED(CONFIG_IOMMU_DEFAULT_PASSTHROUGH))
+			iommu_set_default_passthrough(false);
+		else
+			iommu_set_default_translated(false);
+
+		if (iommu_default_passthrough() && cc_platform_has(CC_ATTR_MEM_ENCRYPT)) {
+			pr_info("Memory encryption detected - Disabling default IOMMU Passthrough\n");
+			iommu_set_default_translated(false);
+		}
+	}
+
+	if (!iommu_default_passthrough() && !iommu_dma_strict)
+		iommu_def_domain_type = IOMMU_DOMAIN_DMA_FQ;
+
+	pr_info("Default domain type: %s%s\n",
+		iommu_domain_type_str(iommu_def_domain_type),
+		(iommu_cmd_line & IOMMU_CMD_LINE_DMA_API) ?
+			" (set via kernel command line)" : "");
+
+	if (!iommu_default_passthrough())
+		pr_info("DMA domain TLB invalidation policy: %s mode%s\n",
+			iommu_dma_strict ? "strict" : "lazy",
+			(iommu_cmd_line & IOMMU_CMD_LINE_STRICT) ?
+				" (set via kernel command line)" : "");
+
+	nb = kcalloc(ARRAY_SIZE(iommu_buses), sizeof(*nb), GFP_KERNEL);
+	if (!nb)
+		return -ENOMEM;
+
+	for (int i = 0; i < ARRAY_SIZE(iommu_buses); i++) {
+		nb[i].notifier_call = iommu_bus_notifier;
+		bus_register_notifier(iommu_buses[i], &nb[i]);
+	}
+
+	return 0;
+}
+subsys_initcall(iommu_subsys_init);
+
+static int remove_iommu_group(struct device *dev, void *data)
+{
+	if (dev->iommu && dev->iommu->iommu_dev == data)
+		iommu_release_device(dev);
+
+	return 0;
+}
+
+/**
+ * iommu_device_register() - Register an IOMMU hardware instance
+ * @iommu: IOMMU handle for the instance
+ * @ops:   IOMMU ops to associate with the instance
+ * @hwdev: (optional) actual instance device, used for fwnode lookup
+ *
+ * Return: 0 on success, or an error.
+ */
+int iommu_device_register(struct iommu_device *iommu,
+			  const struct iommu_ops *ops, struct device *hwdev)
+{
+	int err = 0;
+
+	/* We need to be able to take module references appropriately */
+	if (WARN_ON(is_module_address((unsigned long)ops) && !ops->owner))
+		return -EINVAL;
+
+	iommu->ops = ops;
+	if (hwdev)
+		iommu->fwnode = dev_fwnode(hwdev);
+
+	spin_lock(&iommu_device_lock);
+	list_add_tail(&iommu->list, &iommu_device_list);
+	spin_unlock(&iommu_device_lock);
+
+	for (int i = 0; i < ARRAY_SIZE(iommu_buses) && !err; i++)
+		err = bus_iommu_probe(iommu_buses[i]);
+	if (err)
+		iommu_device_unregister(iommu);
+	else
+		WRITE_ONCE(iommu->ready, true);
+	return err;
+}
+EXPORT_SYMBOL_GPL(iommu_device_register);
+
+void iommu_device_unregister(struct iommu_device *iommu)
+{
+	for (int i = 0; i < ARRAY_SIZE(iommu_buses); i++)
+		bus_for_each_dev(iommu_buses[i], NULL, iommu, remove_iommu_group);
+
+	spin_lock(&iommu_device_lock);
+	list_del(&iommu->list);
+	spin_unlock(&iommu_device_lock);
+
+	/* Pairs with the alloc in generic_single_device_group() */
+	iommu_group_put(iommu->singleton_group);
+	iommu->singleton_group = NULL;
+}
+EXPORT_SYMBOL_GPL(iommu_device_unregister);
+
+#if IS_ENABLED(CONFIG_IOMMUFD_TEST)
+void iommu_device_unregister_bus(struct iommu_device *iommu,
+				 const struct bus_type *bus,
+				 struct notifier_block *nb)
+{
+	bus_unregister_notifier(bus, nb);
+	fwnode_remove_software_node(iommu->fwnode);
+	iommu_device_unregister(iommu);
+}
+EXPORT_SYMBOL_GPL(iommu_device_unregister_bus);
+
+/*
+ * Register an iommu driver against a single bus. This is only used by iommufd
+ * selftest to create a mock iommu driver. The caller must provide
+ * some memory to hold a notifier_block.
+ */
+int iommu_device_register_bus(struct iommu_device *iommu,
+			      const struct iommu_ops *ops,
+			      const struct bus_type *bus,
+			      struct notifier_block *nb)
+{
+	int err;
+
+	iommu->ops = ops;
+	nb->notifier_call = iommu_bus_notifier;
+	err = bus_register_notifier(bus, nb);
+	if (err)
+		return err;
+
+	iommu->fwnode = fwnode_create_software_node(NULL, NULL);
+	if (IS_ERR(iommu->fwnode)) {
+		bus_unregister_notifier(bus, nb);
+		return PTR_ERR(iommu->fwnode);
+	}
+
+	spin_lock(&iommu_device_lock);
+	list_add_tail(&iommu->list, &iommu_device_list);
+	spin_unlock(&iommu_device_lock);
+
+	err = bus_iommu_probe(bus);
+	if (err) {
+		iommu_device_unregister_bus(iommu, bus, nb);
+		return err;
+	}
+	WRITE_ONCE(iommu->ready, true);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(iommu_device_register_bus);
+
+int iommu_mock_device_add(struct device *dev, struct iommu_device *iommu)
+{
+	int rc;
+
+	mutex_lock(&iommu_probe_device_lock);
+	rc = iommu_fwspec_init(dev, iommu->fwnode);
+	mutex_unlock(&iommu_probe_device_lock);
+
+	if (rc)
+		return rc;
+
+	rc = device_add(dev);
+	if (rc)
+		iommu_fwspec_free(dev);
+	return rc;
+}
+EXPORT_SYMBOL_GPL(iommu_mock_device_add);
+#endif
+
+static struct dev_iommu *dev_iommu_get(struct device *dev)
+{
+	struct dev_iommu *param = dev->iommu;
+
+	lockdep_assert_held(&iommu_probe_device_lock);
+
+	if (param)
+		return param;
+
+	param = kzalloc(sizeof(*param), GFP_KERNEL);
+	if (!param)
+		return NULL;
+
+	mutex_init(&param->lock);
+	dev->iommu = param;
+	return param;
+}
+
+void dev_iommu_free(struct device *dev)
+{
+	struct dev_iommu *param = dev->iommu;
+
+	dev->iommu = NULL;
+	if (param->fwspec) {
+		fwnode_handle_put(param->fwspec->iommu_fwnode);
+		kfree(param->fwspec);
+	}
+	kfree(param);
+}
+
+/*
+ * Internal equivalent of device_iommu_mapped() for when we care that a device
+ * actually has API ops, and don't want false positives from VFIO-only groups.
+ */
+static bool dev_has_iommu(struct device *dev)
+{
+	return dev->iommu && dev->iommu->iommu_dev;
+}
+
+static u32 dev_iommu_get_max_pasids(struct device *dev)
+{
+	u32 max_pasids = 0, bits = 0;
+	int ret;
+
+	if (dev_is_pci(dev)) {
+		ret = pci_max_pasids(to_pci_dev(dev));
+		if (ret > 0)
+			max_pasids = ret;
+	} else {
+		ret = device_property_read_u32(dev, "pasid-num-bits", &bits);
+		if (!ret)
+			max_pasids = 1UL << bits;
+	}
+
+	return min_t(u32, max_pasids, dev->iommu->iommu_dev->max_pasids);
+}
+
+void dev_iommu_priv_set(struct device *dev, void *priv)
+{
+	/* FSL_PAMU does something weird */
+	if (!IS_ENABLED(CONFIG_FSL_PAMU))
+		lockdep_assert_held(&iommu_probe_device_lock);
+	dev->iommu->priv = priv;
+}
+EXPORT_SYMBOL_GPL(dev_iommu_priv_set);
+
+/*
+ * Init the dev->iommu and dev->iommu_group in the struct device and get the
+ * driver probed
+ */
+static int iommu_init_device(struct device *dev)
+{
+	const struct iommu_ops *ops;
+	struct iommu_device *iommu_dev;
+	struct iommu_group *group;
+	int ret;
+
+	if (!dev_iommu_get(dev))
+		return -ENOMEM;
+	/*
+	 * For FDT-based systems and ACPI IORT/VIOT, the common firmware parsing
+	 * is buried in the bus dma_configure path. Properly unpicking that is
+	 * still a big job, so for now just invoke the whole thing. The device
+	 * already having a driver bound means dma_configure has already run and
+	 * found no IOMMU to wait for, so there's no point calling it again.
+	 */
+	if (!dev->iommu->fwspec && !dev->driver && dev->bus->dma_configure) {
+		mutex_unlock(&iommu_probe_device_lock);
+		dev->bus->dma_configure(dev);
+		mutex_lock(&iommu_probe_device_lock);
+		/* If another instance finished the job for us, skip it */
+		if (!dev->iommu || dev->iommu_group)
+			return -ENODEV;
+	}
+	/*
+	 * At this point, relevant devices either now have a fwspec which will
+	 * match ops registered with a non-NULL fwnode, or we can reasonably
+	 * assume that only one of Intel, AMD, s390, PAMU or legacy SMMUv2 can
+	 * be present, and that any of their registered instances has suitable
+	 * ops for probing, and thus cheekily co-opt the same mechanism.
+	 */
+	ops = iommu_fwspec_ops(dev->iommu->fwspec);
+	if (!ops) {
+		ret = -ENODEV;
+		goto err_free;
+	}
+
+	if (!try_module_get(ops->owner)) {
+		ret = -EINVAL;
+		goto err_free;
+	}
+
+	iommu_dev = ops->probe_device(dev);
+	if (IS_ERR(iommu_dev)) {
+		ret = PTR_ERR(iommu_dev);
+		goto err_module_put;
+	}
+	dev->iommu->iommu_dev = iommu_dev;
+
+	ret = iommu_device_link(iommu_dev, dev);
+	if (ret)
+		goto err_release;
+
+	group = ops->device_group(dev);
+	if (WARN_ON_ONCE(group == NULL))
+		group = ERR_PTR(-EINVAL);
+	if (IS_ERR(group)) {
+		ret = PTR_ERR(group);
+		goto err_unlink;
+	}
+	dev->iommu_group = group;
+
+	dev->iommu->max_pasids = dev_iommu_get_max_pasids(dev);
+	if (ops->is_attach_deferred)
+		dev->iommu->attach_deferred = ops->is_attach_deferred(dev);
+	return 0;
+
+err_unlink:
+	iommu_device_unlink(iommu_dev, dev);
+err_release:
+	if (ops->release_device)
+		ops->release_device(dev);
+err_module_put:
+	module_put(ops->owner);
+err_free:
+	dev->iommu->iommu_dev = NULL;
+	dev_iommu_free(dev);
+	return ret;
+}
+
+static void iommu_deinit_device(struct device *dev)
+{
+	struct iommu_group *group = dev->iommu_group;
+	const struct iommu_ops *ops = dev_iommu_ops(dev);
+
+	lockdep_assert_held(&group->mutex);
+
+	iommu_device_unlink(dev->iommu->iommu_dev, dev);
+
+	/*
+	 * release_device() must stop using any attached domain on the device.
+	 * If there are still other devices in the group, they are not affected
+	 * by this callback.
+	 *
+	 * If the iommu driver provides release_domain, the core code ensures
+	 * that domain is attached prior to calling release_device. Drivers can
+	 * use this to enforce a translation on the idle iommu. Typically, the
+	 * global static blocked_domain is a good choice.
+	 *
+	 * Otherwise, the iommu driver must set the device to either an identity
+	 * or a blocking translation in release_device() and stop using any
+	 * domain pointer, as it is going to be freed.
+	 *
+	 * Regardless, if a delayed attach never occurred, then the release
+	 * should still avoid touching any hardware configuration either.
+	 */
+	if (!dev->iommu->attach_deferred && ops->release_domain) {
+		struct iommu_domain *release_domain = ops->release_domain;
+
+		/*
+		 * If the device requires direct mappings then it should not
+		 * be parked on a BLOCKED domain during release as that would
+		 * break the direct mappings.
+		 */
+		if (dev->iommu->require_direct && ops->identity_domain &&
+		    release_domain == ops->blocked_domain)
+			release_domain = ops->identity_domain;
+
+		release_domain->ops->attach_dev(release_domain, dev,
+						group->domain);
+	}
+
+	if (ops->release_device)
+		ops->release_device(dev);
+
+	/*
+	 * If this is the last driver to use the group then we must free the
+	 * domains before we do the module_put().
+	 */
+	if (list_empty(&group->devices)) {
+		if (group->default_domain) {
+			iommu_domain_free(group->default_domain);
+			group->default_domain = NULL;
+		}
+		if (group->blocking_domain) {
+			iommu_domain_free(group->blocking_domain);
+			group->blocking_domain = NULL;
+		}
+		group->domain = NULL;
+	}
+
+	/* Caller must put iommu_group */
+	dev->iommu_group = NULL;
+	module_put(ops->owner);
+	dev_iommu_free(dev);
+#ifdef CONFIG_IOMMU_DMA
+	dev->dma_iommu = false;
+#endif
+}
+
+static struct iommu_domain *pasid_array_entry_to_domain(void *entry)
+{
+	if (xa_pointer_tag(entry) == IOMMU_PASID_ARRAY_DOMAIN)
+		return xa_untag_pointer(entry);
+	return ((struct iommu_attach_handle *)xa_untag_pointer(entry))->domain;
+}
+
+DEFINE_MUTEX(iommu_probe_device_lock);
+
+static int __iommu_probe_device(struct device *dev, struct list_head *group_list)
+{
+	struct iommu_group *group;
+	struct group_device *gdev;
+	int ret;
+
+	/*
+	 * Serialise to avoid races between IOMMU drivers registering in
+	 * parallel and/or the "replay" calls from ACPI/OF code via client
+	 * driver probe. Once the latter have been cleaned up we should
+	 * probably be able to use device_lock() here to minimise the scope,
+	 * but for now enforcing a simple global ordering is fine.
+	 */
+	lockdep_assert_held(&iommu_probe_device_lock);
+
+	/* Device is probed already if in a group */
+	if (dev->iommu_group)
+		return 0;
+
+	ret = iommu_init_device(dev);
+	if (ret)
+		return ret;
+	/*
+	 * And if we do now see any replay calls, they would indicate someone
+	 * misusing the dma_configure path outside bus code.
+	 */
+	if (dev->driver)
+		dev_WARN(dev, "late IOMMU probe at driver bind, something fishy here!\n");
+
+	group = dev->iommu_group;
+	gdev = iommu_group_alloc_device(group, dev);
+	mutex_lock(&group->mutex);
+	if (IS_ERR(gdev)) {
+		ret = PTR_ERR(gdev);
+		goto err_put_group;
+	}
+
+	/*
+	 * The gdev must be in the list before calling
+	 * iommu_setup_default_domain()
+	 */
+	list_add_tail(&gdev->list, &group->devices);
+	WARN_ON(group->default_domain && !group->domain);
+	if (group->default_domain)
+		iommu_create_device_direct_mappings(group->default_domain, dev);
+	if (group->domain) {
+		ret = __iommu_device_set_domain(group, dev, group->domain, NULL,
+						0);
+		if (ret)
+			goto err_remove_gdev;
+	} else if (!group->default_domain && !group_list) {
+		ret = iommu_setup_default_domain(group, 0);
+		if (ret)
+			goto err_remove_gdev;
+	} else if (!group->default_domain) {
+		/*
+		 * With a group_list argument we defer the default_domain setup
+		 * to the caller by providing a de-duplicated list of groups
+		 * that need further setup.
+		 */
+		if (list_empty(&group->entry))
+			list_add_tail(&group->entry, group_list);
+	}
+
+	if (group->default_domain)
+		iommu_setup_dma_ops(dev);
+
+	mutex_unlock(&group->mutex);
+
+	return 0;
+
+err_remove_gdev:
+	list_del(&gdev->list);
+	__iommu_group_free_device(group, gdev);
+err_put_group:
+	iommu_deinit_device(dev);
+	mutex_unlock(&group->mutex);
+	iommu_group_put(group);
+
+	return ret;
+}
+
+int iommu_probe_device(struct device *dev)
+{
+	const struct iommu_ops *ops;
+	int ret;
+
+	mutex_lock(&iommu_probe_device_lock);
+	ret = __iommu_probe_device(dev, NULL);
+	mutex_unlock(&iommu_probe_device_lock);
+	if (ret)
+		return ret;
+
+	ops = dev_iommu_ops(dev);
+	if (ops->probe_finalize)
+		ops->probe_finalize(dev);
+
+	return 0;
+}
+
+static void __iommu_group_free_device(struct iommu_group *group,
+				      struct group_device *grp_dev)
+{
+	struct device *dev = grp_dev->dev;
+
+	sysfs_remove_link(group->devices_kobj, grp_dev->name);
+	sysfs_remove_link(&dev->kobj, "iommu_group");
+
+	trace_remove_device_from_group(group->id, dev);
+
+	/*
+	 * If the group has become empty then ownership must have been
+	 * released, and the current domain must be set back to NULL or
+	 * the default domain.
+	 */
+	if (list_empty(&group->devices))
+		WARN_ON(group->owner_cnt ||
+			group->domain != group->default_domain);
+
+	kfree(grp_dev->name);
+	kfree(grp_dev);
+}
+
+/* Remove the iommu_group from the struct device. */
+static void __iommu_group_remove_device(struct device *dev)
+{
+	struct iommu_group *group = dev->iommu_group;
+	struct group_device *device;
+
+	mutex_lock(&group->mutex);
+	for_each_group_device(group, device) {
+		if (device->dev != dev)
+			continue;
+
+		list_del(&device->list);
+		__iommu_group_free_device(group, device);
+		if (dev_has_iommu(dev))
+			iommu_deinit_device(dev);
+		else
+			dev->iommu_group = NULL;
+		break;
+	}
+	mutex_unlock(&group->mutex);
+
+	/*
+	 * Pairs with the get in iommu_init_device() or
+	 * iommu_group_add_device()
+	 */
+	iommu_group_put(group);
+}
+
+static void iommu_release_device(struct device *dev)
+{
+	struct iommu_group *group = dev->iommu_group;
+
+	if (group)
+		__iommu_group_remove_device(dev);
+
+	/* Free any fwspec if no iommu_driver was ever attached */
+	if (dev->iommu)
+		dev_iommu_free(dev);
+}
+
+static int __init iommu_set_def_domain_type(char *str)
+{
+	bool pt;
+	int ret;
+
+	ret = kstrtobool(str, &pt);
+	if (ret)
+		return ret;
+
+	if (pt)
+		iommu_set_default_passthrough(true);
+	else
+		iommu_set_default_translated(true);
+
+	return 0;
+}
+early_param("iommu.passthrough", iommu_set_def_domain_type);
+
+static int __init iommu_dma_setup(char *str)
+{
+	int ret = kstrtobool(str, &iommu_dma_strict);
+
+	if (!ret)
+		iommu_cmd_line |= IOMMU_CMD_LINE_STRICT;
+	return ret;
+}
+early_param("iommu.strict", iommu_dma_setup);
+
+void iommu_set_dma_strict(void)
+{
+	iommu_dma_strict = true;
+	if (iommu_def_domain_type == IOMMU_DOMAIN_DMA_FQ)
+		iommu_def_domain_type = IOMMU_DOMAIN_DMA;
+}
+
+static ssize_t iommu_group_attr_show(struct kobject *kobj,
+				     struct attribute *__attr, char *buf)
+{
+	struct iommu_group_attribute *attr = to_iommu_group_attr(__attr);
+	struct iommu_group *group = to_iommu_group(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->show)
+		ret = attr->show(group, buf);
+	return ret;
+}
+
+static ssize_t iommu_group_attr_store(struct kobject *kobj,
+				      struct attribute *__attr,
+				      const char *buf, size_t count)
+{
+	struct iommu_group_attribute *attr = to_iommu_group_attr(__attr);
+	struct iommu_group *group = to_iommu_group(kobj);
+	ssize_t ret = -EIO;
+
+	if (attr->store)
+		ret = attr->store(group, buf, count);
+	return ret;
+}
+
+static const struct sysfs_ops iommu_group_sysfs_ops = {
+	.show = iommu_group_attr_show,
+	.store = iommu_group_attr_store,
+};
+
+static int iommu_group_create_file(struct iommu_group *group,
+				   struct iommu_group_attribute *attr)
+{
+	return sysfs_create_file(&group->kobj, &attr->attr);
+}
+
+static void iommu_group_remove_file(struct iommu_group *group,
+				    struct iommu_group_attribute *attr)
+{
+	sysfs_remove_file(&group->kobj, &attr->attr);
+}
+
+static ssize_t iommu_group_show_name(struct iommu_group *group, char *buf)
+{
+	return sysfs_emit(buf, "%s\n", group->name);
+}
+
+/**
+ * iommu_insert_resv_region - Insert a new region in the
+ * list of reserved regions.
+ * @new: new region to insert
+ * @regions: list of regions
+ *
+ * Elements are sorted by start address and overlapping segments
+ * of the same type are merged.
+ */
+static int iommu_insert_resv_region(struct iommu_resv_region *new,
+				    struct list_head *regions)
+{
+	struct iommu_resv_region *iter, *tmp, *nr, *top;
+	LIST_HEAD(stack);
+
+	nr = iommu_alloc_resv_region(new->start, new->length,
+				     new->prot, new->type, GFP_KERNEL);
+	if (!nr)
+		return -ENOMEM;
+
+	/* First add the new element based on start address sorting */
+	list_for_each_entry(iter, regions, list) {
+		if (nr->start < iter->start ||
+		    (nr->start == iter->start && nr->type <= iter->type))
+			break;
+	}
+	list_add_tail(&nr->list, &iter->list);
+
+	/* Merge overlapping segments of type nr->type in @regions, if any */
+	list_for_each_entry_safe(iter, tmp, regions, list) {
+		phys_addr_t top_end, iter_end = iter->start + iter->length - 1;
+
+		/* no merge needed on elements of different types than @new */
+		if (iter->type != new->type) {
+			list_move_tail(&iter->list, &stack);
+			continue;
+		}
+
+		/* look for the last stack element of same type as @iter */
+		list_for_each_entry_reverse(top, &stack, list)
+			if (top->type == iter->type)
+				goto check_overlap;
+
+		list_move_tail(&iter->list, &stack);
+		continue;
+
+check_overlap:
+		top_end = top->start + top->length - 1;
+
+		if (iter->start > top_end + 1) {
+			list_move_tail(&iter->list, &stack);
+		} else {
+			top->length = max(top_end, iter_end) - top->start + 1;
+			list_del(&iter->list);
+			kfree(iter);
+		}
+	}
+	list_splice(&stack, regions);
+	return 0;
+}
+
+static int
+iommu_insert_device_resv_regions(struct list_head *dev_resv_regions,
+				 struct list_head *group_resv_regions)
+{
+	struct iommu_resv_region *entry;
+	int ret = 0;
+
+	list_for_each_entry(entry, dev_resv_regions, list) {
+		ret = iommu_insert_resv_region(entry, group_resv_regions);
+		if (ret)
+			break;
+	}
+	return ret;
+}
+
+int iommu_get_group_resv_regions(struct iommu_group *group,
+				 struct list_head *head)
+{
+	struct group_device *device;
+	int ret = 0;
+
+	mutex_lock(&group->mutex);
+	for_each_group_device(group, device) {
+		struct list_head dev_resv_regions;
+
+		/*
+		 * Non-API groups still expose reserved_regions in sysfs,
+		 * so filter out calls that get here that way.
+		 */
+		if (!dev_has_iommu(device->dev))
+			break;
+
+		INIT_LIST_HEAD(&dev_resv_regions);
+		iommu_get_resv_regions(device->dev, &dev_resv_regions);
+		ret = iommu_insert_device_resv_regions(&dev_resv_regions, head);
+		iommu_put_resv_regions(device->dev, &dev_resv_regions);
+		if (ret)
+			break;
+	}
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_get_group_resv_regions);
+
+static ssize_t iommu_group_show_resv_regions(struct iommu_group *group,
+					     char *buf)
+{
+	struct iommu_resv_region *region, *next;
+	struct list_head group_resv_regions;
+	int offset = 0;
+
+	INIT_LIST_HEAD(&group_resv_regions);
+	iommu_get_group_resv_regions(group, &group_resv_regions);
+
+	list_for_each_entry_safe(region, next, &group_resv_regions, list) {
+		offset += sysfs_emit_at(buf, offset, "0x%016llx 0x%016llx %s\n",
+					(long long)region->start,
+					(long long)(region->start +
+						    region->length - 1),
+					iommu_group_resv_type_string[region->type]);
+		kfree(region);
+	}
+
+	return offset;
+}
+
+static ssize_t iommu_group_show_type(struct iommu_group *group,
+				     char *buf)
+{
+	char *type = "unknown";
+
+	mutex_lock(&group->mutex);
+	if (group->default_domain) {
+		switch (group->default_domain->type) {
+		case IOMMU_DOMAIN_BLOCKED:
+			type = "blocked";
+			break;
+		case IOMMU_DOMAIN_IDENTITY:
+			type = "identity";
+			break;
+		case IOMMU_DOMAIN_UNMANAGED:
+			type = "unmanaged";
+			break;
+		case IOMMU_DOMAIN_DMA:
+			type = "DMA";
+			break;
+		case IOMMU_DOMAIN_DMA_FQ:
+			type = "DMA-FQ";
+			break;
+		}
+	}
+	mutex_unlock(&group->mutex);
+
+	return sysfs_emit(buf, "%s\n", type);
+}
+
+static IOMMU_GROUP_ATTR(name, S_IRUGO, iommu_group_show_name, NULL);
+
+static IOMMU_GROUP_ATTR(reserved_regions, 0444,
+			iommu_group_show_resv_regions, NULL);
+
+static IOMMU_GROUP_ATTR(type, 0644, iommu_group_show_type,
+			iommu_group_store_type);
+
+static void iommu_group_release(struct kobject *kobj)
+{
+	struct iommu_group *group = to_iommu_group(kobj);
+
+	pr_debug("Releasing group %d\n", group->id);
+
+	if (group->iommu_data_release)
+		group->iommu_data_release(group->iommu_data);
+
+	ida_free(&iommu_group_ida, group->id);
+
+	/* Domains are free'd by iommu_deinit_device() */
+	WARN_ON(group->default_domain);
+	WARN_ON(group->blocking_domain);
+
+	kfree(group->name);
+	kfree(group);
+}
+
+static const struct kobj_type iommu_group_ktype = {
+	.sysfs_ops = &iommu_group_sysfs_ops,
+	.release = iommu_group_release,
+};
+
+/**
+ * iommu_group_alloc - Allocate a new group
+ *
+ * This function is called by an iommu driver to allocate a new iommu
+ * group.  The iommu group represents the minimum granularity of the iommu.
+ * Upon successful return, the caller holds a reference to the supplied
+ * group in order to hold the group until devices are added.  Use
+ * iommu_group_put() to release this extra reference count, allowing the
+ * group to be automatically reclaimed once it has no devices or external
+ * references.
+ */
+struct iommu_group *iommu_group_alloc(void)
+{
+	struct iommu_group *group;
+	int ret;
+
+	group = kzalloc(sizeof(*group), GFP_KERNEL);
+	if (!group)
+		return ERR_PTR(-ENOMEM);
+
+	group->kobj.kset = iommu_group_kset;
+	mutex_init(&group->mutex);
+	INIT_LIST_HEAD(&group->devices);
+	INIT_LIST_HEAD(&group->entry);
+	xa_init(&group->pasid_array);
+
+	ret = ida_alloc(&iommu_group_ida, GFP_KERNEL);
+	if (ret < 0) {
+		kfree(group);
+		return ERR_PTR(ret);
+	}
+	group->id = ret;
+
+	ret = kobject_init_and_add(&group->kobj, &iommu_group_ktype,
+				   NULL, "%d", group->id);
+	if (ret) {
+		kobject_put(&group->kobj);
+		return ERR_PTR(ret);
+	}
+
+	group->devices_kobj = kobject_create_and_add("devices", &group->kobj);
+	if (!group->devices_kobj) {
+		kobject_put(&group->kobj); /* triggers .release & free */
+		return ERR_PTR(-ENOMEM);
+	}
+
+	/*
+	 * The devices_kobj holds a reference on the group kobject, so
+	 * as long as that exists so will the group.  We can therefore
+	 * use the devices_kobj for reference counting.
+	 */
+	kobject_put(&group->kobj);
+
+	ret = iommu_group_create_file(group,
+				      &iommu_group_attr_reserved_regions);
+	if (ret) {
+		kobject_put(group->devices_kobj);
+		return ERR_PTR(ret);
+	}
+
+	ret = iommu_group_create_file(group, &iommu_group_attr_type);
+	if (ret) {
+		kobject_put(group->devices_kobj);
+		return ERR_PTR(ret);
+	}
+
+	pr_debug("Allocated group %d\n", group->id);
+
+	return group;
+}
+EXPORT_SYMBOL_GPL(iommu_group_alloc);
+
+/**
+ * iommu_group_get_iommudata - retrieve iommu_data registered for a group
+ * @group: the group
+ *
+ * iommu drivers can store data in the group for use when doing iommu
+ * operations.  This function provides a way to retrieve it.  Caller
+ * should hold a group reference.
+ */
+void *iommu_group_get_iommudata(struct iommu_group *group)
+{
+	return group->iommu_data;
+}
+EXPORT_SYMBOL_GPL(iommu_group_get_iommudata);
+
+/**
+ * iommu_group_set_iommudata - set iommu_data for a group
+ * @group: the group
+ * @iommu_data: new data
+ * @release: release function for iommu_data
+ *
+ * iommu drivers can store data in the group for use when doing iommu
+ * operations.  This function provides a way to set the data after
+ * the group has been allocated.  Caller should hold a group reference.
+ */
+void iommu_group_set_iommudata(struct iommu_group *group, void *iommu_data,
+			       void (*release)(void *iommu_data))
+{
+	group->iommu_data = iommu_data;
+	group->iommu_data_release = release;
+}
+EXPORT_SYMBOL_GPL(iommu_group_set_iommudata);
+
+/**
+ * iommu_group_set_name - set name for a group
+ * @group: the group
+ * @name: name
+ *
+ * Allow iommu driver to set a name for a group.  When set it will
+ * appear in a name attribute file under the group in sysfs.
+ */
+int iommu_group_set_name(struct iommu_group *group, const char *name)
+{
+	int ret;
+
+	if (group->name) {
+		iommu_group_remove_file(group, &iommu_group_attr_name);
+		kfree(group->name);
+		group->name = NULL;
+		if (!name)
+			return 0;
+	}
+
+	group->name = kstrdup(name, GFP_KERNEL);
+	if (!group->name)
+		return -ENOMEM;
+
+	ret = iommu_group_create_file(group, &iommu_group_attr_name);
+	if (ret) {
+		kfree(group->name);
+		group->name = NULL;
+		return ret;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(iommu_group_set_name);
+
+static int iommu_create_device_direct_mappings(struct iommu_domain *domain,
+					       struct device *dev)
+{
+	struct iommu_resv_region *entry;
+	struct list_head mappings;
+	unsigned long pg_size;
+	int ret = 0;
+
+	pg_size = domain->pgsize_bitmap ? 1UL << __ffs(domain->pgsize_bitmap) : 0;
+	INIT_LIST_HEAD(&mappings);
+
+	if (WARN_ON_ONCE(iommu_is_dma_domain(domain) && !pg_size))
+		return -EINVAL;
+
+	iommu_get_resv_regions(dev, &mappings);
+
+	/* We need to consider overlapping regions for different devices */
+	list_for_each_entry(entry, &mappings, list) {
+		dma_addr_t start, end, addr;
+		size_t map_size = 0;
+
+		if (entry->type == IOMMU_RESV_DIRECT)
+			dev->iommu->require_direct = 1;
+
+		if ((entry->type != IOMMU_RESV_DIRECT &&
+		     entry->type != IOMMU_RESV_DIRECT_RELAXABLE) ||
+		    !iommu_is_dma_domain(domain))
+			continue;
+
+		start = ALIGN(entry->start, pg_size);
+		end   = ALIGN(entry->start + entry->length, pg_size);
+
+		for (addr = start; addr <= end; addr += pg_size) {
+			phys_addr_t phys_addr;
+
+			if (addr == end)
+				goto map_end;
+
+			phys_addr = iommu_iova_to_phys(domain, addr);
+			if (!phys_addr) {
+				map_size += pg_size;
+				continue;
+			}
+
+map_end:
+			if (map_size) {
+				ret = iommu_map(domain, addr - map_size,
+						addr - map_size, map_size,
+						entry->prot, GFP_KERNEL);
+				if (ret)
+					goto out;
+				map_size = 0;
+			}
+		}
+
+	}
+out:
+	iommu_put_resv_regions(dev, &mappings);
+
+	return ret;
+}
+
+/* This is undone by __iommu_group_free_device() */
+static struct group_device *iommu_group_alloc_device(struct iommu_group *group,
+						     struct device *dev)
+{
+	int ret, i = 0;
+	struct group_device *device;
+
+	device = kzalloc(sizeof(*device), GFP_KERNEL);
+	if (!device)
+		return ERR_PTR(-ENOMEM);
+
+	device->dev = dev;
+
+	ret = sysfs_create_link(&dev->kobj, &group->kobj, "iommu_group");
+	if (ret)
+		goto err_free_device;
+
+	device->name = kasprintf(GFP_KERNEL, "%s", kobject_name(&dev->kobj));
+rename:
+	if (!device->name) {
+		ret = -ENOMEM;
+		goto err_remove_link;
+	}
+
+	ret = sysfs_create_link_nowarn(group->devices_kobj,
+				       &dev->kobj, device->name);
+	if (ret) {
+		if (ret == -EEXIST && i >= 0) {
+			/*
+			 * Account for the slim chance of collision
+			 * and append an instance to the name.
+			 */
+			kfree(device->name);
+			device->name = kasprintf(GFP_KERNEL, "%s.%d",
+						 kobject_name(&dev->kobj), i++);
+			goto rename;
+		}
+		goto err_free_name;
+	}
+
+	trace_add_device_to_group(group->id, dev);
+
+	dev_info(dev, "Adding to iommu group %d\n", group->id);
+
+	return device;
+
+err_free_name:
+	kfree(device->name);
+err_remove_link:
+	sysfs_remove_link(&dev->kobj, "iommu_group");
+err_free_device:
+	kfree(device);
+	dev_err(dev, "Failed to add to iommu group %d: %d\n", group->id, ret);
+	return ERR_PTR(ret);
+}
+
+/**
+ * iommu_group_add_device - add a device to an iommu group
+ * @group: the group into which to add the device (reference should be held)
+ * @dev: the device
+ *
+ * This function is called by an iommu driver to add a device into a
+ * group.  Adding a device increments the group reference count.
+ */
+int iommu_group_add_device(struct iommu_group *group, struct device *dev)
+{
+	struct group_device *gdev;
+
+	gdev = iommu_group_alloc_device(group, dev);
+	if (IS_ERR(gdev))
+		return PTR_ERR(gdev);
+
+	iommu_group_ref_get(group);
+	dev->iommu_group = group;
+
+	mutex_lock(&group->mutex);
+	list_add_tail(&gdev->list, &group->devices);
+	mutex_unlock(&group->mutex);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(iommu_group_add_device);
+
+/**
+ * iommu_group_remove_device - remove a device from it's current group
+ * @dev: device to be removed
+ *
+ * This function is called by an iommu driver to remove the device from
+ * it's current group.  This decrements the iommu group reference count.
+ */
+void iommu_group_remove_device(struct device *dev)
+{
+	struct iommu_group *group = dev->iommu_group;
+
+	if (!group)
+		return;
+
+	dev_info(dev, "Removing from iommu group %d\n", group->id);
+
+	__iommu_group_remove_device(dev);
+}
+EXPORT_SYMBOL_GPL(iommu_group_remove_device);
+
+#if IS_ENABLED(CONFIG_LOCKDEP) && IS_ENABLED(CONFIG_IOMMU_API)
+/**
+ * iommu_group_mutex_assert - Check device group mutex lock
+ * @dev: the device that has group param set
+ *
+ * This function is called by an iommu driver to check whether it holds
+ * group mutex lock for the given device or not.
+ *
+ * Note that this function must be called after device group param is set.
+ */
+void iommu_group_mutex_assert(struct device *dev)
+{
+	struct iommu_group *group = dev->iommu_group;
+
+	lockdep_assert_held(&group->mutex);
+}
+EXPORT_SYMBOL_GPL(iommu_group_mutex_assert);
+#endif
+
+static struct device *iommu_group_first_dev(struct iommu_group *group)
+{
+	lockdep_assert_held(&group->mutex);
+	return list_first_entry(&group->devices, struct group_device, list)->dev;
+}
+
+/**
+ * iommu_group_for_each_dev - iterate over each device in the group
+ * @group: the group
+ * @data: caller opaque data to be passed to callback function
+ * @fn: caller supplied callback function
+ *
+ * This function is called by group users to iterate over group devices.
+ * Callers should hold a reference count to the group during callback.
+ * The group->mutex is held across callbacks, which will block calls to
+ * iommu_group_add/remove_device.
+ */
+int iommu_group_for_each_dev(struct iommu_group *group, void *data,
+			     int (*fn)(struct device *, void *))
+{
+	struct group_device *device;
+	int ret = 0;
+
+	mutex_lock(&group->mutex);
+	for_each_group_device(group, device) {
+		ret = fn(device->dev, data);
+		if (ret)
+			break;
+	}
+	mutex_unlock(&group->mutex);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_group_for_each_dev);
+
+/**
+ * iommu_group_get - Return the group for a device and increment reference
+ * @dev: get the group that this device belongs to
+ *
+ * This function is called by iommu drivers and users to get the group
+ * for the specified device.  If found, the group is returned and the group
+ * reference in incremented, else NULL.
+ */
+struct iommu_group *iommu_group_get(struct device *dev)
+{
+	struct iommu_group *group = dev->iommu_group;
+
+	if (group)
+		kobject_get(group->devices_kobj);
+
+	return group;
+}
+EXPORT_SYMBOL_GPL(iommu_group_get);
+
+/**
+ * iommu_group_ref_get - Increment reference on a group
+ * @group: the group to use, must not be NULL
+ *
+ * This function is called by iommu drivers to take additional references on an
+ * existing group.  Returns the given group for convenience.
+ */
+struct iommu_group *iommu_group_ref_get(struct iommu_group *group)
+{
+	kobject_get(group->devices_kobj);
+	return group;
+}
+EXPORT_SYMBOL_GPL(iommu_group_ref_get);
+
+/**
+ * iommu_group_put - Decrement group reference
+ * @group: the group to use
+ *
+ * This function is called by iommu drivers and users to release the
+ * iommu group.  Once the reference count is zero, the group is released.
+ */
+void iommu_group_put(struct iommu_group *group)
+{
+	if (group)
+		kobject_put(group->devices_kobj);
+}
+EXPORT_SYMBOL_GPL(iommu_group_put);
+
+/**
+ * iommu_group_id - Return ID for a group
+ * @group: the group to ID
+ *
+ * Return the unique ID for the group matching the sysfs group number.
+ */
+int iommu_group_id(struct iommu_group *group)
+{
+	return group->id;
+}
+EXPORT_SYMBOL_GPL(iommu_group_id);
+
+static struct iommu_group *get_pci_alias_group(struct pci_dev *pdev,
+					       unsigned long *devfns);
+
+/*
+ * To consider a PCI device isolated, we require ACS to support Source
+ * Validation, Request Redirection, Completer Redirection, and Upstream
+ * Forwarding.  This effectively means that devices cannot spoof their
+ * requester ID, requests and completions cannot be redirected, and all
+ * transactions are forwarded upstream, even as it passes through a
+ * bridge where the target device is downstream.
+ */
+#define REQ_ACS_FLAGS   (PCI_ACS_SV | PCI_ACS_RR | PCI_ACS_CR | PCI_ACS_UF)
+
+/*
+ * For multifunction devices which are not isolated from each other, find
+ * all the other non-isolated functions and look for existing groups.  For
+ * each function, we also need to look for aliases to or from other devices
+ * that may already have a group.
+ */
+static struct iommu_group *get_pci_function_alias_group(struct pci_dev *pdev,
+							unsigned long *devfns)
+{
+	struct pci_dev *tmp = NULL;
+	struct iommu_group *group;
+
+	if (!pdev->multifunction || pci_acs_enabled(pdev, REQ_ACS_FLAGS))
+		return NULL;
+
+	for_each_pci_dev(tmp) {
+		if (tmp == pdev || tmp->bus != pdev->bus ||
+		    PCI_SLOT(tmp->devfn) != PCI_SLOT(pdev->devfn) ||
+		    pci_acs_enabled(tmp, REQ_ACS_FLAGS))
+			continue;
+
+		group = get_pci_alias_group(tmp, devfns);
+		if (group) {
+			pci_dev_put(tmp);
+			return group;
+		}
+	}
+
+	return NULL;
+}
+
+/*
+ * Look for aliases to or from the given device for existing groups. DMA
+ * aliases are only supported on the same bus, therefore the search
+ * space is quite small (especially since we're really only looking at pcie
+ * device, and therefore only expect multiple slots on the root complex or
+ * downstream switch ports).  It's conceivable though that a pair of
+ * multifunction devices could have aliases between them that would cause a
+ * loop.  To prevent this, we use a bitmap to track where we've been.
+ */
+static struct iommu_group *get_pci_alias_group(struct pci_dev *pdev,
+					       unsigned long *devfns)
+{
+	struct pci_dev *tmp = NULL;
+	struct iommu_group *group;
+
+	if (test_and_set_bit(pdev->devfn & 0xff, devfns))
+		return NULL;
+
+	group = iommu_group_get(&pdev->dev);
+	if (group)
+		return group;
+
+	for_each_pci_dev(tmp) {
+		if (tmp == pdev || tmp->bus != pdev->bus)
+			continue;
+
+		/* We alias them or they alias us */
+		if (pci_devs_are_dma_aliases(pdev, tmp)) {
+			group = get_pci_alias_group(tmp, devfns);
+			if (group) {
+				pci_dev_put(tmp);
+				return group;
+			}
+
+			group = get_pci_function_alias_group(tmp, devfns);
+			if (group) {
+				pci_dev_put(tmp);
+				return group;
+			}
+		}
+	}
+
+	return NULL;
+}
+
+struct group_for_pci_data {
+	struct pci_dev *pdev;
+	struct iommu_group *group;
+};
+
+/*
+ * DMA alias iterator callback, return the last seen device.  Stop and return
+ * the IOMMU group if we find one along the way.
+ */
+static int get_pci_alias_or_group(struct pci_dev *pdev, u16 alias, void *opaque)
+{
+	struct group_for_pci_data *data = opaque;
+
+	data->pdev = pdev;
+	data->group = iommu_group_get(&pdev->dev);
+
+	return data->group != NULL;
+}
+
+/*
+ * Generic device_group call-back function. It just allocates one
+ * iommu-group per device.
+ */
+struct iommu_group *generic_device_group(struct device *dev)
+{
+	return iommu_group_alloc();
+}
+EXPORT_SYMBOL_GPL(generic_device_group);
+
+/*
+ * Generic device_group call-back function. It just allocates one
+ * iommu-group per iommu driver instance shared by every device
+ * probed by that iommu driver.
+ */
+struct iommu_group *generic_single_device_group(struct device *dev)
+{
+	struct iommu_device *iommu = dev->iommu->iommu_dev;
+
+	if (!iommu->singleton_group) {
+		struct iommu_group *group;
+
+		group = iommu_group_alloc();
+		if (IS_ERR(group))
+			return group;
+		iommu->singleton_group = group;
+	}
+	return iommu_group_ref_get(iommu->singleton_group);
+}
+EXPORT_SYMBOL_GPL(generic_single_device_group);
+
+/*
+ * Use standard PCI bus topology, isolation features, and DMA alias quirks
+ * to find or create an IOMMU group for a device.
+ */
+struct iommu_group *pci_device_group(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct group_for_pci_data data;
+	struct pci_bus *bus;
+	struct iommu_group *group = NULL;
+	u64 devfns[4] = { 0 };
+
+	if (WARN_ON(!dev_is_pci(dev)))
+		return ERR_PTR(-EINVAL);
+
+	/*
+	 * Find the upstream DMA alias for the device.  A device must not
+	 * be aliased due to topology in order to have its own IOMMU group.
+	 * If we find an alias along the way that already belongs to a
+	 * group, use it.
+	 */
+	if (pci_for_each_dma_alias(pdev, get_pci_alias_or_group, &data))
+		return data.group;
+
+	pdev = data.pdev;
+
+	/*
+	 * Continue upstream from the point of minimum IOMMU granularity
+	 * due to aliases to the point where devices are protected from
+	 * peer-to-peer DMA by PCI ACS.  Again, if we find an existing
+	 * group, use it.
+	 */
+	for (bus = pdev->bus; !pci_is_root_bus(bus); bus = bus->parent) {
+		if (!bus->self)
+			continue;
+
+		if (pci_acs_path_enabled(bus->self, NULL, REQ_ACS_FLAGS))
+			break;
+
+		pdev = bus->self;
+
+		group = iommu_group_get(&pdev->dev);
+		if (group)
+			return group;
+	}
+
+	/*
+	 * Look for existing groups on device aliases.  If we alias another
+	 * device or another device aliases us, use the same group.
+	 */
+	group = get_pci_alias_group(pdev, (unsigned long *)devfns);
+	if (group)
+		return group;
+
+	/*
+	 * Look for existing groups on non-isolated functions on the same
+	 * slot and aliases of those funcions, if any.  No need to clear
+	 * the search bitmap, the tested devfns are still valid.
+	 */
+	group = get_pci_function_alias_group(pdev, (unsigned long *)devfns);
+	if (group)
+		return group;
+
+	/* No shared group found, allocate new */
+	return iommu_group_alloc();
+}
+EXPORT_SYMBOL_GPL(pci_device_group);
+
+/* Get the IOMMU group for device on fsl-mc bus */
+struct iommu_group *fsl_mc_device_group(struct device *dev)
+{
+	struct device *cont_dev = fsl_mc_cont_dev(dev);
+	struct iommu_group *group;
+
+	group = iommu_group_get(cont_dev);
+	if (!group)
+		group = iommu_group_alloc();
+	return group;
+}
+EXPORT_SYMBOL_GPL(fsl_mc_device_group);
+
+static struct iommu_domain *__iommu_alloc_identity_domain(struct device *dev)
+{
+	const struct iommu_ops *ops = dev_iommu_ops(dev);
+	struct iommu_domain *domain;
+
+	if (ops->identity_domain)
+		return ops->identity_domain;
+
+	if (ops->domain_alloc_identity) {
+		domain = ops->domain_alloc_identity(dev);
+		if (IS_ERR(domain))
+			return domain;
+	} else {
+		return ERR_PTR(-EOPNOTSUPP);
+	}
+
+	iommu_domain_init(domain, IOMMU_DOMAIN_IDENTITY, ops);
+	return domain;
+}
+
+static struct iommu_domain *
+__iommu_group_alloc_default_domain(struct iommu_group *group, int req_type)
+{
+	struct device *dev = iommu_group_first_dev(group);
+	struct iommu_domain *dom;
+
+	if (group->default_domain && group->default_domain->type == req_type)
+		return group->default_domain;
+
+	/*
+	 * When allocating the DMA API domain assume that the driver is going to
+	 * use PASID and make sure the RID's domain is PASID compatible.
+	 */
+	if (req_type & __IOMMU_DOMAIN_PAGING) {
+		dom = __iommu_paging_domain_alloc_flags(dev, req_type,
+			   dev->iommu->max_pasids ? IOMMU_HWPT_ALLOC_PASID : 0);
+
+		/*
+		 * If driver does not support PASID feature then
+		 * try to allocate non-PASID domain
+		 */
+		if (PTR_ERR(dom) == -EOPNOTSUPP)
+			dom = __iommu_paging_domain_alloc_flags(dev, req_type, 0);
+
+		return dom;
+	}
+
+	if (req_type == IOMMU_DOMAIN_IDENTITY)
+		return __iommu_alloc_identity_domain(dev);
+
+	return ERR_PTR(-EINVAL);
+}
+
+/*
+ * req_type of 0 means "auto" which means to select a domain based on
+ * iommu_def_domain_type or what the driver actually supports.
+ */
+static struct iommu_domain *
+iommu_group_alloc_default_domain(struct iommu_group *group, int req_type)
+{
+	const struct iommu_ops *ops = dev_iommu_ops(iommu_group_first_dev(group));
+	struct iommu_domain *dom;
+
+	lockdep_assert_held(&group->mutex);
+
+	/*
+	 * Allow legacy drivers to specify the domain that will be the default
+	 * domain. This should always be either an IDENTITY/BLOCKED/PLATFORM
+	 * domain. Do not use in new drivers.
+	 */
+	if (ops->default_domain) {
+		if (req_type != ops->default_domain->type)
+			return ERR_PTR(-EINVAL);
+		return ops->default_domain;
+	}
+
+	if (req_type)
+		return __iommu_group_alloc_default_domain(group, req_type);
+
+	/* The driver gave no guidance on what type to use, try the default */
+	dom = __iommu_group_alloc_default_domain(group, iommu_def_domain_type);
+	if (!IS_ERR(dom))
+		return dom;
+
+	/* Otherwise IDENTITY and DMA_FQ defaults will try DMA */
+	if (iommu_def_domain_type == IOMMU_DOMAIN_DMA)
+		return ERR_PTR(-EINVAL);
+	dom = __iommu_group_alloc_default_domain(group, IOMMU_DOMAIN_DMA);
+	if (IS_ERR(dom))
+		return dom;
+
+	pr_warn("Failed to allocate default IOMMU domain of type %u for group %s - Falling back to IOMMU_DOMAIN_DMA",
+		iommu_def_domain_type, group->name);
+	return dom;
+}
+
+struct iommu_domain *iommu_group_default_domain(struct iommu_group *group)
+{
+	return group->default_domain;
+}
+
+static int probe_iommu_group(struct device *dev, void *data)
+{
+	struct list_head *group_list = data;
+	int ret;
+
+	mutex_lock(&iommu_probe_device_lock);
+	ret = __iommu_probe_device(dev, group_list);
+	mutex_unlock(&iommu_probe_device_lock);
+	if (ret == -ENODEV)
+		ret = 0;
+
+	return ret;
+}
+
+static int iommu_bus_notifier(struct notifier_block *nb,
+			      unsigned long action, void *data)
+{
+	struct device *dev = data;
+
+	if (action == BUS_NOTIFY_ADD_DEVICE) {
+		int ret;
+
+		ret = iommu_probe_device(dev);
+		return (ret) ? NOTIFY_DONE : NOTIFY_OK;
+	} else if (action == BUS_NOTIFY_REMOVED_DEVICE) {
+		iommu_release_device(dev);
+		return NOTIFY_OK;
+	}
+
+	return 0;
+}
+
+/*
+ * Combine the driver's chosen def_domain_type across all the devices in a
+ * group. Drivers must give a consistent result.
+ */
+static int iommu_get_def_domain_type(struct iommu_group *group,
+				     struct device *dev, int cur_type)
+{
+	const struct iommu_ops *ops = dev_iommu_ops(dev);
+	int type;
+
+	if (ops->default_domain) {
+		/*
+		 * Drivers that declare a global static default_domain will
+		 * always choose that.
+		 */
+		type = ops->default_domain->type;
+	} else {
+		if (ops->def_domain_type)
+			type = ops->def_domain_type(dev);
+		else
+			return cur_type;
+	}
+	if (!type || cur_type == type)
+		return cur_type;
+	if (!cur_type)
+		return type;
+
+	dev_err_ratelimited(
+		dev,
+		"IOMMU driver error, requesting conflicting def_domain_type, %s and %s, for devices in group %u.\n",
+		iommu_domain_type_str(cur_type), iommu_domain_type_str(type),
+		group->id);
+
+	/*
+	 * Try to recover, drivers are allowed to force IDENTITY or DMA, IDENTITY
+	 * takes precedence.
+	 */
+	if (type == IOMMU_DOMAIN_IDENTITY)
+		return type;
+	return cur_type;
+}
+
+/*
+ * A target_type of 0 will select the best domain type. 0 can be returned in
+ * this case meaning the global default should be used.
+ */
+static int iommu_get_default_domain_type(struct iommu_group *group,
+					 int target_type)
+{
+	struct device *untrusted = NULL;
+	struct group_device *gdev;
+	int driver_type = 0;
+
+	lockdep_assert_held(&group->mutex);
+
+	/*
+	 * ARM32 drivers supporting CONFIG_ARM_DMA_USE_IOMMU can declare an
+	 * identity_domain and it will automatically become their default
+	 * domain. Later on ARM_DMA_USE_IOMMU will install its UNMANAGED domain.
+	 * Override the selection to IDENTITY.
+	 */
+	if (IS_ENABLED(CONFIG_ARM_DMA_USE_IOMMU)) {
+		static_assert(!(IS_ENABLED(CONFIG_ARM_DMA_USE_IOMMU) &&
+				IS_ENABLED(CONFIG_IOMMU_DMA)));
+		driver_type = IOMMU_DOMAIN_IDENTITY;
+	}
+
+	for_each_group_device(group, gdev) {
+		driver_type = iommu_get_def_domain_type(group, gdev->dev,
+							driver_type);
+
+		if (dev_is_pci(gdev->dev) && to_pci_dev(gdev->dev)->untrusted) {
+			/*
+			 * No ARM32 using systems will set untrusted, it cannot
+			 * work.
+			 */
+			if (WARN_ON(IS_ENABLED(CONFIG_ARM_DMA_USE_IOMMU)))
+				return -1;
+			untrusted = gdev->dev;
+		}
+	}
+
+	/*
+	 * If the common dma ops are not selected in kconfig then we cannot use
+	 * IOMMU_DOMAIN_DMA at all. Force IDENTITY if nothing else has been
+	 * selected.
+	 */
+	if (!IS_ENABLED(CONFIG_IOMMU_DMA)) {
+		if (WARN_ON(driver_type == IOMMU_DOMAIN_DMA))
+			return -1;
+		if (!driver_type)
+			driver_type = IOMMU_DOMAIN_IDENTITY;
+	}
+
+	if (untrusted) {
+		if (driver_type && driver_type != IOMMU_DOMAIN_DMA) {
+			dev_err_ratelimited(
+				untrusted,
+				"Device is not trusted, but driver is overriding group %u to %s, refusing to probe.\n",
+				group->id, iommu_domain_type_str(driver_type));
+			return -1;
+		}
+		driver_type = IOMMU_DOMAIN_DMA;
+	}
+
+	if (target_type) {
+		if (driver_type && target_type != driver_type)
+			return -1;
+		return target_type;
+	}
+	return driver_type;
+}
+
+static void iommu_group_do_probe_finalize(struct device *dev)
+{
+	const struct iommu_ops *ops = dev_iommu_ops(dev);
+
+	if (ops->probe_finalize)
+		ops->probe_finalize(dev);
+}
+
+static int bus_iommu_probe(const struct bus_type *bus)
+{
+	struct iommu_group *group, *next;
+	LIST_HEAD(group_list);
+	int ret;
+
+	ret = bus_for_each_dev(bus, NULL, &group_list, probe_iommu_group);
+	if (ret)
+		return ret;
+
+	list_for_each_entry_safe(group, next, &group_list, entry) {
+		struct group_device *gdev;
+
+		mutex_lock(&group->mutex);
+
+		/* Remove item from the list */
+		list_del_init(&group->entry);
+
+		/*
+		 * We go to the trouble of deferred default domain creation so
+		 * that the cross-group default domain type and the setup of the
+		 * IOMMU_RESV_DIRECT will work correctly in non-hotpug scenarios.
+		 */
+		ret = iommu_setup_default_domain(group, 0);
+		if (ret) {
+			mutex_unlock(&group->mutex);
+			return ret;
+		}
+		for_each_group_device(group, gdev)
+			iommu_setup_dma_ops(gdev->dev);
+		mutex_unlock(&group->mutex);
+
+		/*
+		 * FIXME: Mis-locked because the ops->probe_finalize() call-back
+		 * of some IOMMU drivers calls arm_iommu_attach_device() which
+		 * in-turn might call back into IOMMU core code, where it tries
+		 * to take group->mutex, resulting in a deadlock.
+		 */
+		for_each_group_device(group, gdev)
+			iommu_group_do_probe_finalize(gdev->dev);
+	}
+
+	return 0;
+}
+
+/**
+ * device_iommu_capable() - check for a general IOMMU capability
+ * @dev: device to which the capability would be relevant, if available
+ * @cap: IOMMU capability
+ *
+ * Return: true if an IOMMU is present and supports the given capability
+ * for the given device, otherwise false.
+ */
+bool device_iommu_capable(struct device *dev, enum iommu_cap cap)
+{
+	const struct iommu_ops *ops;
+
+	if (!dev_has_iommu(dev))
+		return false;
+
+	ops = dev_iommu_ops(dev);
+	if (!ops->capable)
+		return false;
+
+	return ops->capable(dev, cap);
+}
+EXPORT_SYMBOL_GPL(device_iommu_capable);
+
+/**
+ * iommu_group_has_isolated_msi() - Compute msi_device_has_isolated_msi()
+ *       for a group
+ * @group: Group to query
+ *
+ * IOMMU groups should not have differing values of
+ * msi_device_has_isolated_msi() for devices in a group. However nothing
+ * directly prevents this, so ensure mistakes don't result in isolation failures
+ * by checking that all the devices are the same.
+ */
+bool iommu_group_has_isolated_msi(struct iommu_group *group)
+{
+	struct group_device *group_dev;
+	bool ret = true;
+
+	mutex_lock(&group->mutex);
+	for_each_group_device(group, group_dev)
+		ret &= msi_device_has_isolated_msi(group_dev->dev);
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_group_has_isolated_msi);
+
+/**
+ * iommu_set_fault_handler() - set a fault handler for an iommu domain
+ * @domain: iommu domain
+ * @handler: fault handler
+ * @token: user data, will be passed back to the fault handler
+ *
+ * This function should be used by IOMMU users which want to be notified
+ * whenever an IOMMU fault happens.
+ *
+ * The fault handler itself should return 0 on success, and an appropriate
+ * error code otherwise.
+ */
+void iommu_set_fault_handler(struct iommu_domain *domain,
+					iommu_fault_handler_t handler,
+					void *token)
+{
+	if (WARN_ON(!domain || domain->cookie_type != IOMMU_COOKIE_NONE))
+		return;
+
+	domain->cookie_type = IOMMU_COOKIE_FAULT_HANDLER;
+	domain->handler = handler;
+	domain->handler_token = token;
+}
+EXPORT_SYMBOL_GPL(iommu_set_fault_handler);
+
+static void iommu_domain_init(struct iommu_domain *domain, unsigned int type,
+			      const struct iommu_ops *ops)
+{
+	domain->type = type;
+	domain->owner = ops;
+	if (!domain->ops)
+		domain->ops = ops->default_domain_ops;
+}
+
+static struct iommu_domain *
+__iommu_paging_domain_alloc_flags(struct device *dev, unsigned int type,
+				  unsigned int flags)
+{
+	const struct iommu_ops *ops;
+	struct iommu_domain *domain;
+
+	if (!dev_has_iommu(dev))
+		return ERR_PTR(-ENODEV);
+
+	ops = dev_iommu_ops(dev);
+
+	if (ops->domain_alloc_paging && !flags)
+		domain = ops->domain_alloc_paging(dev);
+	else if (ops->domain_alloc_paging_flags)
+		domain = ops->domain_alloc_paging_flags(dev, flags, NULL);
+#if IS_ENABLED(CONFIG_FSL_PAMU)
+	else if (ops->domain_alloc && !flags)
+		domain = ops->domain_alloc(IOMMU_DOMAIN_UNMANAGED);
+#endif
+	else
+		return ERR_PTR(-EOPNOTSUPP);
+
+	if (IS_ERR(domain))
+		return domain;
+	if (!domain)
+		return ERR_PTR(-ENOMEM);
+
+	iommu_domain_init(domain, type, ops);
+	return domain;
+}
+
+/**
+ * iommu_paging_domain_alloc_flags() - Allocate a paging domain
+ * @dev: device for which the domain is allocated
+ * @flags: Bitmap of iommufd_hwpt_alloc_flags
+ *
+ * Allocate a paging domain which will be managed by a kernel driver. Return
+ * allocated domain if successful, or an ERR pointer for failure.
+ */
+struct iommu_domain *iommu_paging_domain_alloc_flags(struct device *dev,
+						     unsigned int flags)
+{
+	return __iommu_paging_domain_alloc_flags(dev,
+					 IOMMU_DOMAIN_UNMANAGED, flags);
+}
+EXPORT_SYMBOL_GPL(iommu_paging_domain_alloc_flags);
+
+void iommu_domain_free(struct iommu_domain *domain)
+{
+	switch (domain->cookie_type) {
+	case IOMMU_COOKIE_DMA_IOVA:
+		iommu_put_dma_cookie(domain);
+		break;
+	case IOMMU_COOKIE_DMA_MSI:
+		iommu_put_msi_cookie(domain);
+		break;
+	case IOMMU_COOKIE_SVA:
+		mmdrop(domain->mm);
+		break;
+	default:
+		break;
+	}
+	if (domain->ops->free)
+		domain->ops->free(domain);
+}
+EXPORT_SYMBOL_GPL(iommu_domain_free);
+
+/*
+ * Put the group's domain back to the appropriate core-owned domain - either the
+ * standard kernel-mode DMA configuration or an all-DMA-blocked domain.
+ */
+static void __iommu_group_set_core_domain(struct iommu_group *group)
+{
+	struct iommu_domain *new_domain;
+
+	if (group->owner)
+		new_domain = group->blocking_domain;
+	else
+		new_domain = group->default_domain;
+
+	__iommu_group_set_domain_nofail(group, new_domain);
+}
+
+static int __iommu_attach_device(struct iommu_domain *domain,
+				 struct device *dev, struct iommu_domain *old)
+{
+	int ret;
+
+	if (unlikely(domain->ops->attach_dev == NULL))
+		return -ENODEV;
+
+	ret = domain->ops->attach_dev(domain, dev, old);
+	if (ret)
+		return ret;
+	dev->iommu->attach_deferred = 0;
+	trace_attach_device_to_domain(dev);
+	return 0;
+}
+
+/**
+ * iommu_attach_device - Attach an IOMMU domain to a device
+ * @domain: IOMMU domain to attach
+ * @dev: Device that will be attached
+ *
+ * Returns 0 on success and error code on failure
+ *
+ * Note that EINVAL can be treated as a soft failure, indicating
+ * that certain configuration of the domain is incompatible with
+ * the device. In this case attaching a different domain to the
+ * device may succeed.
+ */
+int iommu_attach_device(struct iommu_domain *domain, struct device *dev)
+{
+	/* Caller must be a probed driver on dev */
+	struct iommu_group *group = dev->iommu_group;
+	int ret;
+
+	if (!group)
+		return -ENODEV;
+
+	/*
+	 * Lock the group to make sure the device-count doesn't
+	 * change while we are attaching
+	 */
+	mutex_lock(&group->mutex);
+	ret = -EINVAL;
+	if (list_count_nodes(&group->devices) != 1)
+		goto out_unlock;
+
+	ret = __iommu_attach_group(domain, group);
+
+out_unlock:
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_attach_device);
+
+int iommu_deferred_attach(struct device *dev, struct iommu_domain *domain)
+{
+	if (dev->iommu && dev->iommu->attach_deferred)
+		return __iommu_attach_device(domain, dev, NULL);
+
+	return 0;
+}
+
+void iommu_detach_device(struct iommu_domain *domain, struct device *dev)
+{
+	/* Caller must be a probed driver on dev */
+	struct iommu_group *group = dev->iommu_group;
+
+	if (!group)
+		return;
+
+	mutex_lock(&group->mutex);
+	if (WARN_ON(domain != group->domain) ||
+	    WARN_ON(list_count_nodes(&group->devices) != 1))
+		goto out_unlock;
+	__iommu_group_set_core_domain(group);
+
+out_unlock:
+	mutex_unlock(&group->mutex);
+}
+EXPORT_SYMBOL_GPL(iommu_detach_device);
+
+struct iommu_domain *iommu_get_domain_for_dev(struct device *dev)
+{
+	/* Caller must be a probed driver on dev */
+	struct iommu_group *group = dev->iommu_group;
+
+	if (!group)
+		return NULL;
+
+	return group->domain;
+}
+EXPORT_SYMBOL_GPL(iommu_get_domain_for_dev);
+
+/*
+ * For IOMMU_DOMAIN_DMA implementations which already provide their own
+ * guarantees that the group and its default domain are valid and correct.
+ */
+struct iommu_domain *iommu_get_dma_domain(struct device *dev)
+{
+	struct iommu_group *group = dev->iommu_group;
+
+	/*
+	 * If a DMA domain was explicitly attached to the group, honor it so
+	 * dma_map*() uses the shared mappings (e.g. multi-device shared DMA).
+	 */
+	if (group && group->domain && iommu_is_dma_domain(group->domain))
+		return group->domain;
+
+	return group ? group->default_domain : NULL;
+}
+
+static void *iommu_make_pasid_array_entry(struct iommu_domain *domain,
+					  struct iommu_attach_handle *handle)
+{
+	if (handle) {
+		handle->domain = domain;
+		return xa_tag_pointer(handle, IOMMU_PASID_ARRAY_HANDLE);
+	}
+
+	return xa_tag_pointer(domain, IOMMU_PASID_ARRAY_DOMAIN);
+}
+
+static bool domain_iommu_ops_compatible(const struct iommu_ops *ops,
+					struct iommu_domain *domain)
+{
+	if (domain->owner == ops)
+		return true;
+
+	/* For static domains, owner isn't set. */
+	if (domain == ops->blocked_domain || domain == ops->identity_domain)
+		return true;
+
+	return false;
+}
+
+static int __iommu_attach_group(struct iommu_domain *domain,
+				struct iommu_group *group)
+{
+	struct device *dev;
+
+	if (group->domain && group->domain != group->default_domain &&
+	    group->domain != group->blocking_domain)
+		return -EBUSY;
+
+	dev = iommu_group_first_dev(group);
+	if (!dev_has_iommu(dev) ||
+	    !domain_iommu_ops_compatible(dev_iommu_ops(dev), domain))
+		return -EINVAL;
+
+	return __iommu_group_set_domain(group, domain);
+}
+
+/**
+ * iommu_attach_group - Attach an IOMMU domain to an IOMMU group
+ * @domain: IOMMU domain to attach
+ * @group: IOMMU group that will be attached
+ *
+ * Returns 0 on success and error code on failure
+ *
+ * Note that EINVAL can be treated as a soft failure, indicating
+ * that certain configuration of the domain is incompatible with
+ * the group. In this case attaching a different domain to the
+ * group may succeed.
+ */
+int iommu_attach_group(struct iommu_domain *domain, struct iommu_group *group)
+{
+	int ret;
+
+	mutex_lock(&group->mutex);
+	ret = __iommu_attach_group(domain, group);
+	mutex_unlock(&group->mutex);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_attach_group);
+
+static int __iommu_device_set_domain(struct iommu_group *group,
+				     struct device *dev,
+				     struct iommu_domain *new_domain,
+				     struct iommu_domain *old_domain,
+				     unsigned int flags)
+{
+	int ret;
+
+	/*
+	 * If the device requires IOMMU_RESV_DIRECT then we cannot allow
+	 * the blocking domain to be attached as it does not contain the
+	 * required 1:1 mapping. This test effectively excludes the device
+	 * being used with iommu_group_claim_dma_owner() which will block
+	 * vfio and iommufd as well.
+	 */
+	if (dev->iommu->require_direct &&
+	    (new_domain->type == IOMMU_DOMAIN_BLOCKED ||
+	     new_domain == group->blocking_domain)) {
+		dev_warn(dev,
+			 "Firmware has requested this device have a 1:1 IOMMU mapping, rejecting configuring the device without a 1:1 mapping. Contact your platform vendor.\n");
+		return -EINVAL;
+	}
+
+	if (dev->iommu->attach_deferred) {
+		if (new_domain == group->default_domain)
+			return 0;
+		dev->iommu->attach_deferred = 0;
+	}
+
+	ret = __iommu_attach_device(new_domain, dev, old_domain);
+	if (ret) {
+		/*
+		 * If we have a blocking domain then try to attach that in hopes
+		 * of avoiding a UAF. Modern drivers should implement blocking
+		 * domains as global statics that cannot fail.
+		 */
+		if ((flags & IOMMU_SET_DOMAIN_MUST_SUCCEED) &&
+		    group->blocking_domain &&
+		    group->blocking_domain != new_domain)
+			__iommu_attach_device(group->blocking_domain, dev,
+					      old_domain);
+		return ret;
+	}
+	return 0;
+}
+
+/*
+ * If 0 is returned the group's domain is new_domain. If an error is returned
+ * then the group's domain will be set back to the existing domain unless
+ * IOMMU_SET_DOMAIN_MUST_SUCCEED, otherwise an error is returned and the group's
+ * domains is left inconsistent. This is a driver bug to fail attach with a
+ * previously good domain. We try to avoid a kernel UAF because of this.
+ *
+ * IOMMU groups are really the natural working unit of the IOMMU, but the IOMMU
+ * API works on domains and devices.  Bridge that gap by iterating over the
+ * devices in a group.  Ideally we'd have a single device which represents the
+ * requestor ID of the group, but we also allow IOMMU drivers to create policy
+ * defined minimum sets, where the physical hardware may be able to distiguish
+ * members, but we wish to group them at a higher level (ex. untrusted
+ * multi-function PCI devices).  Thus we attach each device.
+ */
+static int __iommu_group_set_domain_internal(struct iommu_group *group,
+					     struct iommu_domain *new_domain,
+					     unsigned int flags)
+{
+	struct group_device *last_gdev;
+	struct group_device *gdev;
+	int result;
+	int ret;
+
+	lockdep_assert_held(&group->mutex);
+
+	if (group->domain == new_domain)
+		return 0;
+
+	if (WARN_ON(!new_domain))
+		return -EINVAL;
+
+	/*
+	 * Changing the domain is done by calling attach_dev() on the new
+	 * domain. This switch does not have to be atomic and DMA can be
+	 * discarded during the transition. DMA must only be able to access
+	 * either new_domain or group->domain, never something else.
+	 */
+	result = 0;
+	for_each_group_device(group, gdev) {
+		ret = __iommu_device_set_domain(group, gdev->dev, new_domain,
+						group->domain, flags);
+		if (ret) {
+			result = ret;
+			/*
+			 * Keep trying the other devices in the group. If a
+			 * driver fails attach to an otherwise good domain, and
+			 * does not support blocking domains, it should at least
+			 * drop its reference on the current domain so we don't
+			 * UAF.
+			 */
+			if (flags & IOMMU_SET_DOMAIN_MUST_SUCCEED)
+				continue;
+			goto err_revert;
+		}
+	}
+	group->domain = new_domain;
+	return result;
+
+err_revert:
+	/*
+	 * This is called in error unwind paths. A well behaved driver should
+	 * always allow us to attach to a domain that was already attached.
+	 */
+	last_gdev = gdev;
+	for_each_group_device(group, gdev) {
+		/* No need to revert the last gdev that failed to set domain */
+		if (gdev == last_gdev)
+			break;
+		/*
+		 * A NULL domain can happen only for first probe, in which case
+		 * we leave group->domain as NULL and let release clean
+		 * everything up.
+		 */
+		if (group->domain)
+			WARN_ON(__iommu_device_set_domain(
+				group, gdev->dev, group->domain, new_domain,
+				IOMMU_SET_DOMAIN_MUST_SUCCEED));
+	}
+	return ret;
+}
+
+void iommu_detach_group(struct iommu_domain *domain, struct iommu_group *group)
+{
+	mutex_lock(&group->mutex);
+	__iommu_group_set_core_domain(group);
+	mutex_unlock(&group->mutex);
+}
+EXPORT_SYMBOL_GPL(iommu_detach_group);
+
+phys_addr_t iommu_iova_to_phys(struct iommu_domain *domain, dma_addr_t iova)
+{
+	if (domain->type == IOMMU_DOMAIN_IDENTITY)
+		return iova;
+
+	if (domain->type == IOMMU_DOMAIN_BLOCKED)
+		return 0;
+
+	return domain->ops->iova_to_phys(domain, iova);
+}
+EXPORT_SYMBOL_GPL(iommu_iova_to_phys);
+
+static size_t iommu_pgsize(struct iommu_domain *domain, unsigned long iova,
+			   phys_addr_t paddr, size_t size, size_t *count)
+{
+	unsigned int pgsize_idx, pgsize_idx_next;
+	unsigned long pgsizes;
+	size_t offset, pgsize, pgsize_next;
+	size_t offset_end;
+	unsigned long addr_merge = paddr | iova;
+
+	/* Page sizes supported by the hardware and small enough for @size */
+	pgsizes = domain->pgsize_bitmap & GENMASK(__fls(size), 0);
+
+	/* Constrain the page sizes further based on the maximum alignment */
+	if (likely(addr_merge))
+		pgsizes &= GENMASK(__ffs(addr_merge), 0);
+
+	/* Make sure we have at least one suitable page size */
+	BUG_ON(!pgsizes);
+
+	/* Pick the biggest page size remaining */
+	pgsize_idx = __fls(pgsizes);
+	pgsize = BIT(pgsize_idx);
+	if (!count)
+		return pgsize;
+
+	/* Find the next biggest support page size, if it exists */
+	pgsizes = domain->pgsize_bitmap & ~GENMASK(pgsize_idx, 0);
+	if (!pgsizes)
+		goto out_set_count;
+
+	pgsize_idx_next = __ffs(pgsizes);
+	pgsize_next = BIT(pgsize_idx_next);
+
+	/*
+	 * There's no point trying a bigger page size unless the virtual
+	 * and physical addresses are similarly offset within the larger page.
+	 */
+	if ((iova ^ paddr) & (pgsize_next - 1))
+		goto out_set_count;
+
+	/* Calculate the offset to the next page size alignment boundary */
+	offset = pgsize_next - (addr_merge & (pgsize_next - 1));
+
+	/*
+	 * If size is big enough to accommodate the larger page, reduce
+	 * the number of smaller pages.
+	 */
+	if (!check_add_overflow(offset, pgsize_next, &offset_end) &&
+	    offset_end <= size)
+		size = offset;
+
+out_set_count:
+	*count = size >> pgsize_idx;
+	return pgsize;
+}
+
+int iommu_map_nosync(struct iommu_domain *domain, unsigned long iova,
+		phys_addr_t paddr, size_t size, int prot, gfp_t gfp)
+{
+	const struct iommu_domain_ops *ops = domain->ops;
+	unsigned long orig_iova = iova;
+	unsigned int min_pagesz;
+	size_t orig_size = size;
+	phys_addr_t orig_paddr = paddr;
+	int ret = 0;
+
+	might_sleep_if(gfpflags_allow_blocking(gfp));
+
+	if (unlikely(!(domain->type & __IOMMU_DOMAIN_PAGING)))
+		return -EINVAL;
+
+	if (WARN_ON(!ops->map_pages || domain->pgsize_bitmap == 0UL))
+		return -ENODEV;
+
+	/* Discourage passing strange GFP flags */
+	if (WARN_ON_ONCE(gfp & (__GFP_COMP | __GFP_DMA | __GFP_DMA32 |
+				__GFP_HIGHMEM)))
+		return -EINVAL;
+
+	/* find out the minimum page size supported */
+	min_pagesz = 1 << __ffs(domain->pgsize_bitmap);
+
+	/*
+	 * both the virtual address and the physical one, as well as
+	 * the size of the mapping, must be aligned (at least) to the
+	 * size of the smallest page supported by the hardware
+	 */
+	if (!IS_ALIGNED(iova | paddr | size, min_pagesz)) {
+		pr_err("unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\n",
+		       iova, &paddr, size, min_pagesz);
+		return -EINVAL;
+	}
+
+	pr_debug("map: iova 0x%lx pa %pa size 0x%zx\n", iova, &paddr, size);
+
+	while (size) {
+		size_t pgsize, count, mapped = 0;
+
+		pgsize = iommu_pgsize(domain, iova, paddr, size, &count);
+
+		pr_debug("mapping: iova 0x%lx pa %pa pgsize 0x%zx count %zu\n",
+			 iova, &paddr, pgsize, count);
+		ret = ops->map_pages(domain, iova, paddr, pgsize, count, prot,
+				     gfp, &mapped);
+		/*
+		 * Some pages may have been mapped, even if an error occurred,
+		 * so we should account for those so they can be unmapped.
+		 */
+		size -= mapped;
+
+		if (ret)
+			break;
+
+		iova += mapped;
+		paddr += mapped;
+	}
+
+	/* unroll mapping in case something went wrong */
+	if (ret)
+		iommu_unmap(domain, orig_iova, orig_size - size);
+	else
+		trace_map(orig_iova, orig_paddr, orig_size);
+
+	return ret;
+}
+
+int iommu_sync_map(struct iommu_domain *domain, unsigned long iova, size_t size)
+{
+	const struct iommu_domain_ops *ops = domain->ops;
+
+	if (!ops->iotlb_sync_map)
+		return 0;
+	return ops->iotlb_sync_map(domain, iova, size);
+}
+
+int iommu_map(struct iommu_domain *domain, unsigned long iova,
+	      phys_addr_t paddr, size_t size, int prot, gfp_t gfp)
+{
+	int ret;
+
+	ret = iommu_map_nosync(domain, iova, paddr, size, prot, gfp);
+	if (ret)
+		return ret;
+
+	ret = iommu_sync_map(domain, iova, size);
+	if (ret)
+		iommu_unmap(domain, iova, size);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_map);
+
+static size_t __iommu_unmap(struct iommu_domain *domain,
+			    unsigned long iova, size_t size,
+			    struct iommu_iotlb_gather *iotlb_gather)
+{
+	const struct iommu_domain_ops *ops = domain->ops;
+	size_t unmapped_page, unmapped = 0;
+	unsigned long orig_iova = iova;
+	unsigned int min_pagesz;
+
+	if (unlikely(!(domain->type & __IOMMU_DOMAIN_PAGING)))
+		return 0;
+
+	if (WARN_ON(!ops->unmap_pages || domain->pgsize_bitmap == 0UL))
+		return 0;
+
+	/* find out the minimum page size supported */
+	min_pagesz = 1 << __ffs(domain->pgsize_bitmap);
+
+	/*
+	 * The virtual address, as well as the size of the mapping, must be
+	 * aligned (at least) to the size of the smallest page supported
+	 * by the hardware
+	 */
+	if (!IS_ALIGNED(iova | size, min_pagesz)) {
+		pr_err("unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%x\n",
+		       iova, size, min_pagesz);
+		return 0;
+	}
+
+	pr_debug("unmap this: iova 0x%lx size 0x%zx\n", iova, size);
+
+	/*
+	 * Keep iterating until we either unmap 'size' bytes (or more)
+	 * or we hit an area that isn't mapped.
+	 */
+	while (unmapped < size) {
+		size_t pgsize, count;
+
+		pgsize = iommu_pgsize(domain, iova, iova, size - unmapped, &count);
+		unmapped_page = ops->unmap_pages(domain, iova, pgsize, count, iotlb_gather);
+		if (!unmapped_page)
+			break;
+
+		pr_debug("unmapped: iova 0x%lx size 0x%zx\n",
+			 iova, unmapped_page);
+
+		iova += unmapped_page;
+		unmapped += unmapped_page;
+	}
+
+	trace_unmap(orig_iova, size, unmapped);
+	return unmapped;
+}
+
+/**
+ * iommu_unmap() - Remove mappings from a range of IOVA
+ * @domain: Domain to manipulate
+ * @iova: IO virtual address to start
+ * @size: Length of the range starting from @iova
+ *
+ * iommu_unmap() will remove a translation created by iommu_map(). It cannot
+ * subdivide a mapping created by iommu_map(), so it should be called with IOVA
+ * ranges that match what was passed to iommu_map(). The range can aggregate
+ * contiguous iommu_map() calls so long as no individual range is split.
+ *
+ * Returns: Number of bytes of IOVA unmapped. iova + res will be the point
+ * unmapping stopped.
+ */
+size_t iommu_unmap(struct iommu_domain *domain,
+		   unsigned long iova, size_t size)
+{
+	struct iommu_iotlb_gather iotlb_gather;
+	size_t ret;
+
+	iommu_iotlb_gather_init(&iotlb_gather);
+	ret = __iommu_unmap(domain, iova, size, &iotlb_gather);
+	iommu_iotlb_sync(domain, &iotlb_gather);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_unmap);
+
+/**
+ * iommu_unmap_fast() - Remove mappings from a range of IOVA without IOTLB sync
+ * @domain: Domain to manipulate
+ * @iova: IO virtual address to start
+ * @size: Length of the range starting from @iova
+ * @iotlb_gather: range information for a pending IOTLB flush
+ *
+ * iommu_unmap_fast() will remove a translation created by iommu_map().
+ * It can't subdivide a mapping created by iommu_map(), so it should be
+ * called with IOVA ranges that match what was passed to iommu_map(). The
+ * range can aggregate contiguous iommu_map() calls so long as no individual
+ * range is split.
+ *
+ * Basically iommu_unmap_fast() is the same as iommu_unmap() but for callers
+ * which manage the IOTLB flushing externally to perform a batched sync.
+ *
+ * Returns: Number of bytes of IOVA unmapped. iova + res will be the point
+ * unmapping stopped.
+ */
+size_t iommu_unmap_fast(struct iommu_domain *domain,
+			unsigned long iova, size_t size,
+			struct iommu_iotlb_gather *iotlb_gather)
+{
+	return __iommu_unmap(domain, iova, size, iotlb_gather);
+}
+EXPORT_SYMBOL_GPL(iommu_unmap_fast);
+
+ssize_t iommu_map_sg(struct iommu_domain *domain, unsigned long iova,
+		     struct scatterlist *sg, unsigned int nents, int prot,
+		     gfp_t gfp)
+{
+	size_t len = 0, mapped = 0;
+	phys_addr_t start;
+	unsigned int i = 0;
+	int ret;
+
+	while (i <= nents) {
+		phys_addr_t s_phys = sg_phys(sg);
+
+		if (len && s_phys != start + len) {
+			ret = iommu_map_nosync(domain, iova + mapped, start,
+					len, prot, gfp);
+			if (ret)
+				goto out_err;
+
+			mapped += len;
+			len = 0;
+		}
+
+		if (sg_dma_is_bus_address(sg))
+			goto next;
+
+		if (len) {
+			len += sg->length;
+		} else {
+			len = sg->length;
+			start = s_phys;
+		}
+
+next:
+		if (++i < nents)
+			sg = sg_next(sg);
+	}
+
+	ret = iommu_sync_map(domain, iova, mapped);
+	if (ret)
+		goto out_err;
+
+	return mapped;
+
+out_err:
+	/* undo mappings already done */
+	iommu_unmap(domain, iova, mapped);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_map_sg);
+
+/**
+ * report_iommu_fault() - report about an IOMMU fault to the IOMMU framework
+ * @domain: the iommu domain where the fault has happened
+ * @dev: the device where the fault has happened
+ * @iova: the faulting address
+ * @flags: mmu fault flags (e.g. IOMMU_FAULT_READ/IOMMU_FAULT_WRITE/...)
+ *
+ * This function should be called by the low-level IOMMU implementations
+ * whenever IOMMU faults happen, to allow high-level users, that are
+ * interested in such events, to know about them.
+ *
+ * This event may be useful for several possible use cases:
+ * - mere logging of the event
+ * - dynamic TLB/PTE loading
+ * - if restarting of the faulting device is required
+ *
+ * Returns 0 on success and an appropriate error code otherwise (if dynamic
+ * PTE/TLB loading will one day be supported, implementations will be able
+ * to tell whether it succeeded or not according to this return value).
+ *
+ * Specifically, -ENOSYS is returned if a fault handler isn't installed
+ * (though fault handlers can also return -ENOSYS, in case they want to
+ * elicit the default behavior of the IOMMU drivers).
+ */
+int report_iommu_fault(struct iommu_domain *domain, struct device *dev,
+		       unsigned long iova, int flags)
+{
+	int ret = -ENOSYS;
+
+	/*
+	 * if upper layers showed interest and installed a fault handler,
+	 * invoke it.
+	 */
+	if (domain->cookie_type == IOMMU_COOKIE_FAULT_HANDLER &&
+	    domain->handler)
+		ret = domain->handler(domain, dev, iova, flags,
+						domain->handler_token);
+
+	trace_io_page_fault(dev, iova, flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(report_iommu_fault);
+
+static int __init iommu_init(void)
+{
+	iommu_group_kset = kset_create_and_add("iommu_groups",
+					       NULL, kernel_kobj);
+	BUG_ON(!iommu_group_kset);
+
+	iommu_debugfs_setup();
+
+	return 0;
+}
+core_initcall(iommu_init);
+
+int iommu_set_pgtable_quirks(struct iommu_domain *domain,
+		unsigned long quirk)
+{
+	if (domain->type != IOMMU_DOMAIN_UNMANAGED)
+		return -EINVAL;
+	if (!domain->ops->set_pgtable_quirks)
+		return -EINVAL;
+	return domain->ops->set_pgtable_quirks(domain, quirk);
+}
+EXPORT_SYMBOL_GPL(iommu_set_pgtable_quirks);
+
+/**
+ * iommu_get_resv_regions - get reserved regions
+ * @dev: device for which to get reserved regions
+ * @list: reserved region list for device
+ *
+ * This returns a list of reserved IOVA regions specific to this device.
+ * A domain user should not map IOVA in these ranges.
+ */
+void iommu_get_resv_regions(struct device *dev, struct list_head *list)
+{
+	const struct iommu_ops *ops = dev_iommu_ops(dev);
+
+	if (ops->get_resv_regions)
+		ops->get_resv_regions(dev, list);
+}
+EXPORT_SYMBOL_GPL(iommu_get_resv_regions);
+
+/**
+ * iommu_put_resv_regions - release reserved regions
+ * @dev: device for which to free reserved regions
+ * @list: reserved region list for device
+ *
+ * This releases a reserved region list acquired by iommu_get_resv_regions().
+ */
+void iommu_put_resv_regions(struct device *dev, struct list_head *list)
+{
+	struct iommu_resv_region *entry, *next;
+
+	list_for_each_entry_safe(entry, next, list, list) {
+		if (entry->free)
+			entry->free(dev, entry);
+		else
+			kfree(entry);
+	}
+}
+EXPORT_SYMBOL(iommu_put_resv_regions);
+
+struct iommu_resv_region *iommu_alloc_resv_region(phys_addr_t start,
+						  size_t length, int prot,
+						  enum iommu_resv_type type,
+						  gfp_t gfp)
+{
+	struct iommu_resv_region *region;
+
+	region = kzalloc(sizeof(*region), gfp);
+	if (!region)
+		return NULL;
+
+	INIT_LIST_HEAD(&region->list);
+	region->start = start;
+	region->length = length;
+	region->prot = prot;
+	region->type = type;
+	return region;
+}
+EXPORT_SYMBOL_GPL(iommu_alloc_resv_region);
+
+void iommu_set_default_passthrough(bool cmd_line)
+{
+	if (cmd_line)
+		iommu_cmd_line |= IOMMU_CMD_LINE_DMA_API;
+	iommu_def_domain_type = IOMMU_DOMAIN_IDENTITY;
+}
+
+void iommu_set_default_translated(bool cmd_line)
+{
+	if (cmd_line)
+		iommu_cmd_line |= IOMMU_CMD_LINE_DMA_API;
+	iommu_def_domain_type = IOMMU_DOMAIN_DMA;
+}
+
+bool iommu_default_passthrough(void)
+{
+	return iommu_def_domain_type == IOMMU_DOMAIN_IDENTITY;
+}
+EXPORT_SYMBOL_GPL(iommu_default_passthrough);
+
+static const struct iommu_device *iommu_from_fwnode(const struct fwnode_handle *fwnode)
+{
+	const struct iommu_device *iommu, *ret = NULL;
+
+	spin_lock(&iommu_device_lock);
+	list_for_each_entry(iommu, &iommu_device_list, list)
+		if (iommu->fwnode == fwnode) {
+			ret = iommu;
+			break;
+		}
+	spin_unlock(&iommu_device_lock);
+	return ret;
+}
+
+const struct iommu_ops *iommu_ops_from_fwnode(const struct fwnode_handle *fwnode)
+{
+	const struct iommu_device *iommu = iommu_from_fwnode(fwnode);
+
+	return iommu ? iommu->ops : NULL;
+}
+
+int iommu_fwspec_init(struct device *dev, struct fwnode_handle *iommu_fwnode)
+{
+	const struct iommu_device *iommu = iommu_from_fwnode(iommu_fwnode);
+	struct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);
+
+	if (!iommu)
+		return driver_deferred_probe_check_state(dev);
+	if (!dev->iommu && !READ_ONCE(iommu->ready))
+		return -EPROBE_DEFER;
+
+	if (fwspec)
+		return iommu->ops == iommu_fwspec_ops(fwspec) ? 0 : -EINVAL;
+
+	if (!dev_iommu_get(dev))
+		return -ENOMEM;
+
+	/* Preallocate for the overwhelmingly common case of 1 ID */
+	fwspec = kzalloc(struct_size(fwspec, ids, 1), GFP_KERNEL);
+	if (!fwspec)
+		return -ENOMEM;
+
+	fwnode_handle_get(iommu_fwnode);
+	fwspec->iommu_fwnode = iommu_fwnode;
+	dev_iommu_fwspec_set(dev, fwspec);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(iommu_fwspec_init);
+
+void iommu_fwspec_free(struct device *dev)
+{
+	struct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);
+
+	if (fwspec) {
+		fwnode_handle_put(fwspec->iommu_fwnode);
+		kfree(fwspec);
+		dev_iommu_fwspec_set(dev, NULL);
+	}
+}
+
+int iommu_fwspec_add_ids(struct device *dev, const u32 *ids, int num_ids)
+{
+	struct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);
+	int i, new_num;
+
+	if (!fwspec)
+		return -EINVAL;
+
+	new_num = fwspec->num_ids + num_ids;
+	if (new_num > 1) {
+		fwspec = krealloc(fwspec, struct_size(fwspec, ids, new_num),
+				  GFP_KERNEL);
+		if (!fwspec)
+			return -ENOMEM;
+
+		dev_iommu_fwspec_set(dev, fwspec);
+	}
+
+	for (i = 0; i < num_ids; i++)
+		fwspec->ids[fwspec->num_ids + i] = ids[i];
+
+	fwspec->num_ids = new_num;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(iommu_fwspec_add_ids);
+
+/**
+ * iommu_setup_default_domain - Set the default_domain for the group
+ * @group: Group to change
+ * @target_type: Domain type to set as the default_domain
+ *
+ * Allocate a default domain and set it as the current domain on the group. If
+ * the group already has a default domain it will be changed to the target_type.
+ * When target_type is 0 the default domain is selected based on driver and
+ * system preferences.
+ */
+static int iommu_setup_default_domain(struct iommu_group *group,
+				      int target_type)
+{
+	struct iommu_domain *old_dom = group->default_domain;
+	struct group_device *gdev;
+	struct iommu_domain *dom;
+	bool direct_failed;
+	int req_type;
+	int ret;
+
+	lockdep_assert_held(&group->mutex);
+
+	req_type = iommu_get_default_domain_type(group, target_type);
+	if (req_type < 0)
+		return -EINVAL;
+
+	dom = iommu_group_alloc_default_domain(group, req_type);
+	if (IS_ERR(dom))
+		return PTR_ERR(dom);
+
+	if (group->default_domain == dom)
+		return 0;
+
+	if (iommu_is_dma_domain(dom)) {
+		ret = iommu_get_dma_cookie(dom);
+		if (ret) {
+			iommu_domain_free(dom);
+			return ret;
+		}
+	}
+
+	/*
+	 * IOMMU_RESV_DIRECT and IOMMU_RESV_DIRECT_RELAXABLE regions must be
+	 * mapped before their device is attached, in order to guarantee
+	 * continuity with any FW activity
+	 */
+	direct_failed = false;
+	for_each_group_device(group, gdev) {
+		if (iommu_create_device_direct_mappings(dom, gdev->dev)) {
+			direct_failed = true;
+			dev_warn_once(
+				gdev->dev->iommu->iommu_dev->dev,
+				"IOMMU driver was not able to establish FW requested direct mapping.");
+		}
+	}
+
+	/* We must set default_domain early for __iommu_device_set_domain */
+	group->default_domain = dom;
+	if (!group->domain) {
+		/*
+		 * Drivers are not allowed to fail the first domain attach.
+		 * The only way to recover from this is to fail attaching the
+		 * iommu driver and call ops->release_device. Put the domain
+		 * in group->default_domain so it is freed after.
+		 */
+		ret = __iommu_group_set_domain_internal(
+			group, dom, IOMMU_SET_DOMAIN_MUST_SUCCEED);
+		if (WARN_ON(ret))
+			goto out_free_old;
+	} else {
+		ret = __iommu_group_set_domain(group, dom);
+		if (ret)
+			goto err_restore_def_domain;
+	}
+
+	/*
+	 * Drivers are supposed to allow mappings to be installed in a domain
+	 * before device attachment, but some don't. Hack around this defect by
+	 * trying again after attaching. If this happens it means the device
+	 * will not continuously have the IOMMU_RESV_DIRECT map.
+	 */
+	if (direct_failed) {
+		for_each_group_device(group, gdev) {
+			ret = iommu_create_device_direct_mappings(dom, gdev->dev);
+			if (ret)
+				goto err_restore_domain;
+		}
+	}
+
+out_free_old:
+	if (old_dom)
+		iommu_domain_free(old_dom);
+	return ret;
+
+err_restore_domain:
+	if (old_dom)
+		__iommu_group_set_domain_internal(
+			group, old_dom, IOMMU_SET_DOMAIN_MUST_SUCCEED);
+err_restore_def_domain:
+	if (old_dom) {
+		iommu_domain_free(dom);
+		group->default_domain = old_dom;
+	}
+	return ret;
+}
+
+/*
+ * Changing the default domain through sysfs requires the users to unbind the
+ * drivers from the devices in the iommu group, except for a DMA -> DMA-FQ
+ * transition. Return failure if this isn't met.
+ *
+ * We need to consider the race between this and the device release path.
+ * group->mutex is used here to guarantee that the device release path
+ * will not be entered at the same time.
+ */
+static ssize_t iommu_group_store_type(struct iommu_group *group,
+				      const char *buf, size_t count)
+{
+	struct group_device *gdev;
+	int ret, req_type;
+
+	if (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))
+		return -EACCES;
+
+	if (WARN_ON(!group) || !group->default_domain)
+		return -EINVAL;
+
+	if (sysfs_streq(buf, "identity"))
+		req_type = IOMMU_DOMAIN_IDENTITY;
+	else if (sysfs_streq(buf, "DMA"))
+		req_type = IOMMU_DOMAIN_DMA;
+	else if (sysfs_streq(buf, "DMA-FQ"))
+		req_type = IOMMU_DOMAIN_DMA_FQ;
+	else if (sysfs_streq(buf, "auto"))
+		req_type = 0;
+	else
+		return -EINVAL;
+
+	mutex_lock(&group->mutex);
+	/* We can bring up a flush queue without tearing down the domain. */
+	if (req_type == IOMMU_DOMAIN_DMA_FQ &&
+	    group->default_domain->type == IOMMU_DOMAIN_DMA) {
+		ret = iommu_dma_init_fq(group->default_domain);
+		if (ret)
+			goto out_unlock;
+
+		group->default_domain->type = IOMMU_DOMAIN_DMA_FQ;
+		ret = count;
+		goto out_unlock;
+	}
+
+	/* Otherwise, ensure that device exists and no driver is bound. */
+	if (list_empty(&group->devices) || group->owner_cnt) {
+		ret = -EPERM;
+		goto out_unlock;
+	}
+
+	ret = iommu_setup_default_domain(group, req_type);
+	if (ret)
+		goto out_unlock;
+
+	/* Make sure dma_ops is appropriatley set */
+	for_each_group_device(group, gdev)
+		iommu_setup_dma_ops(gdev->dev);
+
+out_unlock:
+	mutex_unlock(&group->mutex);
+	return ret ?: count;
+}
+
+/**
+ * iommu_device_use_default_domain() - Device driver wants to handle device
+ *                                     DMA through the kernel DMA API.
+ * @dev: The device.
+ *
+ * The device driver about to bind @dev wants to do DMA through the kernel
+ * DMA API. Return 0 if it is allowed, otherwise an error.
+ */
+int iommu_device_use_default_domain(struct device *dev)
+{
+	/* Caller is the driver core during the pre-probe path */
+	struct iommu_group *group = dev->iommu_group;
+	int ret = 0;
+
+	if (!group)
+		return 0;
+
+	mutex_lock(&group->mutex);
+	/* We may race against bus_iommu_probe() finalising groups here */
+	if (!group->default_domain) {
+		ret = -EPROBE_DEFER;
+		goto unlock_out;
+	}
+	if (group->owner_cnt) {
+		if (group->domain != group->default_domain || group->owner ||
+		    !xa_empty(&group->pasid_array)) {
+			ret = -EBUSY;
+			goto unlock_out;
+		}
+	}
+
+	group->owner_cnt++;
+
+unlock_out:
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+
+/**
+ * iommu_device_unuse_default_domain() - Device driver stops handling device
+ *                                       DMA through the kernel DMA API.
+ * @dev: The device.
+ *
+ * The device driver doesn't want to do DMA through kernel DMA API anymore.
+ * It must be called after iommu_device_use_default_domain().
+ */
+void iommu_device_unuse_default_domain(struct device *dev)
+{
+	/* Caller is the driver core during the post-probe path */
+	struct iommu_group *group = dev->iommu_group;
+
+	if (!group)
+		return;
+
+	mutex_lock(&group->mutex);
+	if (!WARN_ON(!group->owner_cnt || !xa_empty(&group->pasid_array)))
+		group->owner_cnt--;
+
+	mutex_unlock(&group->mutex);
+}
+
+static int __iommu_group_alloc_blocking_domain(struct iommu_group *group)
+{
+	struct device *dev = iommu_group_first_dev(group);
+	const struct iommu_ops *ops = dev_iommu_ops(dev);
+	struct iommu_domain *domain;
+
+	if (group->blocking_domain)
+		return 0;
+
+	if (ops->blocked_domain) {
+		group->blocking_domain = ops->blocked_domain;
+		return 0;
+	}
+
+	/*
+	 * For drivers that do not yet understand IOMMU_DOMAIN_BLOCKED create an
+	 * empty PAGING domain instead.
+	 */
+	domain = iommu_paging_domain_alloc(dev);
+	if (IS_ERR(domain))
+		return PTR_ERR(domain);
+	group->blocking_domain = domain;
+	return 0;
+}
+
+static int __iommu_take_dma_ownership(struct iommu_group *group, void *owner)
+{
+	int ret;
+
+	if ((group->domain && group->domain != group->default_domain) ||
+	    !xa_empty(&group->pasid_array))
+		return -EBUSY;
+
+	ret = __iommu_group_alloc_blocking_domain(group);
+	if (ret)
+		return ret;
+	ret = __iommu_group_set_domain(group, group->blocking_domain);
+	if (ret)
+		return ret;
+
+	group->owner = owner;
+	group->owner_cnt++;
+	return 0;
+}
+
+/**
+ * iommu_group_claim_dma_owner() - Set DMA ownership of a group
+ * @group: The group.
+ * @owner: Caller specified pointer. Used for exclusive ownership.
+ *
+ * This is to support backward compatibility for vfio which manages the dma
+ * ownership in iommu_group level. New invocations on this interface should be
+ * prohibited. Only a single owner may exist for a group.
+ */
+int iommu_group_claim_dma_owner(struct iommu_group *group, void *owner)
+{
+	int ret = 0;
+
+	if (WARN_ON(!owner))
+		return -EINVAL;
+
+	mutex_lock(&group->mutex);
+	if (group->owner_cnt) {
+		ret = -EPERM;
+		goto unlock_out;
+	}
+
+	ret = __iommu_take_dma_ownership(group, owner);
+unlock_out:
+	mutex_unlock(&group->mutex);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_group_claim_dma_owner);
+
+/**
+ * iommu_device_claim_dma_owner() - Set DMA ownership of a device
+ * @dev: The device.
+ * @owner: Caller specified pointer. Used for exclusive ownership.
+ *
+ * Claim the DMA ownership of a device. Multiple devices in the same group may
+ * concurrently claim ownership if they present the same owner value. Returns 0
+ * on success and error code on failure
+ */
+int iommu_device_claim_dma_owner(struct device *dev, void *owner)
+{
+	/* Caller must be a probed driver on dev */
+	struct iommu_group *group = dev->iommu_group;
+	int ret = 0;
+
+	if (WARN_ON(!owner))
+		return -EINVAL;
+
+	if (!group)
+		return -ENODEV;
+
+	mutex_lock(&group->mutex);
+	if (group->owner_cnt) {
+		if (group->owner != owner) {
+			ret = -EPERM;
+			goto unlock_out;
+		}
+		group->owner_cnt++;
+		goto unlock_out;
+	}
+
+	ret = __iommu_take_dma_ownership(group, owner);
+unlock_out:
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_device_claim_dma_owner);
+
+static void __iommu_release_dma_ownership(struct iommu_group *group)
+{
+	if (WARN_ON(!group->owner_cnt || !group->owner ||
+		    !xa_empty(&group->pasid_array)))
+		return;
+
+	group->owner_cnt = 0;
+	group->owner = NULL;
+	__iommu_group_set_domain_nofail(group, group->default_domain);
+}
+
+/**
+ * iommu_group_release_dma_owner() - Release DMA ownership of a group
+ * @group: The group
+ *
+ * Release the DMA ownership claimed by iommu_group_claim_dma_owner().
+ */
+void iommu_group_release_dma_owner(struct iommu_group *group)
+{
+	mutex_lock(&group->mutex);
+	__iommu_release_dma_ownership(group);
+	mutex_unlock(&group->mutex);
+}
+EXPORT_SYMBOL_GPL(iommu_group_release_dma_owner);
+
+/**
+ * iommu_device_release_dma_owner() - Release DMA ownership of a device
+ * @dev: The device.
+ *
+ * Release the DMA ownership claimed by iommu_device_claim_dma_owner().
+ */
+void iommu_device_release_dma_owner(struct device *dev)
+{
+	/* Caller must be a probed driver on dev */
+	struct iommu_group *group = dev->iommu_group;
+
+	mutex_lock(&group->mutex);
+	if (group->owner_cnt > 1)
+		group->owner_cnt--;
+	else
+		__iommu_release_dma_ownership(group);
+	mutex_unlock(&group->mutex);
+}
+EXPORT_SYMBOL_GPL(iommu_device_release_dma_owner);
+
+/**
+ * iommu_group_dma_owner_claimed() - Query group dma ownership status
+ * @group: The group.
+ *
+ * This provides status query on a given group. It is racy and only for
+ * non-binding status reporting.
+ */
+bool iommu_group_dma_owner_claimed(struct iommu_group *group)
+{
+	unsigned int user;
+
+	mutex_lock(&group->mutex);
+	user = group->owner_cnt;
+	mutex_unlock(&group->mutex);
+
+	return user;
+}
+EXPORT_SYMBOL_GPL(iommu_group_dma_owner_claimed);
+
+static void iommu_remove_dev_pasid(struct device *dev, ioasid_t pasid,
+				   struct iommu_domain *domain)
+{
+	const struct iommu_ops *ops = dev_iommu_ops(dev);
+	struct iommu_domain *blocked_domain = ops->blocked_domain;
+
+	WARN_ON(blocked_domain->ops->set_dev_pasid(blocked_domain,
+						   dev, pasid, domain));
+}
+
+static int __iommu_set_group_pasid(struct iommu_domain *domain,
+				   struct iommu_group *group, ioasid_t pasid,
+				   struct iommu_domain *old)
+{
+	struct group_device *device, *last_gdev;
+	int ret;
+
+	for_each_group_device(group, device) {
+		if (device->dev->iommu->max_pasids > 0) {
+			ret = domain->ops->set_dev_pasid(domain, device->dev,
+							 pasid, old);
+			if (ret)
+				goto err_revert;
+		}
+	}
+
+	return 0;
+
+err_revert:
+	last_gdev = device;
+	for_each_group_device(group, device) {
+		if (device == last_gdev)
+			break;
+		if (device->dev->iommu->max_pasids > 0) {
+			/*
+			 * If no old domain, undo the succeeded devices/pasid.
+			 * Otherwise, rollback the succeeded devices/pasid to
+			 * the old domain. And it is a driver bug to fail
+			 * attaching with a previously good domain.
+			 */
+			if (!old ||
+			    WARN_ON(old->ops->set_dev_pasid(old, device->dev,
+							    pasid, domain)))
+				iommu_remove_dev_pasid(device->dev, pasid, domain);
+		}
+	}
+	return ret;
+}
+
+static void __iommu_remove_group_pasid(struct iommu_group *group,
+				       ioasid_t pasid,
+				       struct iommu_domain *domain)
+{
+	struct group_device *device;
+
+	for_each_group_device(group, device) {
+		if (device->dev->iommu->max_pasids > 0)
+			iommu_remove_dev_pasid(device->dev, pasid, domain);
+	}
+}
+
+/*
+ * iommu_attach_device_pasid() - Attach a domain to pasid of device
+ * @domain: the iommu domain.
+ * @dev: the attached device.
+ * @pasid: the pasid of the device.
+ * @handle: the attach handle.
+ *
+ * Caller should always provide a new handle to avoid race with the paths
+ * that have lockless reference to handle if it intends to pass a valid handle.
+ *
+ * Return: 0 on success, or an error.
+ */
+int iommu_attach_device_pasid(struct iommu_domain *domain,
+			      struct device *dev, ioasid_t pasid,
+			      struct iommu_attach_handle *handle)
+{
+	/* Caller must be a probed driver on dev */
+	struct iommu_group *group = dev->iommu_group;
+	struct group_device *device;
+	const struct iommu_ops *ops;
+	void *entry;
+	int ret;
+
+	if (!group)
+		return -ENODEV;
+
+	ops = dev_iommu_ops(dev);
+
+	if (!domain->ops->set_dev_pasid ||
+	    !ops->blocked_domain ||
+	    !ops->blocked_domain->ops->set_dev_pasid)
+		return -EOPNOTSUPP;
+
+	if (!domain_iommu_ops_compatible(ops, domain) ||
+	    pasid == IOMMU_NO_PASID)
+		return -EINVAL;
+
+	mutex_lock(&group->mutex);
+	for_each_group_device(group, device) {
+		/*
+		 * Skip PASID validation for devices without PASID support
+		 * (max_pasids = 0). These devices cannot issue transactions
+		 * with PASID, so they don't affect group's PASID usage.
+		 */
+		if ((device->dev->iommu->max_pasids > 0) &&
+		    (pasid >= device->dev->iommu->max_pasids)) {
+			ret = -EINVAL;
+			goto out_unlock;
+		}
+	}
+
+	entry = iommu_make_pasid_array_entry(domain, handle);
+
+	/*
+	 * Entry present is a failure case. Use xa_insert() instead of
+	 * xa_reserve().
+	 */
+	ret = xa_insert(&group->pasid_array, pasid, XA_ZERO_ENTRY, GFP_KERNEL);
+	if (ret)
+		goto out_unlock;
+
+	ret = __iommu_set_group_pasid(domain, group, pasid, NULL);
+	if (ret) {
+		xa_release(&group->pasid_array, pasid);
+		goto out_unlock;
+	}
+
+	/*
+	 * The xa_insert() above reserved the memory, and the group->mutex is
+	 * held, this cannot fail. The new domain cannot be visible until the
+	 * operation succeeds as we cannot tolerate PRIs becoming concurrently
+	 * queued and then failing attach.
+	 */
+	WARN_ON(xa_is_err(xa_store(&group->pasid_array,
+				   pasid, entry, GFP_KERNEL)));
+
+out_unlock:
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(iommu_attach_device_pasid);
+
+/**
+ * iommu_replace_device_pasid - Replace the domain that a specific pasid
+ *                              of the device is attached to
+ * @domain: the new iommu domain
+ * @dev: the attached device.
+ * @pasid: the pasid of the device.
+ * @handle: the attach handle.
+ *
+ * This API allows the pasid to switch domains. The @pasid should have been
+ * attached. Otherwise, this fails. The pasid will keep the old configuration
+ * if replacement failed.
+ *
+ * Caller should always provide a new handle to avoid race with the paths
+ * that have lockless reference to handle if it intends to pass a valid handle.
+ *
+ * Return 0 on success, or an error.
+ */
+int iommu_replace_device_pasid(struct iommu_domain *domain,
+			       struct device *dev, ioasid_t pasid,
+			       struct iommu_attach_handle *handle)
+{
+	/* Caller must be a probed driver on dev */
+	struct iommu_group *group = dev->iommu_group;
+	struct iommu_attach_handle *entry;
+	struct iommu_domain *curr_domain;
+	void *curr;
+	int ret;
+
+	if (!group)
+		return -ENODEV;
+
+	if (!domain->ops->set_dev_pasid)
+		return -EOPNOTSUPP;
+
+	if (!domain_iommu_ops_compatible(dev_iommu_ops(dev), domain) ||
+	    pasid == IOMMU_NO_PASID || !handle)
+		return -EINVAL;
+
+	mutex_lock(&group->mutex);
+	entry = iommu_make_pasid_array_entry(domain, handle);
+	curr = xa_cmpxchg(&group->pasid_array, pasid, NULL,
+			  XA_ZERO_ENTRY, GFP_KERNEL);
+	if (xa_is_err(curr)) {
+		ret = xa_err(curr);
+		goto out_unlock;
+	}
+
+	/*
+	 * No domain (with or without handle) attached, hence not
+	 * a replace case.
+	 */
+	if (!curr) {
+		xa_release(&group->pasid_array, pasid);
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
+	/*
+	 * Reusing handle is problematic as there are paths that refers
+	 * the handle without lock. To avoid race, reject the callers that
+	 * attempt it.
+	 */
+	if (curr == entry) {
+		WARN_ON(1);
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
+	curr_domain = pasid_array_entry_to_domain(curr);
+	ret = 0;
+
+	if (curr_domain != domain) {
+		ret = __iommu_set_group_pasid(domain, group,
+					      pasid, curr_domain);
+		if (ret)
+			goto out_unlock;
+	}
+
+	/*
+	 * The above xa_cmpxchg() reserved the memory, and the
+	 * group->mutex is held, this cannot fail.
+	 */
+	WARN_ON(xa_is_err(xa_store(&group->pasid_array,
+				   pasid, entry, GFP_KERNEL)));
+
+out_unlock:
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+EXPORT_SYMBOL_NS_GPL(iommu_replace_device_pasid, "IOMMUFD_INTERNAL");
+
+/*
+ * iommu_detach_device_pasid() - Detach the domain from pasid of device
+ * @domain: the iommu domain.
+ * @dev: the attached device.
+ * @pasid: the pasid of the device.
+ *
+ * The @domain must have been attached to @pasid of the @dev with
+ * iommu_attach_device_pasid().
+ */
+void iommu_detach_device_pasid(struct iommu_domain *domain, struct device *dev,
+			       ioasid_t pasid)
+{
+	/* Caller must be a probed driver on dev */
+	struct iommu_group *group = dev->iommu_group;
+
+	mutex_lock(&group->mutex);
+	__iommu_remove_group_pasid(group, pasid, domain);
+	xa_erase(&group->pasid_array, pasid);
+	mutex_unlock(&group->mutex);
+}
+EXPORT_SYMBOL_GPL(iommu_detach_device_pasid);
+
+ioasid_t iommu_alloc_global_pasid(struct device *dev)
+{
+	int ret;
+
+	/* max_pasids == 0 means that the device does not support PASID */
+	if (!dev->iommu->max_pasids)
+		return IOMMU_PASID_INVALID;
+
+	/*
+	 * max_pasids is set up by vendor driver based on number of PASID bits
+	 * supported but the IDA allocation is inclusive.
+	 */
+	ret = ida_alloc_range(&iommu_global_pasid_ida, IOMMU_FIRST_GLOBAL_PASID,
+			      dev->iommu->max_pasids - 1, GFP_KERNEL);
+	return ret < 0 ? IOMMU_PASID_INVALID : ret;
+}
+EXPORT_SYMBOL_GPL(iommu_alloc_global_pasid);
+
+void iommu_free_global_pasid(ioasid_t pasid)
+{
+	if (WARN_ON(pasid == IOMMU_PASID_INVALID))
+		return;
+
+	ida_free(&iommu_global_pasid_ida, pasid);
+}
+EXPORT_SYMBOL_GPL(iommu_free_global_pasid);
+
+/**
+ * iommu_attach_handle_get - Return the attach handle
+ * @group: the iommu group that domain was attached to
+ * @pasid: the pasid within the group
+ * @type: matched domain type, 0 for any match
+ *
+ * Return handle or ERR_PTR(-ENOENT) on none, ERR_PTR(-EBUSY) on mismatch.
+ *
+ * Return the attach handle to the caller. The life cycle of an iommu attach
+ * handle is from the time when the domain is attached to the time when the
+ * domain is detached. Callers are required to synchronize the call of
+ * iommu_attach_handle_get() with domain attachment and detachment. The attach
+ * handle can only be used during its life cycle.
+ */
+struct iommu_attach_handle *
+iommu_attach_handle_get(struct iommu_group *group, ioasid_t pasid, unsigned int type)
+{
+	struct iommu_attach_handle *handle;
+	void *entry;
+
+	xa_lock(&group->pasid_array);
+	entry = xa_load(&group->pasid_array, pasid);
+	if (!entry || xa_pointer_tag(entry) != IOMMU_PASID_ARRAY_HANDLE) {
+		handle = ERR_PTR(-ENOENT);
+	} else {
+		handle = xa_untag_pointer(entry);
+		if (type && handle->domain->type != type)
+			handle = ERR_PTR(-EBUSY);
+	}
+	xa_unlock(&group->pasid_array);
+
+	return handle;
+}
+EXPORT_SYMBOL_NS_GPL(iommu_attach_handle_get, "IOMMUFD_INTERNAL");
+
+/**
+ * iommu_attach_group_handle - Attach an IOMMU domain to an IOMMU group
+ * @domain: IOMMU domain to attach
+ * @group: IOMMU group that will be attached
+ * @handle: attach handle
+ *
+ * Returns 0 on success and error code on failure.
+ *
+ * This is a variant of iommu_attach_group(). It allows the caller to provide
+ * an attach handle and use it when the domain is attached. This is currently
+ * used by IOMMUFD to deliver the I/O page faults.
+ *
+ * Caller should always provide a new handle to avoid race with the paths
+ * that have lockless reference to handle.
+ */
+int iommu_attach_group_handle(struct iommu_domain *domain,
+			      struct iommu_group *group,
+			      struct iommu_attach_handle *handle)
+{
+	void *entry;
+	int ret;
+
+	if (!handle)
+		return -EINVAL;
+
+	mutex_lock(&group->mutex);
+	entry = iommu_make_pasid_array_entry(domain, handle);
+	ret = xa_insert(&group->pasid_array,
+			IOMMU_NO_PASID, XA_ZERO_ENTRY, GFP_KERNEL);
+	if (ret)
+		goto out_unlock;
+
+	ret = __iommu_attach_group(domain, group);
+	if (ret) {
+		xa_release(&group->pasid_array, IOMMU_NO_PASID);
+		goto out_unlock;
+	}
+
+	/*
+	 * The xa_insert() above reserved the memory, and the group->mutex is
+	 * held, this cannot fail. The new domain cannot be visible until the
+	 * operation succeeds as we cannot tolerate PRIs becoming concurrently
+	 * queued and then failing attach.
+	 */
+	WARN_ON(xa_is_err(xa_store(&group->pasid_array,
+				   IOMMU_NO_PASID, entry, GFP_KERNEL)));
+
+out_unlock:
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+EXPORT_SYMBOL_NS_GPL(iommu_attach_group_handle, "IOMMUFD_INTERNAL");
+
+/**
+ * iommu_detach_group_handle - Detach an IOMMU domain from an IOMMU group
+ * @domain: IOMMU domain to attach
+ * @group: IOMMU group that will be attached
+ *
+ * Detach the specified IOMMU domain from the specified IOMMU group.
+ * It must be used in conjunction with iommu_attach_group_handle().
+ */
+void iommu_detach_group_handle(struct iommu_domain *domain,
+			       struct iommu_group *group)
+{
+	mutex_lock(&group->mutex);
+	__iommu_group_set_core_domain(group);
+	xa_erase(&group->pasid_array, IOMMU_NO_PASID);
+	mutex_unlock(&group->mutex);
+}
+EXPORT_SYMBOL_NS_GPL(iommu_detach_group_handle, "IOMMUFD_INTERNAL");
+
+/**
+ * iommu_replace_group_handle - replace the domain that a group is attached to
+ * @group: IOMMU group that will be attached to the new domain
+ * @new_domain: new IOMMU domain to replace with
+ * @handle: attach handle
+ *
+ * This API allows the group to switch domains without being forced to go to
+ * the blocking domain in-between. It allows the caller to provide an attach
+ * handle for the new domain and use it when the domain is attached.
+ *
+ * If the currently attached domain is a core domain (e.g. a default_domain),
+ * it will act just like the iommu_attach_group_handle().
+ *
+ * Caller should always provide a new handle to avoid race with the paths
+ * that have lockless reference to handle.
+ */
+int iommu_replace_group_handle(struct iommu_group *group,
+			       struct iommu_domain *new_domain,
+			       struct iommu_attach_handle *handle)
+{
+	void *curr, *entry;
+	int ret;
+
+	if (!new_domain || !handle)
+		return -EINVAL;
+
+	mutex_lock(&group->mutex);
+	entry = iommu_make_pasid_array_entry(new_domain, handle);
+	ret = xa_reserve(&group->pasid_array, IOMMU_NO_PASID, GFP_KERNEL);
+	if (ret)
+		goto err_unlock;
+
+	ret = __iommu_group_set_domain(group, new_domain);
+	if (ret)
+		goto err_release;
+
+	curr = xa_store(&group->pasid_array, IOMMU_NO_PASID, entry, GFP_KERNEL);
+	WARN_ON(xa_is_err(curr));
+
+	mutex_unlock(&group->mutex);
+
+	return 0;
+err_release:
+	xa_release(&group->pasid_array, IOMMU_NO_PASID);
+err_unlock:
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+EXPORT_SYMBOL_NS_GPL(iommu_replace_group_handle, "IOMMUFD_INTERNAL");
+
+#if IS_ENABLED(CONFIG_IRQ_MSI_IOMMU)
+/**
+ * iommu_dma_prepare_msi() - Map the MSI page in the IOMMU domain
+ * @desc: MSI descriptor, will store the MSI page
+ * @msi_addr: MSI target address to be mapped
+ *
+ * The implementation of sw_msi() should take msi_addr and map it to
+ * an IOVA in the domain and call msi_desc_set_iommu_msi_iova() with the
+ * mapping information.
+ *
+ * Return: 0 on success or negative error code if the mapping failed.
+ */
+int iommu_dma_prepare_msi(struct msi_desc *desc, phys_addr_t msi_addr)
+{
+	struct device *dev = msi_desc_to_dev(desc);
+	struct iommu_group *group = dev->iommu_group;
+	int ret = 0;
+
+	if (!group)
+		return 0;
+
+	mutex_lock(&group->mutex);
+	/* An IDENTITY domain must pass through */
+	if (group->domain && group->domain->type != IOMMU_DOMAIN_IDENTITY) {
+		switch (group->domain->cookie_type) {
+		case IOMMU_COOKIE_DMA_MSI:
+		case IOMMU_COOKIE_DMA_IOVA:
+			ret = iommu_dma_sw_msi(group->domain, desc, msi_addr);
+			break;
+		case IOMMU_COOKIE_IOMMUFD:
+			ret = iommufd_sw_msi(group->domain, desc, msi_addr);
+			break;
+		default:
+			ret = -EOPNOTSUPP;
+			break;
+		}
+	}
+	mutex_unlock(&group->mutex);
+	return ret;
+}
+#endif /* CONFIG_IRQ_MSI_IOMMU */
diff -urN a/drivers/iommu/rockchip-iommu.c bb/drivers/iommu/rockchip-iommu.c
--- a/drivers/iommu/rockchip-iommu.c	1969-12-31 16:00:00
+++ bb/drivers/iommu/rockchip-iommu.c	2026-02-07 23:38:39
@@ -0,0 +1,1387 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * IOMMU API for Rockchip
+ *
+ * Module Authors:	Simon Xue <xxm@rock-chips.com>
+ *			Daniel Kurtz <djkurtz@chromium.org>
+ */
+
+#include <linux/clk.h>
+#include <linux/compiler.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/errno.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/iommu.h>
+#include <linux/iopoll.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/init.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/pm_runtime.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/string_choices.h>
+
+#include "dma-iommu.h"
+#include "iommu-pages.h"
+
+/** MMU register offsets */
+#define RK_MMU_DTE_ADDR		0x00	/* Directory table address */
+#define RK_MMU_STATUS		0x04
+#define RK_MMU_COMMAND		0x08
+#define RK_MMU_PAGE_FAULT_ADDR	0x0C	/* IOVA of last page fault */
+#define RK_MMU_ZAP_ONE_LINE	0x10	/* Shootdown one IOTLB entry */
+#define RK_MMU_INT_RAWSTAT	0x14	/* IRQ status ignoring mask */
+#define RK_MMU_INT_CLEAR	0x18	/* Acknowledge and re-arm irq */
+#define RK_MMU_INT_MASK		0x1C	/* IRQ enable */
+#define RK_MMU_INT_STATUS	0x20	/* IRQ status after masking */
+#define RK_MMU_AUTO_GATING	0x24
+
+#define DTE_ADDR_DUMMY		0xCAFEBABE
+
+#define RK_MMU_POLL_PERIOD_US		100
+#define RK_MMU_FORCE_RESET_TIMEOUT_US	100000
+#define RK_MMU_POLL_TIMEOUT_US		1000
+
+/* RK_MMU_STATUS fields */
+#define RK_MMU_STATUS_PAGING_ENABLED       BIT(0)
+#define RK_MMU_STATUS_PAGE_FAULT_ACTIVE    BIT(1)
+#define RK_MMU_STATUS_STALL_ACTIVE         BIT(2)
+#define RK_MMU_STATUS_IDLE                 BIT(3)
+#define RK_MMU_STATUS_REPLAY_BUFFER_EMPTY  BIT(4)
+#define RK_MMU_STATUS_PAGE_FAULT_IS_WRITE  BIT(5)
+#define RK_MMU_STATUS_STALL_NOT_ACTIVE     BIT(31)
+
+/* RK_MMU_COMMAND command values */
+#define RK_MMU_CMD_ENABLE_PAGING    0  /* Enable memory translation */
+#define RK_MMU_CMD_DISABLE_PAGING   1  /* Disable memory translation */
+#define RK_MMU_CMD_ENABLE_STALL     2  /* Stall paging to allow other cmds */
+#define RK_MMU_CMD_DISABLE_STALL    3  /* Stop stall re-enables paging */
+#define RK_MMU_CMD_ZAP_CACHE        4  /* Shoot down entire IOTLB */
+#define RK_MMU_CMD_PAGE_FAULT_DONE  5  /* Clear page fault */
+#define RK_MMU_CMD_FORCE_RESET      6  /* Reset all registers */
+
+/* RK_MMU_INT_* register fields */
+#define RK_MMU_IRQ_PAGE_FAULT    0x01  /* page fault */
+#define RK_MMU_IRQ_BUS_ERROR     0x02  /* bus read error */
+#define RK_MMU_IRQ_MASK          (RK_MMU_IRQ_PAGE_FAULT | RK_MMU_IRQ_BUS_ERROR)
+
+#define NUM_DT_ENTRIES 1024
+#define NUM_PT_ENTRIES 1024
+
+#define SPAGE_ORDER 12
+#define SPAGE_SIZE (1 << SPAGE_ORDER)
+
+ /*
+  * Support mapping any size that fits in one page table:
+  *   4 KiB to 4 MiB
+  */
+#define RK_IOMMU_PGSIZE_BITMAP 0x007ff000
+
+struct rk_iommu_domain {
+	struct list_head iommus;
+	u32 *dt; /* page directory table */
+	dma_addr_t dt_dma;
+	spinlock_t iommus_lock; /* lock for iommus list */
+	spinlock_t dt_lock; /* lock for modifying page directory table */
+	struct device *dma_dev;
+
+	struct iommu_domain domain;
+};
+
+/* list of clocks required by IOMMU */
+static const char * const rk_iommu_clocks[] = {
+	"aclk", "iface",
+};
+
+struct rk_iommu_ops {
+	phys_addr_t (*pt_address)(u32 dte);
+	u32 (*mk_dtentries)(dma_addr_t pt_dma);
+	u32 (*mk_ptentries)(phys_addr_t page, int prot);
+	u64 dma_bit_mask;
+	gfp_t gfp_flags;
+};
+
+struct rk_iommu {
+	struct device *dev;
+	void __iomem **bases;
+	int num_mmu;
+	int num_irq;
+	struct clk_bulk_data *clocks;
+	int num_clocks;
+	bool reset_disabled;
+	struct iommu_device iommu;
+	struct list_head node; /* entry in rk_iommu_domain.iommus */
+	struct iommu_domain *domain; /* domain to which iommu is attached */
+};
+
+struct rk_iommudata {
+	struct device_link *link; /* runtime PM link from IOMMU to master */
+	struct rk_iommu *iommu;
+};
+
+static const struct rk_iommu_ops *rk_ops;
+static struct iommu_domain rk_identity_domain;
+
+static inline void rk_table_flush(struct rk_iommu_domain *dom, dma_addr_t dma,
+				  unsigned int count)
+{
+	size_t size = count * sizeof(u32); /* count of u32 entry */
+
+	dma_sync_single_for_device(dom->dma_dev, dma, size, DMA_TO_DEVICE);
+}
+
+static struct rk_iommu_domain *to_rk_domain(struct iommu_domain *dom)
+{
+	return container_of(dom, struct rk_iommu_domain, domain);
+}
+
+/*
+ * The Rockchip rk3288 iommu uses a 2-level page table.
+ * The first level is the "Directory Table" (DT).
+ * The DT consists of 1024 4-byte Directory Table Entries (DTEs), each pointing
+ * to a "Page Table".
+ * The second level is the 1024 Page Tables (PT).
+ * Each PT consists of 1024 4-byte Page Table Entries (PTEs), each pointing to
+ * a 4 KB page of physical memory.
+ *
+ * The DT and each PT fits in a single 4 KB page (4-bytes * 1024 entries).
+ * Each iommu device has a MMU_DTE_ADDR register that contains the physical
+ * address of the start of the DT page.
+ *
+ * The structure of the page table is as follows:
+ *
+ *                   DT
+ * MMU_DTE_ADDR -> +-----+
+ *                 |     |
+ *                 +-----+     PT
+ *                 | DTE | -> +-----+
+ *                 +-----+    |     |     Memory
+ *                 |     |    +-----+     Page
+ *                 |     |    | PTE | -> +-----+
+ *                 +-----+    +-----+    |     |
+ *                            |     |    |     |
+ *                            |     |    |     |
+ *                            +-----+    |     |
+ *                                       |     |
+ *                                       |     |
+ *                                       +-----+
+ */
+
+/*
+ * Each DTE has a PT address and a valid bit:
+ * +---------------------+-----------+-+
+ * | PT address          | Reserved  |V|
+ * +---------------------+-----------+-+
+ *  31:12 - PT address (PTs always starts on a 4 KB boundary)
+ *  11: 1 - Reserved
+ *      0 - 1 if PT @ PT address is valid
+ */
+#define RK_DTE_PT_ADDRESS_MASK    0xfffff000
+#define RK_DTE_PT_VALID           BIT(0)
+
+static inline phys_addr_t rk_dte_pt_address(u32 dte)
+{
+	return (phys_addr_t)dte & RK_DTE_PT_ADDRESS_MASK;
+}
+
+/*
+ * In v2:
+ * 31:12 - PT address bit 31:0
+ * 11: 8 - PT address bit 35:32
+ *  7: 4 - PT address bit 39:36
+ *  3: 1 - Reserved
+ *     0 - 1 if PT @ PT address is valid
+ */
+#define RK_DTE_PT_ADDRESS_MASK_V2 GENMASK_ULL(31, 4)
+#define DTE_HI_MASK1	GENMASK(11, 8)
+#define DTE_HI_MASK2	GENMASK(7, 4)
+#define DTE_HI_SHIFT1	24 /* shift bit 8 to bit 32 */
+#define DTE_HI_SHIFT2	32 /* shift bit 4 to bit 36 */
+#define PAGE_DESC_HI_MASK1	GENMASK_ULL(35, 32)
+#define PAGE_DESC_HI_MASK2	GENMASK_ULL(39, 36)
+
+static inline phys_addr_t rk_dte_pt_address_v2(u32 dte)
+{
+	u64 dte_v2 = dte;
+
+	dte_v2 = ((dte_v2 & DTE_HI_MASK2) << DTE_HI_SHIFT2) |
+		 ((dte_v2 & DTE_HI_MASK1) << DTE_HI_SHIFT1) |
+		 (dte_v2 & RK_DTE_PT_ADDRESS_MASK);
+
+	return (phys_addr_t)dte_v2;
+}
+
+static inline bool rk_dte_is_pt_valid(u32 dte)
+{
+	return dte & RK_DTE_PT_VALID;
+}
+
+static inline u32 rk_mk_dte(dma_addr_t pt_dma)
+{
+	return (pt_dma & RK_DTE_PT_ADDRESS_MASK) | RK_DTE_PT_VALID;
+}
+
+static inline u32 rk_mk_dte_v2(dma_addr_t pt_dma)
+{
+	pt_dma = (pt_dma & RK_DTE_PT_ADDRESS_MASK) |
+		 ((pt_dma & PAGE_DESC_HI_MASK1) >> DTE_HI_SHIFT1) |
+		 (pt_dma & PAGE_DESC_HI_MASK2) >> DTE_HI_SHIFT2;
+
+	return (pt_dma & RK_DTE_PT_ADDRESS_MASK_V2) | RK_DTE_PT_VALID;
+}
+
+/*
+ * Each PTE has a Page address, some flags and a valid bit:
+ * +---------------------+---+-------+-+
+ * | Page address        |Rsv| Flags |V|
+ * +---------------------+---+-------+-+
+ *  31:12 - Page address (Pages always start on a 4 KB boundary)
+ *  11: 9 - Reserved
+ *   8: 1 - Flags
+ *      8 - Read allocate - allocate cache space on read misses
+ *      7 - Read cache - enable cache & prefetch of data
+ *      6 - Write buffer - enable delaying writes on their way to memory
+ *      5 - Write allocate - allocate cache space on write misses
+ *      4 - Write cache - different writes can be merged together
+ *      3 - Override cache attributes
+ *          if 1, bits 4-8 control cache attributes
+ *          if 0, the system bus defaults are used
+ *      2 - Writable
+ *      1 - Readable
+ *      0 - 1 if Page @ Page address is valid
+ */
+#define RK_PTE_PAGE_ADDRESS_MASK  0xfffff000
+#define RK_PTE_PAGE_FLAGS_MASK    0x000001fe
+#define RK_PTE_PAGE_WRITABLE      BIT(2)
+#define RK_PTE_PAGE_READABLE      BIT(1)
+#define RK_PTE_PAGE_VALID         BIT(0)
+
+static inline bool rk_pte_is_page_valid(u32 pte)
+{
+	return pte & RK_PTE_PAGE_VALID;
+}
+
+/* TODO: set cache flags per prot IOMMU_CACHE */
+static u32 rk_mk_pte(phys_addr_t page, int prot)
+{
+	u32 flags = 0;
+	flags |= (prot & IOMMU_READ) ? RK_PTE_PAGE_READABLE : 0;
+	flags |= (prot & IOMMU_WRITE) ? RK_PTE_PAGE_WRITABLE : 0;
+	page &= RK_PTE_PAGE_ADDRESS_MASK;
+	return page | flags | RK_PTE_PAGE_VALID;
+}
+
+/*
+ * In v2:
+ * 31:12 - Page address bit 31:0
+ * 11: 8 - Page address bit 35:32
+ *  7: 4 - Page address bit 39:36
+ *     3 - Security
+ *     2 - Writable
+ *     1 - Readable
+ *     0 - 1 if Page @ Page address is valid
+ */
+
+static u32 rk_mk_pte_v2(phys_addr_t page, int prot)
+{
+	u32 flags = 0;
+
+	flags |= (prot & IOMMU_READ) ? RK_PTE_PAGE_READABLE : 0;
+	flags |= (prot & IOMMU_WRITE) ? RK_PTE_PAGE_WRITABLE : 0;
+
+	return rk_mk_dte_v2(page) | flags;
+}
+
+static u32 rk_mk_pte_invalid(u32 pte)
+{
+	return pte & ~RK_PTE_PAGE_VALID;
+}
+
+/*
+ * rk3288 iova (IOMMU Virtual Address) format
+ *  31       22.21       12.11          0
+ * +-----------+-----------+-------------+
+ * | DTE index | PTE index | Page offset |
+ * +-----------+-----------+-------------+
+ *  31:22 - DTE index   - index of DTE in DT
+ *  21:12 - PTE index   - index of PTE in PT @ DTE.pt_address
+ *  11: 0 - Page offset - offset into page @ PTE.page_address
+ */
+#define RK_IOVA_DTE_MASK    0xffc00000
+#define RK_IOVA_DTE_SHIFT   22
+#define RK_IOVA_PTE_MASK    0x003ff000
+#define RK_IOVA_PTE_SHIFT   12
+#define RK_IOVA_PAGE_MASK   0x00000fff
+#define RK_IOVA_PAGE_SHIFT  0
+
+static u32 rk_iova_dte_index(dma_addr_t iova)
+{
+	return (u32)(iova & RK_IOVA_DTE_MASK) >> RK_IOVA_DTE_SHIFT;
+}
+
+static u32 rk_iova_pte_index(dma_addr_t iova)
+{
+	return (u32)(iova & RK_IOVA_PTE_MASK) >> RK_IOVA_PTE_SHIFT;
+}
+
+static u32 rk_iova_page_offset(dma_addr_t iova)
+{
+	return (u32)(iova & RK_IOVA_PAGE_MASK) >> RK_IOVA_PAGE_SHIFT;
+}
+
+static u32 rk_iommu_read(void __iomem *base, u32 offset)
+{
+	return readl(base + offset);
+}
+
+static void rk_iommu_write(void __iomem *base, u32 offset, u32 value)
+{
+	writel(value, base + offset);
+}
+
+static void rk_iommu_command(struct rk_iommu *iommu, u32 command)
+{
+	int i;
+
+	for (i = 0; i < iommu->num_mmu; i++)
+		writel(command, iommu->bases[i] + RK_MMU_COMMAND);
+}
+
+static void rk_iommu_base_command(void __iomem *base, u32 command)
+{
+	writel(command, base + RK_MMU_COMMAND);
+}
+static void rk_iommu_zap_lines(struct rk_iommu *iommu, dma_addr_t iova_start,
+			       size_t size)
+{
+	int i;
+	dma_addr_t iova_end = iova_start + size;
+	/*
+	 * TODO(djkurtz): Figure out when it is more efficient to shootdown the
+	 * entire iotlb rather than iterate over individual iovas.
+	 */
+	for (i = 0; i < iommu->num_mmu; i++) {
+		dma_addr_t iova;
+
+		for (iova = iova_start; iova < iova_end; iova += SPAGE_SIZE)
+			rk_iommu_write(iommu->bases[i], RK_MMU_ZAP_ONE_LINE, iova);
+	}
+}
+
+static bool rk_iommu_is_stall_active(struct rk_iommu *iommu)
+{
+	bool active = true;
+	int i;
+
+	for (i = 0; i < iommu->num_mmu; i++)
+		active &= !!(rk_iommu_read(iommu->bases[i], RK_MMU_STATUS) &
+					   RK_MMU_STATUS_STALL_ACTIVE);
+
+	return active;
+}
+
+static bool rk_iommu_is_paging_enabled(struct rk_iommu *iommu)
+{
+	bool enable = true;
+	int i;
+
+	for (i = 0; i < iommu->num_mmu; i++)
+		enable &= !!(rk_iommu_read(iommu->bases[i], RK_MMU_STATUS) &
+					   RK_MMU_STATUS_PAGING_ENABLED);
+
+	return enable;
+}
+
+static bool rk_iommu_is_reset_done(struct rk_iommu *iommu)
+{
+	bool done = true;
+	int i;
+
+	for (i = 0; i < iommu->num_mmu; i++)
+		done &= rk_iommu_read(iommu->bases[i], RK_MMU_DTE_ADDR) == 0;
+
+	return done;
+}
+
+static int rk_iommu_enable_stall(struct rk_iommu *iommu)
+{
+	int ret, i;
+	bool val;
+
+	if (rk_iommu_is_stall_active(iommu))
+		return 0;
+
+	/* Stall can only be enabled if paging is enabled */
+	if (!rk_iommu_is_paging_enabled(iommu))
+		return 0;
+
+	rk_iommu_command(iommu, RK_MMU_CMD_ENABLE_STALL);
+
+	ret = readx_poll_timeout(rk_iommu_is_stall_active, iommu, val,
+				 val, RK_MMU_POLL_PERIOD_US,
+				 RK_MMU_POLL_TIMEOUT_US);
+	if (ret)
+		for (i = 0; i < iommu->num_mmu; i++)
+			dev_err(iommu->dev, "Enable stall request timed out, status: %#08x\n",
+				rk_iommu_read(iommu->bases[i], RK_MMU_STATUS));
+
+	return ret;
+}
+
+static int rk_iommu_disable_stall(struct rk_iommu *iommu)
+{
+	int ret, i;
+	bool val;
+
+	if (!rk_iommu_is_stall_active(iommu))
+		return 0;
+
+	rk_iommu_command(iommu, RK_MMU_CMD_DISABLE_STALL);
+
+	ret = readx_poll_timeout(rk_iommu_is_stall_active, iommu, val,
+				 !val, RK_MMU_POLL_PERIOD_US,
+				 RK_MMU_POLL_TIMEOUT_US);
+	if (ret)
+		for (i = 0; i < iommu->num_mmu; i++)
+			dev_err(iommu->dev, "Disable stall request timed out, status: %#08x\n",
+				rk_iommu_read(iommu->bases[i], RK_MMU_STATUS));
+
+	return ret;
+}
+
+static int rk_iommu_enable_paging(struct rk_iommu *iommu)
+{
+	int ret, i;
+	bool val;
+
+	if (rk_iommu_is_paging_enabled(iommu))
+		return 0;
+
+	rk_iommu_command(iommu, RK_MMU_CMD_ENABLE_PAGING);
+
+	ret = readx_poll_timeout(rk_iommu_is_paging_enabled, iommu, val,
+				 val, RK_MMU_POLL_PERIOD_US,
+				 RK_MMU_POLL_TIMEOUT_US);
+	if (ret)
+		for (i = 0; i < iommu->num_mmu; i++)
+			dev_err(iommu->dev, "Enable paging request timed out, status: %#08x\n",
+				rk_iommu_read(iommu->bases[i], RK_MMU_STATUS));
+
+	return ret;
+}
+
+static int rk_iommu_disable_paging(struct rk_iommu *iommu)
+{
+	int ret, i;
+	bool val;
+
+	if (!rk_iommu_is_paging_enabled(iommu))
+		return 0;
+
+	rk_iommu_command(iommu, RK_MMU_CMD_DISABLE_PAGING);
+
+	ret = readx_poll_timeout(rk_iommu_is_paging_enabled, iommu, val,
+				 !val, RK_MMU_POLL_PERIOD_US,
+				 RK_MMU_POLL_TIMEOUT_US);
+	if (ret)
+		for (i = 0; i < iommu->num_mmu; i++)
+			dev_err(iommu->dev, "Disable paging request timed out, status: %#08x\n",
+				rk_iommu_read(iommu->bases[i], RK_MMU_STATUS));
+
+	return ret;
+}
+
+static int rk_iommu_force_reset(struct rk_iommu *iommu)
+{
+	int ret, i;
+	u32 dte_addr;
+	bool val;
+
+	if (iommu->reset_disabled)
+		return 0;
+
+	/*
+	 * Check if register DTE_ADDR is working by writing DTE_ADDR_DUMMY
+	 * and verifying that upper 5 (v1) or 7 (v2) nybbles are read back.
+	 */
+	for (i = 0; i < iommu->num_mmu; i++) {
+		dte_addr = rk_ops->pt_address(DTE_ADDR_DUMMY);
+		rk_iommu_write(iommu->bases[i], RK_MMU_DTE_ADDR, dte_addr);
+
+		if (dte_addr != rk_iommu_read(iommu->bases[i], RK_MMU_DTE_ADDR)) {
+			dev_err(iommu->dev, "Error during raw reset. MMU_DTE_ADDR is not functioning\n");
+			return -EFAULT;
+		}
+	}
+
+	rk_iommu_command(iommu, RK_MMU_CMD_FORCE_RESET);
+
+	ret = readx_poll_timeout(rk_iommu_is_reset_done, iommu, val,
+				 val, RK_MMU_FORCE_RESET_TIMEOUT_US,
+				 RK_MMU_POLL_TIMEOUT_US);
+	if (ret) {
+		dev_err(iommu->dev, "FORCE_RESET command timed out\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static void log_iova(struct rk_iommu *iommu, int index, dma_addr_t iova)
+{
+	void __iomem *base = iommu->bases[index];
+	u32 dte_index, pte_index, page_offset;
+	u32 mmu_dte_addr;
+	phys_addr_t mmu_dte_addr_phys, dte_addr_phys;
+	u32 *dte_addr;
+	u32 dte;
+	phys_addr_t pte_addr_phys = 0;
+	u32 *pte_addr = NULL;
+	u32 pte = 0;
+	phys_addr_t page_addr_phys = 0;
+	u32 page_flags = 0;
+
+	dte_index = rk_iova_dte_index(iova);
+	pte_index = rk_iova_pte_index(iova);
+	page_offset = rk_iova_page_offset(iova);
+
+	mmu_dte_addr = rk_iommu_read(base, RK_MMU_DTE_ADDR);
+	mmu_dte_addr_phys = rk_ops->pt_address(mmu_dte_addr);
+
+	dte_addr_phys = mmu_dte_addr_phys + (4 * dte_index);
+	dte_addr = phys_to_virt(dte_addr_phys);
+	dte = *dte_addr;
+
+	if (!rk_dte_is_pt_valid(dte))
+		goto print_it;
+
+	pte_addr_phys = rk_ops->pt_address(dte) + (pte_index * 4);
+	pte_addr = phys_to_virt(pte_addr_phys);
+	pte = *pte_addr;
+
+	if (!rk_pte_is_page_valid(pte))
+		goto print_it;
+
+	page_addr_phys = rk_ops->pt_address(pte) + page_offset;
+	page_flags = pte & RK_PTE_PAGE_FLAGS_MASK;
+
+print_it:
+	dev_err(iommu->dev, "iova = %pad: dte_index: %#03x pte_index: %#03x page_offset: %#03x\n",
+		&iova, dte_index, pte_index, page_offset);
+	dev_err(iommu->dev, "mmu_dte_addr: %pa dte@%pa: %#08x valid: %u pte@%pa: %#08x valid: %u page@%pa flags: %#03x\n",
+		&mmu_dte_addr_phys, &dte_addr_phys, dte,
+		rk_dte_is_pt_valid(dte), &pte_addr_phys, pte,
+		rk_pte_is_page_valid(pte), &page_addr_phys, page_flags);
+}
+
+static irqreturn_t rk_iommu_irq(int irq, void *dev_id)
+{
+	struct rk_iommu *iommu = dev_id;
+	u32 status;
+	u32 int_status;
+	dma_addr_t iova;
+	irqreturn_t ret = IRQ_NONE;
+	int i, err;
+
+	err = pm_runtime_get_if_in_use(iommu->dev);
+	if (!err || WARN_ON_ONCE(err < 0))
+		return ret;
+
+	if (WARN_ON(clk_bulk_enable(iommu->num_clocks, iommu->clocks)))
+		goto out;
+
+	for (i = 0; i < iommu->num_mmu; i++) {
+		int_status = rk_iommu_read(iommu->bases[i], RK_MMU_INT_STATUS);
+		if (int_status == 0)
+			continue;
+
+		ret = IRQ_HANDLED;
+		iova = rk_iommu_read(iommu->bases[i], RK_MMU_PAGE_FAULT_ADDR);
+
+		if (int_status & RK_MMU_IRQ_PAGE_FAULT) {
+			int flags;
+
+			status = rk_iommu_read(iommu->bases[i], RK_MMU_STATUS);
+			flags = (status & RK_MMU_STATUS_PAGE_FAULT_IS_WRITE) ?
+					IOMMU_FAULT_WRITE : IOMMU_FAULT_READ;
+
+			dev_err(iommu->dev, "Page fault at %pad of type %s\n",
+				&iova,
+				str_write_read(flags == IOMMU_FAULT_WRITE));
+
+			log_iova(iommu, i, iova);
+
+			/*
+			 * Report page fault to any installed handlers.
+			 * Ignore the return code, though, since we always zap cache
+			 * and clear the page fault anyway.
+			 */
+			if (iommu->domain != &rk_identity_domain)
+				report_iommu_fault(iommu->domain, iommu->dev, iova,
+						   flags);
+			else
+				dev_err(iommu->dev, "Page fault while iommu not attached to domain?\n");
+
+			rk_iommu_base_command(iommu->bases[i], RK_MMU_CMD_ZAP_CACHE);
+			rk_iommu_base_command(iommu->bases[i], RK_MMU_CMD_PAGE_FAULT_DONE);
+		}
+
+		if (int_status & RK_MMU_IRQ_BUS_ERROR)
+			dev_err(iommu->dev, "BUS_ERROR occurred at %pad\n", &iova);
+
+		if (int_status & ~RK_MMU_IRQ_MASK)
+			dev_err(iommu->dev, "unexpected int_status: %#08x\n",
+				int_status);
+
+		rk_iommu_write(iommu->bases[i], RK_MMU_INT_CLEAR, int_status);
+	}
+
+	clk_bulk_disable(iommu->num_clocks, iommu->clocks);
+
+out:
+	pm_runtime_put(iommu->dev);
+	return ret;
+}
+
+static phys_addr_t rk_iommu_iova_to_phys(struct iommu_domain *domain,
+					 dma_addr_t iova)
+{
+	struct rk_iommu_domain *rk_domain = to_rk_domain(domain);
+	unsigned long flags;
+	phys_addr_t pt_phys, phys = 0;
+	u32 dte, pte;
+	u32 *page_table;
+
+	spin_lock_irqsave(&rk_domain->dt_lock, flags);
+
+	dte = rk_domain->dt[rk_iova_dte_index(iova)];
+	if (!rk_dte_is_pt_valid(dte))
+		goto out;
+
+	pt_phys = rk_ops->pt_address(dte);
+	page_table = (u32 *)phys_to_virt(pt_phys);
+	pte = page_table[rk_iova_pte_index(iova)];
+	if (!rk_pte_is_page_valid(pte))
+		goto out;
+
+	phys = rk_ops->pt_address(pte) + rk_iova_page_offset(iova);
+out:
+	spin_unlock_irqrestore(&rk_domain->dt_lock, flags);
+
+	return phys;
+}
+
+static void rk_iommu_zap_iova(struct rk_iommu_domain *rk_domain,
+			      dma_addr_t iova, size_t size)
+{
+	struct list_head *pos;
+	unsigned long flags;
+
+	/* shootdown these iova from all iommus using this domain */
+	spin_lock_irqsave(&rk_domain->iommus_lock, flags);
+	list_for_each(pos, &rk_domain->iommus) {
+		struct rk_iommu *iommu;
+		int ret;
+
+		iommu = list_entry(pos, struct rk_iommu, node);
+
+		/* Only zap TLBs of IOMMUs that are powered on. */
+		ret = pm_runtime_get_if_in_use(iommu->dev);
+		if (WARN_ON_ONCE(ret < 0))
+			continue;
+		if (ret) {
+			WARN_ON(clk_bulk_enable(iommu->num_clocks,
+						iommu->clocks));
+			rk_iommu_zap_lines(iommu, iova, size);
+			clk_bulk_disable(iommu->num_clocks, iommu->clocks);
+			pm_runtime_put(iommu->dev);
+		}
+	}
+	spin_unlock_irqrestore(&rk_domain->iommus_lock, flags);
+}
+
+static void rk_iommu_zap_iova_first_last(struct rk_iommu_domain *rk_domain,
+					 dma_addr_t iova, size_t size)
+{
+	rk_iommu_zap_iova(rk_domain, iova, SPAGE_SIZE);
+	if (size > SPAGE_SIZE)
+		rk_iommu_zap_iova(rk_domain, iova + size - SPAGE_SIZE,
+					SPAGE_SIZE);
+}
+
+static u32 *rk_dte_get_page_table(struct rk_iommu_domain *rk_domain,
+				  dma_addr_t iova)
+{
+	u32 *page_table, *dte_addr;
+	u32 dte_index, dte;
+	phys_addr_t pt_phys;
+	dma_addr_t pt_dma;
+
+	assert_spin_locked(&rk_domain->dt_lock);
+
+	dte_index = rk_iova_dte_index(iova);
+	dte_addr = &rk_domain->dt[dte_index];
+	dte = *dte_addr;
+	if (rk_dte_is_pt_valid(dte))
+		goto done;
+
+	page_table = iommu_alloc_pages_sz(GFP_ATOMIC | rk_ops->gfp_flags,
+					  SPAGE_SIZE);
+	if (!page_table)
+		return ERR_PTR(-ENOMEM);
+
+	pt_dma = dma_map_single(rk_domain->dma_dev, page_table, SPAGE_SIZE, DMA_TO_DEVICE);
+	if (dma_mapping_error(rk_domain->dma_dev, pt_dma)) {
+		dev_err(rk_domain->dma_dev, "DMA mapping error while allocating page table\n");
+		iommu_free_pages(page_table);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	dte = rk_ops->mk_dtentries(pt_dma);
+	*dte_addr = dte;
+
+	rk_table_flush(rk_domain,
+		       rk_domain->dt_dma + dte_index * sizeof(u32), 1);
+done:
+	pt_phys = rk_ops->pt_address(dte);
+	return (u32 *)phys_to_virt(pt_phys);
+}
+
+static size_t rk_iommu_unmap_iova(struct rk_iommu_domain *rk_domain,
+				  u32 *pte_addr, dma_addr_t pte_dma,
+				  size_t size)
+{
+	unsigned int pte_count;
+	unsigned int pte_total = size / SPAGE_SIZE;
+
+	assert_spin_locked(&rk_domain->dt_lock);
+
+	for (pte_count = 0; pte_count < pte_total; pte_count++) {
+		u32 pte = pte_addr[pte_count];
+		if (!rk_pte_is_page_valid(pte))
+			break;
+
+		pte_addr[pte_count] = rk_mk_pte_invalid(pte);
+	}
+
+	rk_table_flush(rk_domain, pte_dma, pte_count);
+
+	return pte_count * SPAGE_SIZE;
+}
+
+static int rk_iommu_map_iova(struct rk_iommu_domain *rk_domain, u32 *pte_addr,
+			     dma_addr_t pte_dma, dma_addr_t iova,
+			     phys_addr_t paddr, size_t size, int prot)
+{
+	unsigned int pte_count;
+	unsigned int pte_total = size / SPAGE_SIZE;
+	phys_addr_t page_phys;
+
+	assert_spin_locked(&rk_domain->dt_lock);
+
+	for (pte_count = 0; pte_count < pte_total; pte_count++) {
+		u32 pte = pte_addr[pte_count];
+
+		if (rk_pte_is_page_valid(pte))
+			goto unwind;
+
+		pte_addr[pte_count] = rk_ops->mk_ptentries(paddr, prot);
+
+		paddr += SPAGE_SIZE;
+	}
+
+	rk_table_flush(rk_domain, pte_dma, pte_total);
+
+	/*
+	 * Zap the first and last iova to evict from iotlb any previously
+	 * mapped cachelines holding stale values for its dte and pte.
+	 * We only zap the first and last iova, since only they could have
+	 * dte or pte shared with an existing mapping.
+	 */
+	rk_iommu_zap_iova_first_last(rk_domain, iova, size);
+
+	return 0;
+unwind:
+	/* Unmap the range of iovas that we just mapped */
+	rk_iommu_unmap_iova(rk_domain, pte_addr, pte_dma,
+			    pte_count * SPAGE_SIZE);
+
+	iova += pte_count * SPAGE_SIZE;
+	page_phys = rk_ops->pt_address(pte_addr[pte_count]);
+	pr_err("iova: %pad already mapped to %pa cannot remap to phys: %pa prot: %#x\n",
+	       &iova, &page_phys, &paddr, prot);
+
+	return -EADDRINUSE;
+}
+
+static int rk_iommu_map(struct iommu_domain *domain, unsigned long _iova,
+			phys_addr_t paddr, size_t size, size_t count,
+			int prot, gfp_t gfp, size_t *mapped)
+{
+	struct rk_iommu_domain *rk_domain = to_rk_domain(domain);
+	unsigned long flags;
+	dma_addr_t pte_dma, iova = (dma_addr_t)_iova;
+	u32 *page_table, *pte_addr;
+	u32 dte_index, pte_index;
+	int ret;
+
+	spin_lock_irqsave(&rk_domain->dt_lock, flags);
+
+	/*
+	 * pgsize_bitmap specifies iova sizes that fit in one page table
+	 * (1024 4-KiB pages = 4 MiB).
+	 * So, size will always be 4096 <= size <= 4194304.
+	 * Since iommu_map() guarantees that both iova and size will be
+	 * aligned, we will always only be mapping from a single dte here.
+	 */
+	page_table = rk_dte_get_page_table(rk_domain, iova);
+	if (IS_ERR(page_table)) {
+		spin_unlock_irqrestore(&rk_domain->dt_lock, flags);
+		return PTR_ERR(page_table);
+	}
+
+	dte_index = rk_domain->dt[rk_iova_dte_index(iova)];
+	pte_index = rk_iova_pte_index(iova);
+	pte_addr = &page_table[pte_index];
+
+	pte_dma = rk_ops->pt_address(dte_index) + pte_index * sizeof(u32);
+	ret = rk_iommu_map_iova(rk_domain, pte_addr, pte_dma, iova,
+				paddr, size, prot);
+
+	spin_unlock_irqrestore(&rk_domain->dt_lock, flags);
+	if (!ret)
+		*mapped = size;
+
+	return ret;
+}
+
+static size_t rk_iommu_unmap(struct iommu_domain *domain, unsigned long _iova,
+			     size_t size, size_t count, struct iommu_iotlb_gather *gather)
+{
+	struct rk_iommu_domain *rk_domain = to_rk_domain(domain);
+	unsigned long flags;
+	dma_addr_t pte_dma, iova = (dma_addr_t)_iova;
+	phys_addr_t pt_phys;
+	u32 dte;
+	u32 *pte_addr;
+	size_t unmap_size;
+
+	spin_lock_irqsave(&rk_domain->dt_lock, flags);
+
+	/*
+	 * pgsize_bitmap specifies iova sizes that fit in one page table
+	 * (1024 4-KiB pages = 4 MiB).
+	 * So, size will always be 4096 <= size <= 4194304.
+	 * Since iommu_unmap() guarantees that both iova and size will be
+	 * aligned, we will always only be unmapping from a single dte here.
+	 */
+	dte = rk_domain->dt[rk_iova_dte_index(iova)];
+	/* Just return 0 if iova is unmapped */
+	if (!rk_dte_is_pt_valid(dte)) {
+		spin_unlock_irqrestore(&rk_domain->dt_lock, flags);
+		return 0;
+	}
+
+	pt_phys = rk_ops->pt_address(dte);
+	pte_addr = (u32 *)phys_to_virt(pt_phys) + rk_iova_pte_index(iova);
+	pte_dma = pt_phys + rk_iova_pte_index(iova) * sizeof(u32);
+	unmap_size = rk_iommu_unmap_iova(rk_domain, pte_addr, pte_dma, size);
+
+	spin_unlock_irqrestore(&rk_domain->dt_lock, flags);
+
+	/* Shootdown iotlb entries for iova range that was just unmapped */
+	rk_iommu_zap_iova(rk_domain, iova, unmap_size);
+
+	return unmap_size;
+}
+
+static struct rk_iommu *rk_iommu_from_dev(struct device *dev)
+{
+	struct rk_iommudata *data = dev_iommu_priv_get(dev);
+
+	return data ? data->iommu : NULL;
+}
+
+/* Must be called with iommu powered on and attached */
+static void rk_iommu_disable(struct rk_iommu *iommu)
+{
+	int i;
+
+	/* Ignore error while disabling, just keep going */
+	WARN_ON(clk_bulk_enable(iommu->num_clocks, iommu->clocks));
+	rk_iommu_enable_stall(iommu);
+	rk_iommu_disable_paging(iommu);
+	for (i = 0; i < iommu->num_mmu; i++) {
+		rk_iommu_write(iommu->bases[i], RK_MMU_INT_MASK, 0);
+		rk_iommu_write(iommu->bases[i], RK_MMU_DTE_ADDR, 0);
+	}
+	rk_iommu_disable_stall(iommu);
+	clk_bulk_disable(iommu->num_clocks, iommu->clocks);
+}
+
+/* Must be called with iommu powered on and attached */
+static int rk_iommu_enable(struct rk_iommu *iommu)
+{
+	struct iommu_domain *domain = iommu->domain;
+	struct rk_iommu_domain *rk_domain = to_rk_domain(domain);
+	int ret, i;
+
+	ret = clk_bulk_enable(iommu->num_clocks, iommu->clocks);
+	if (ret)
+		return ret;
+
+	ret = rk_iommu_enable_stall(iommu);
+	if (ret)
+		goto out_disable_clocks;
+
+	ret = rk_iommu_force_reset(iommu);
+	if (ret)
+		goto out_disable_stall;
+
+	for (i = 0; i < iommu->num_mmu; i++) {
+		rk_iommu_write(iommu->bases[i], RK_MMU_DTE_ADDR,
+			       rk_ops->mk_dtentries(rk_domain->dt_dma));
+		rk_iommu_base_command(iommu->bases[i], RK_MMU_CMD_ZAP_CACHE);
+		rk_iommu_write(iommu->bases[i], RK_MMU_INT_MASK, RK_MMU_IRQ_MASK);
+	}
+
+	ret = rk_iommu_enable_paging(iommu);
+
+out_disable_stall:
+	rk_iommu_disable_stall(iommu);
+out_disable_clocks:
+	clk_bulk_disable(iommu->num_clocks, iommu->clocks);
+	return ret;
+}
+
+static int rk_iommu_identity_attach(struct iommu_domain *identity_domain,
+				    struct device *dev,
+				    struct iommu_domain *old)
+{
+	struct rk_iommu *iommu;
+	struct rk_iommu_domain *rk_domain;
+	unsigned long flags;
+	int ret;
+
+	/* Allow 'virtual devices' (eg drm) to detach from domain */
+	iommu = rk_iommu_from_dev(dev);
+	if (!iommu)
+		return -ENODEV;
+
+	rk_domain = to_rk_domain(iommu->domain);
+
+	dev_dbg(dev, "Detaching from iommu domain\n");
+
+	if (iommu->domain == identity_domain)
+		return 0;
+
+	iommu->domain = identity_domain;
+
+	spin_lock_irqsave(&rk_domain->iommus_lock, flags);
+	list_del_init(&iommu->node);
+	spin_unlock_irqrestore(&rk_domain->iommus_lock, flags);
+
+	ret = pm_runtime_get_if_in_use(iommu->dev);
+	WARN_ON_ONCE(ret < 0);
+	if (ret > 0) {
+		rk_iommu_disable(iommu);
+		pm_runtime_put(iommu->dev);
+	}
+
+	return 0;
+}
+
+static struct iommu_domain_ops rk_identity_ops = {
+	.attach_dev = rk_iommu_identity_attach,
+};
+
+static struct iommu_domain rk_identity_domain = {
+	.type = IOMMU_DOMAIN_IDENTITY,
+	.ops = &rk_identity_ops,
+};
+
+static int rk_iommu_attach_device(struct iommu_domain *domain,
+				  struct device *dev, struct iommu_domain *old)
+{
+	struct rk_iommu *iommu;
+	struct rk_iommu_domain *rk_domain = to_rk_domain(domain);
+	unsigned long flags;
+	int ret;
+
+	/*
+	 * Allow 'virtual devices' (e.g., drm) to attach to domain.
+	 * Such a device does not belong to an iommu group.
+	 */
+	iommu = rk_iommu_from_dev(dev);
+	if (!iommu)
+		return 0;
+
+	dev_dbg(dev, "Attaching to iommu domain\n");
+
+	/* iommu already attached */
+	if (iommu->domain == domain)
+		return 0;
+
+	ret = rk_iommu_identity_attach(&rk_identity_domain, dev, old);
+	if (ret)
+		return ret;
+
+	iommu->domain = domain;
+
+	spin_lock_irqsave(&rk_domain->iommus_lock, flags);
+	list_add_tail(&iommu->node, &rk_domain->iommus);
+	spin_unlock_irqrestore(&rk_domain->iommus_lock, flags);
+
+	ret = pm_runtime_get_if_in_use(iommu->dev);
+	if (!ret || WARN_ON_ONCE(ret < 0))
+		return 0;
+
+	ret = rk_iommu_enable(iommu);
+	if (ret) {
+		/*
+		 * Note rk_iommu_identity_attach() might fail before physically
+		 * attaching the dev to iommu->domain, in which case the actual
+		 * old domain for this revert should be rk_identity_domain v.s.
+		 * iommu->domain. Since rk_iommu_identity_attach() does not care
+		 * about the old domain argument for now, this is not a problem.
+		 */
+		WARN_ON(rk_iommu_identity_attach(&rk_identity_domain, dev,
+						 iommu->domain));
+	}
+
+	pm_runtime_put(iommu->dev);
+
+	return ret;
+}
+
+static struct iommu_domain *rk_iommu_domain_alloc_paging(struct device *dev)
+{
+	struct rk_iommu_domain *rk_domain;
+	struct rk_iommu *iommu;
+
+	rk_domain = kzalloc(sizeof(*rk_domain), GFP_KERNEL);
+	if (!rk_domain)
+		return NULL;
+
+	/*
+	 * rk32xx iommus use a 2 level pagetable.
+	 * Each level1 (dt) and level2 (pt) table has 1024 4-byte entries.
+	 * Allocate one 4 KiB page for each table.
+	 */
+	rk_domain->dt = iommu_alloc_pages_sz(GFP_KERNEL | rk_ops->gfp_flags,
+					     SPAGE_SIZE);
+	if (!rk_domain->dt)
+		goto err_free_domain;
+
+	iommu = rk_iommu_from_dev(dev);
+	rk_domain->dma_dev = iommu->dev;
+	rk_domain->dt_dma = dma_map_single(rk_domain->dma_dev, rk_domain->dt,
+					   SPAGE_SIZE, DMA_TO_DEVICE);
+	if (dma_mapping_error(rk_domain->dma_dev, rk_domain->dt_dma)) {
+		dev_err(rk_domain->dma_dev, "DMA map error for DT\n");
+		goto err_free_dt;
+	}
+
+	spin_lock_init(&rk_domain->iommus_lock);
+	spin_lock_init(&rk_domain->dt_lock);
+	INIT_LIST_HEAD(&rk_domain->iommus);
+
+	rk_domain->domain.pgsize_bitmap = RK_IOMMU_PGSIZE_BITMAP;
+
+	rk_domain->domain.geometry.aperture_start = 0;
+	rk_domain->domain.geometry.aperture_end   = DMA_BIT_MASK(32);
+	rk_domain->domain.geometry.force_aperture = true;
+
+	return &rk_domain->domain;
+
+err_free_dt:
+	iommu_free_pages(rk_domain->dt);
+err_free_domain:
+	kfree(rk_domain);
+
+	return NULL;
+}
+
+static void rk_iommu_domain_free(struct iommu_domain *domain)
+{
+	struct rk_iommu_domain *rk_domain = to_rk_domain(domain);
+	int i;
+
+	WARN_ON(!list_empty(&rk_domain->iommus));
+
+	for (i = 0; i < NUM_DT_ENTRIES; i++) {
+		u32 dte = rk_domain->dt[i];
+		if (rk_dte_is_pt_valid(dte)) {
+			phys_addr_t pt_phys = rk_ops->pt_address(dte);
+			u32 *page_table = phys_to_virt(pt_phys);
+			dma_unmap_single(rk_domain->dma_dev, pt_phys,
+					 SPAGE_SIZE, DMA_TO_DEVICE);
+			iommu_free_pages(page_table);
+		}
+	}
+
+	dma_unmap_single(rk_domain->dma_dev, rk_domain->dt_dma,
+			 SPAGE_SIZE, DMA_TO_DEVICE);
+	iommu_free_pages(rk_domain->dt);
+
+	kfree(rk_domain);
+}
+
+static struct iommu_device *rk_iommu_probe_device(struct device *dev)
+{
+	struct rk_iommudata *data;
+	struct rk_iommu *iommu;
+
+	data = dev_iommu_priv_get(dev);
+	if (!data)
+		return ERR_PTR(-ENODEV);
+
+	iommu = rk_iommu_from_dev(dev);
+
+	data->link = device_link_add(dev, iommu->dev,
+				     DL_FLAG_STATELESS | DL_FLAG_PM_RUNTIME);
+
+	return &iommu->iommu;
+}
+
+static void rk_iommu_release_device(struct device *dev)
+{
+	struct rk_iommudata *data = dev_iommu_priv_get(dev);
+
+	device_link_del(data->link);
+}
+
+static int rk_iommu_of_xlate(struct device *dev,
+			     const struct of_phandle_args *args)
+{
+	struct platform_device *iommu_dev;
+	struct rk_iommudata *data;
+
+	iommu_dev = of_find_device_by_node(args->np);
+
+	data = devm_kzalloc(&iommu_dev->dev, sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	data->iommu = platform_get_drvdata(iommu_dev);
+	dev_iommu_priv_set(dev, data);
+
+	platform_device_put(iommu_dev);
+
+	return 0;
+}
+
+static const struct iommu_ops rk_iommu_ops = {
+	.identity_domain = &rk_identity_domain,
+	.domain_alloc_paging = rk_iommu_domain_alloc_paging,
+	.probe_device = rk_iommu_probe_device,
+	.release_device = rk_iommu_release_device,
+	.device_group = generic_single_device_group,
+	.get_resv_regions = iommu_dma_get_resv_regions,
+	.of_xlate = rk_iommu_of_xlate,
+	.default_domain_ops = &(const struct iommu_domain_ops) {
+		.attach_dev	= rk_iommu_attach_device,
+		.map_pages	= rk_iommu_map,
+		.unmap_pages	= rk_iommu_unmap,
+		.iova_to_phys	= rk_iommu_iova_to_phys,
+		.free		= rk_iommu_domain_free,
+	}
+};
+
+static int rk_iommu_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct rk_iommu *iommu;
+	struct resource *res;
+	const struct rk_iommu_ops *ops;
+	int num_res = pdev->num_resources;
+	int err, i;
+
+	iommu = devm_kzalloc(dev, sizeof(*iommu), GFP_KERNEL);
+	if (!iommu)
+		return -ENOMEM;
+
+	iommu->domain = &rk_identity_domain;
+
+	platform_set_drvdata(pdev, iommu);
+	iommu->dev = dev;
+	iommu->num_mmu = 0;
+
+	ops = of_device_get_match_data(dev);
+	if (!rk_ops)
+		rk_ops = ops;
+
+	/*
+	 * That should not happen unless different versions of the
+	 * hardware block are embedded the same SoC
+	 */
+	if (WARN_ON(rk_ops != ops))
+		return -EINVAL;
+
+	iommu->bases = devm_kcalloc(dev, num_res, sizeof(*iommu->bases),
+				    GFP_KERNEL);
+	if (!iommu->bases)
+		return -ENOMEM;
+
+	for (i = 0; i < num_res; i++) {
+		res = platform_get_resource(pdev, IORESOURCE_MEM, i);
+		if (!res)
+			continue;
+		iommu->bases[i] = devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(iommu->bases[i]))
+			continue;
+		iommu->num_mmu++;
+	}
+	if (iommu->num_mmu == 0)
+		return PTR_ERR(iommu->bases[0]);
+
+	iommu->num_irq = platform_irq_count(pdev);
+	if (iommu->num_irq < 0)
+		return iommu->num_irq;
+
+	iommu->reset_disabled = device_property_read_bool(dev,
+					"rockchip,disable-mmu-reset");
+
+	iommu->num_clocks = ARRAY_SIZE(rk_iommu_clocks);
+	iommu->clocks = devm_kcalloc(iommu->dev, iommu->num_clocks,
+				     sizeof(*iommu->clocks), GFP_KERNEL);
+	if (!iommu->clocks)
+		return -ENOMEM;
+
+	for (i = 0; i < iommu->num_clocks; ++i)
+		iommu->clocks[i].id = rk_iommu_clocks[i];
+
+	/*
+	 * iommu clocks should be present for all new devices and devicetrees
+	 * but there are older devicetrees without clocks out in the wild.
+	 * So clocks as optional for the time being.
+	 */
+	err = devm_clk_bulk_get(iommu->dev, iommu->num_clocks, iommu->clocks);
+	if (err == -ENOENT)
+		iommu->num_clocks = 0;
+	else if (err)
+		return err;
+
+	err = clk_bulk_prepare(iommu->num_clocks, iommu->clocks);
+	if (err)
+		return err;
+
+	pm_runtime_enable(dev);
+
+	for (i = 0; i < iommu->num_irq; i++) {
+		int irq = platform_get_irq(pdev, i);
+
+		if (irq < 0) {
+			err = irq;
+			goto err_pm_disable;
+		}
+
+		err = devm_request_irq(iommu->dev, irq, rk_iommu_irq,
+				       IRQF_SHARED, dev_name(dev), iommu);
+		if (err)
+			goto err_pm_disable;
+	}
+
+	dma_set_mask_and_coherent(dev, rk_ops->dma_bit_mask);
+
+	err = iommu_device_sysfs_add(&iommu->iommu, dev, NULL, dev_name(dev));
+	if (err)
+		goto err_pm_disable;
+
+	err = iommu_device_register(&iommu->iommu, &rk_iommu_ops, dev);
+	if (err)
+		goto err_remove_sysfs;
+
+	return 0;
+err_remove_sysfs:
+	iommu_device_sysfs_remove(&iommu->iommu);
+err_pm_disable:
+	pm_runtime_disable(dev);
+	clk_bulk_unprepare(iommu->num_clocks, iommu->clocks);
+	return err;
+}
+
+static void rk_iommu_shutdown(struct platform_device *pdev)
+{
+	struct rk_iommu *iommu = platform_get_drvdata(pdev);
+	int i;
+
+	for (i = 0; i < iommu->num_irq; i++) {
+		int irq = platform_get_irq(pdev, i);
+
+		devm_free_irq(iommu->dev, irq, iommu);
+	}
+
+	pm_runtime_force_suspend(&pdev->dev);
+}
+
+static int __maybe_unused rk_iommu_suspend(struct device *dev)
+{
+	struct rk_iommu *iommu = dev_get_drvdata(dev);
+
+	if (iommu->domain == &rk_identity_domain)
+		return 0;
+
+	rk_iommu_disable(iommu);
+	return 0;
+}
+
+static int __maybe_unused rk_iommu_resume(struct device *dev)
+{
+	struct rk_iommu *iommu = dev_get_drvdata(dev);
+
+	if (iommu->domain == &rk_identity_domain)
+		return 0;
+
+	return rk_iommu_enable(iommu);
+}
+
+static const struct dev_pm_ops rk_iommu_pm_ops = {
+	SET_RUNTIME_PM_OPS(rk_iommu_suspend, rk_iommu_resume, NULL)
+	SET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend,
+				pm_runtime_force_resume)
+};
+
+static struct rk_iommu_ops iommu_data_ops_v1 = {
+	.pt_address = &rk_dte_pt_address,
+	.mk_dtentries = &rk_mk_dte,
+	.mk_ptentries = &rk_mk_pte,
+	.dma_bit_mask = DMA_BIT_MASK(32),
+	.gfp_flags = GFP_DMA32,
+};
+
+static struct rk_iommu_ops iommu_data_ops_v2 = {
+	.pt_address = &rk_dte_pt_address_v2,
+	.mk_dtentries = &rk_mk_dte_v2,
+	.mk_ptentries = &rk_mk_pte_v2,
+	.dma_bit_mask = DMA_BIT_MASK(40),
+	.gfp_flags = 0,
+};
+
+static const struct of_device_id rk_iommu_dt_ids[] = {
+	{	.compatible = "rockchip,iommu",
+		.data = &iommu_data_ops_v1,
+	},
+	{	.compatible = "rockchip,rk3568-iommu",
+		.data = &iommu_data_ops_v2,
+	},
+	{ /* sentinel */ }
+};
+
+static struct platform_driver rk_iommu_driver = {
+	.probe = rk_iommu_probe,
+	.shutdown = rk_iommu_shutdown,
+	.driver = {
+		   .name = "rk_iommu",
+		   .of_match_table = rk_iommu_dt_ids,
+		   .pm = &rk_iommu_pm_ops,
+		   .suppress_bind_attrs = true,
+	},
+};
+builtin_platform_driver(rk_iommu_driver);
diff -urN a/drivers/media/platform/rockchip/Kconfig bb/drivers/media/platform/rockchip/Kconfig
--- a/drivers/media/platform/rockchip/Kconfig	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/Kconfig	2026-02-07 01:32:53
@@ -0,0 +1,17 @@
+config VIDEO_ROCKCHIP_RKVENC
+	tristate "Rockchip RKVENC (VEPU580) encoder driver"
+	depends on ARCH_ROCKCHIP || COMPILE_TEST
+	depends on VIDEO_DEV
+	depends on IOMMU_API
+	select VIDEOBUF2_DMA_CONTIG
+	help
+	  Support for the Rockchip VEPU580 hardware video encoder found on
+	  RK3588 SoCs. This encoder supports H.264, H.265/HEVC, and JPEG
+	  encoding at up to 8K resolution.
+
+	  This driver provides the /dev/mpp_service character device interface
+	  used by the Rockchip MPP userspace library for hardware-accelerated
+	  video encoding.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called rkvenc.
diff -urN a/drivers/media/platform/rockchip/Makefile bb/drivers/media/platform/rockchip/Makefile
--- a/drivers/media/platform/rockchip/Makefile	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/Makefile	2026-02-07 01:32:58
@@ -0,0 +1,2 @@
+obj-$(CONFIG_VIDEO_ROCKCHIP_RKVENC) += rkvenc.o
+rkvenc-objs := rkvenc_drv.o rkvenc_hw.o rkvenc_service.o rkvenc_task.o rkvenc_iommu.o
diff -urN a/drivers/media/platform/rockchip/rkvenc/rkvenc_drv.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_drv.c
--- a/drivers/media/platform/rockchip/rkvenc/rkvenc_drv.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_drv.c	2026-02-08 03:39:42
@@ -0,0 +1,452 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - Platform driver
+ * Ported from Rockchip BSP mpp_rkvenc2.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ * Copyright (C) 2026 Ross Cawston
+ */
+
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/iommu.h>
+#include <linux/iopoll.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/pm_runtime.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+
+#include "rkvenc_hw.h"
+
+/* ---- PM ops ---- */
+static int __maybe_unused rkvenc_runtime_suspend(struct device *dev)
+{
+	return 0;
+}
+
+static int __maybe_unused rkvenc_runtime_resume(struct device *dev)
+{
+	return 0;
+}
+
+static const struct dev_pm_ops rkvenc_pm_ops = {
+	SET_RUNTIME_PM_OPS(rkvenc_runtime_suspend, rkvenc_runtime_resume, NULL)
+	SET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend, pm_runtime_force_resume)
+};
+
+/* ---- CCU probe ---- */
+static int rkvenc_ccu_probe(struct platform_device *pdev)
+{
+	struct rkvenc_ccu *ccu;
+	struct device *dev = &pdev->dev;
+
+	ccu = devm_kzalloc(dev, sizeof(*ccu), GFP_KERNEL);
+	if (!ccu)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ccu);
+
+	mutex_init(&ccu->lock);
+	INIT_LIST_HEAD(&ccu->core_list);
+	spin_lock_init(&ccu->lock_dchs);
+
+	dev_info(dev, "rkvenc ccu probe success\n");
+	return 0;
+}
+
+/* ---- Attach core to CCU ---- */
+static int rkvenc_attach_ccu(struct device *dev, struct rkvenc_dev *enc)
+{
+	struct device_node *np;
+	struct platform_device *pdev;
+	struct rkvenc_ccu *ccu;
+
+	np = of_parse_phandle(dev->of_node, "rockchip,ccu", 0);
+	if (!np || !of_device_is_available(np))
+		return -ENODEV;
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev)
+		return -ENODEV;
+
+	ccu = platform_get_drvdata(pdev);
+	if (!ccu)
+		return -EPROBE_DEFER;
+
+	INIT_LIST_HEAD(&enc->core_link);
+	mutex_lock(&ccu->lock);
+	ccu->core_num++;
+	list_add_tail(&enc->core_link, &ccu->core_list);
+	mutex_unlock(&ccu->lock);
+
+	/* First core becomes the main core */
+	if (!ccu->main_core) {
+		ccu->main_core = enc;
+	} else {
+		struct rkvenc_iommu_info *main_iommu =
+			ccu->main_core ? ccu->main_core->iommu_info : NULL;
+		struct rkvenc_iommu_info *sec_iommu = enc->iommu_info;
+		struct iommu_domain *shared;
+		int ret;
+
+		shared = main_iommu ? main_iommu->domain : NULL;
+		if (!shared || !sec_iommu || !sec_iommu->group) {
+			dev_err(dev, "missing IOMMU info for shared domain\n");
+			return -ENODEV;
+		}
+
+		ret = iommu_attach_group(shared, sec_iommu->group);
+		if (ret) {
+			dev_err(dev, "attach shared IOMMU domain failed: %d\n", ret);
+			return ret;
+		}
+
+		/* Ensure local pointer uses the shared DMA domain */
+		sec_iommu->domain = shared;
+	}
+	enc->ccu = ccu;
+
+	dev_info(dev, "attach ccu as core %d%s\n", enc->core_id,
+		 enc == ccu->main_core ? " [main]" :
+		 (enc->queue && enc->core_id >= 0 &&
+		  enc->core_id < MPP_MAX_CORE_NUM &&
+		  enc->queue->cores[enc->core_id] == enc ?
+		  " [secondary, active]" : " [secondary, inactive]"));
+	return 0;
+}
+
+/* ---- Attach core to service ---- */
+static int rkvenc_attach_service(struct rkvenc_dev *enc)
+{
+	struct device_node *np;
+	struct platform_device *pdev;
+	struct rkvenc_service *srv;
+	struct rkvenc_taskqueue *queue;
+	u32 taskqueue_node = 0;
+	u32 resetgroup_node = 0;
+
+	np = of_parse_phandle(enc->dev->of_node, "rockchip,srv", 0);
+	if (!np || !of_device_is_available(np))
+		return -ENODEV;
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev)
+		return -ENODEV;
+
+	srv = platform_get_drvdata(pdev);
+	if (!srv)
+		return -EPROBE_DEFER;
+
+	enc->srv = srv;
+
+	of_property_read_u32(enc->dev->of_node, "rockchip,taskqueue-node", &taskqueue_node);
+	of_property_read_u32(enc->dev->of_node, "rockchip,resetgroup-node", &resetgroup_node);
+
+	/* Attach to task queue */
+	queue = srv->task_queues[MPP_DEVICE_RKVENC];
+	if (!queue) {
+		dev_err(enc->dev, "no task queue for RKVENC\n");
+		return -ENODEV;
+	}
+	enc->queue = queue;
+
+	/* Register core in the task queue */
+	if (enc->core_id >= 0 && enc->core_id < MPP_MAX_CORE_NUM) {
+		queue->cores[enc->core_id] = enc;
+		queue->core_count++;
+		if (enc->core_id > queue->core_id_max)
+			queue->core_id_max = enc->core_id;
+		set_bit(enc->core_id, &queue->core_idle);
+	}
+
+	/* Attach to reset group */
+	if (resetgroup_node < srv->reset_group_cnt)
+		enc->reset_group = srv->reset_groups[resetgroup_node];
+
+	/* Init kthread work */
+	kthread_init_work(&enc->work, rkvenc_task_worker_default);
+
+	return 0;
+}
+
+/* ---- SRAM RCB allocation ---- */
+static int rkvenc2_alloc_rcbbuf(struct platform_device *pdev, struct rkvenc_dev *enc)
+{
+	int ret;
+	u32 vals[2];
+	dma_addr_t iova;
+	u32 sram_used, sram_size;
+	struct device_node *sram_np;
+	struct resource sram_res;
+	resource_size_t sram_start, sram_end;
+	struct iommu_domain *domain;
+	struct device *dev = &pdev->dev;
+
+	ret = device_property_read_u32_array(dev, "rockchip,rcb-iova", vals, 2);
+	if (ret)
+		return ret;
+
+	iova = PAGE_ALIGN(vals[0]);
+	sram_used = PAGE_ALIGN(vals[1]);
+	if (!sram_used) {
+		dev_err(dev, "sram rcb invalid\n");
+		return -EINVAL;
+	}
+
+	sram_np = of_parse_phandle(dev->of_node, "rockchip,sram", 0);
+	if (!sram_np) {
+		dev_err(dev, "could not find phandle sram\n");
+		return -ENODEV;
+	}
+
+	ret = of_address_to_resource(sram_np, 0, &sram_res);
+	of_node_put(sram_np);
+	if (ret) {
+		dev_err(dev, "find sram res error\n");
+		return ret;
+	}
+
+	sram_start = round_up(sram_res.start, PAGE_SIZE);
+	sram_end = round_down(sram_res.start + resource_size(&sram_res), PAGE_SIZE);
+	if (sram_end <= sram_start) {
+		dev_err(dev, "no available sram\n");
+		return -ENOMEM;
+	}
+	sram_size = sram_end - sram_start;
+	sram_size = sram_used < sram_size ? sram_used : sram_size;
+
+	if (!enc->iommu_info || !enc->iommu_info->domain)
+		return -ENODEV;
+
+	domain = enc->iommu_info->domain;
+	ret = iommu_map(domain, iova, sram_start, sram_size, IOMMU_READ | IOMMU_WRITE,
+			GFP_KERNEL);
+	if (ret) {
+		dev_err(dev, "sram iommu_map error\n");
+		return ret;
+	}
+
+	if (sram_size < sram_used) {
+		struct page *page;
+		size_t page_size = PAGE_ALIGN(sram_used - sram_size);
+
+		page = alloc_pages(GFP_KERNEL | __GFP_ZERO, get_order(page_size));
+		if (!page) {
+			dev_err(dev, "unable to allocate pages\n");
+			iommu_unmap(domain, iova, sram_size);
+			return -ENOMEM;
+		}
+		ret = iommu_map(domain, iova + sram_size, page_to_phys(page),
+				page_size, IOMMU_READ | IOMMU_WRITE, GFP_KERNEL);
+		if (ret) {
+			dev_err(dev, "page iommu_map error\n");
+			__free_pages(page, get_order(page_size));
+			iommu_unmap(domain, iova, sram_size);
+			return ret;
+		}
+		enc->rcb_page = page;
+	}
+
+	enc->sram_size = sram_size;
+	enc->sram_used = sram_used;
+	enc->sram_iova = iova;
+	enc->sram_enabled = -1;
+	dev_info(dev, "sram iova %pad size %u used %u\n",
+		 &enc->sram_iova, enc->sram_size, enc->sram_used);
+
+	return 0;
+}
+
+/* ---- Core probe ---- */
+static int rkvenc_core_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct device *dev = &pdev->dev;
+	struct rkvenc_dev *enc;
+
+	enc = devm_kzalloc(dev, sizeof(*enc), GFP_KERNEL);
+	if (!enc)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, enc);
+
+	/* Get core ID from alias, falling back to DTS property */
+	enc->core_id = of_alias_get_id(dev->of_node, "rkvenc");
+	if (enc->core_id < 0) {
+		u32 core_id = 0;
+
+		of_property_read_u32(dev->of_node, "rockchip,core-id", &core_id);
+		enc->core_id = core_id;
+	}
+
+	/* HW probe: clocks, resets, IOMMU, register space */
+	ret = rkvenc_hw_probe(enc, pdev);
+	if (ret)
+		return ret;
+
+	/* Attach to service */
+	ret = rkvenc_attach_service(enc);
+	if (ret) {
+		dev_err_probe(dev, ret, "failed to attach service\n");
+		goto err_hw;
+	}
+
+	/* Attach core to CCU */
+	ret = rkvenc_attach_ccu(dev, enc);
+	if (ret) {
+		dev_err_probe(dev, ret, "attach ccu failed\n");
+		goto err_hw;
+	}
+
+	/* Try SRAM allocation (optional, non-fatal) */
+	rkvenc2_alloc_rcbbuf(pdev, enc);
+
+	/* Register IRQ */
+	ret = devm_request_irq(dev, enc->irq, rkvenc_hw_irq,
+			       IRQF_SHARED, dev_name(dev), enc);
+	if (ret) {
+		dev_err(dev, "register interrupt failed: %d\n", ret);
+		goto err_hw;
+	}
+
+	/* If this is the main core, register with the service */
+	if (enc->ccu && enc == enc->ccu->main_core) {
+		enc->srv->sub_devices[MPP_DEVICE_RKVENC] = enc;
+		set_bit(MPP_DEVICE_RKVENC, &enc->srv->hw_support);
+	}
+
+	dev_info(dev, "rkvenc core %d probe success (hw_id: %08x)\n",
+		 enc->core_id, enc->hw_info->hw.hw_id);
+	return 0;
+
+err_hw:
+	rkvenc_hw_remove(enc);
+	return ret;
+}
+
+/* ---- Core remove ---- */
+static int rkvenc_core_remove(struct platform_device *pdev)
+{
+	struct rkvenc_dev *enc = platform_get_drvdata(pdev);
+
+	if (!enc)
+		return 0;
+
+	if (enc->ccu) {
+		mutex_lock(&enc->ccu->lock);
+		list_del_init(&enc->core_link);
+		enc->ccu->core_num--;
+		mutex_unlock(&enc->ccu->lock);
+	}
+
+	/* Free SRAM */
+	if (enc->sram_iova && enc->iommu_info && enc->iommu_info->domain) {
+		struct iommu_domain *domain = enc->iommu_info->domain;
+
+		if (enc->rcb_page) {
+			size_t page_size = PAGE_ALIGN(enc->sram_used - enc->sram_size);
+
+			iommu_unmap(domain, enc->sram_iova + enc->sram_size, page_size);
+			__free_pages(enc->rcb_page, get_order(page_size));
+		}
+		iommu_unmap(domain, enc->sram_iova, enc->sram_size);
+	}
+
+	rkvenc_hw_remove(enc);
+	dev_info(&pdev->dev, "rkvenc core removed\n");
+	return 0;
+}
+
+/* ---- Top-level probe dispatcher ---- */
+static int rkvenc_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	dev_info(dev, "probing start\n");
+
+	if (strstr(np->name, "ccu"))
+		return rkvenc_ccu_probe(pdev);
+	else if (strstr(np->name, "core"))
+		return rkvenc_core_probe(pdev);
+	else if (of_device_is_compatible(np, "rockchip,mpp-service"))
+		return rkvenc_service_probe(pdev);
+
+	dev_err(dev, "unknown node type: %s\n", np->name);
+	return -ENODEV;
+}
+
+static void rkvenc_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	if (strstr(np->name, "ccu")) {
+		dev_info(dev, "remove ccu\n");
+	} else if (strstr(np->name, "core")) {
+		rkvenc_core_remove(pdev);
+	} else if (of_device_is_compatible(np, "rockchip,mpp-service")) {
+		rkvenc_service_remove(pdev);
+	}
+}
+
+static void rkvenc_shutdown(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+	struct rkvenc_dev *enc;
+	int ret, val;
+
+	if (strstr(np->name, "ccu"))
+		return;
+	if (of_device_is_compatible(np, "rockchip,mpp-service"))
+		return;
+
+	enc = platform_get_drvdata(pdev);
+	if (!enc || !enc->srv)
+		return;
+
+	dev_info(dev, "shutdown device\n");
+	atomic_inc(&enc->srv->shutdown_request);
+
+	ret = readx_poll_timeout(atomic_read, &enc->task_count,
+				 val, val == 0, 20000, 200000);
+	if (ret == -ETIMEDOUT)
+		dev_err(dev, "wait total %d running time out\n",
+			atomic_read(&enc->task_count));
+	else
+		dev_info(dev, "shutdown success\n");
+}
+
+/* ---- OF match table ---- */
+static const struct of_device_id rkvenc_dt_match[] = {
+	{ .compatible = "rockchip,mpp-service" },
+	{ .compatible = "rockchip,rkv-encoder-v2-ccu" },
+	{ .compatible = "rockchip,rkv-encoder-v2-core" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, rkvenc_dt_match);
+
+static struct platform_driver rkvenc_driver = {
+	.probe = rkvenc_probe,
+	.remove = rkvenc_remove,
+	.shutdown = rkvenc_shutdown,
+	.driver = {
+		.name = MPP_DRIVER_NAME,
+		.of_match_table = rkvenc_dt_match,
+		.pm = &rkvenc_pm_ops,
+	},
+};
+
+module_platform_driver(rkvenc_driver);
+
+MODULE_DESCRIPTION("Rockchip VEPU580 (RKVENC v2) H.265/H.264/JPEG encoder driver");
+MODULE_LICENSE("Dual MIT/GPL");
+MODULE_AUTHOR("Rockchip Electronics Co., Ltd.");
+MODULE_IMPORT_NS("DMA_BUF");
diff -urN a/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.c
--- a/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.c	2026-02-08 02:36:38
@@ -0,0 +1,885 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - Hardware operations
+ * Ported from Rockchip BSP mpp_rkvenc2.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ * Copyright (C) 2026 Ross Cawston
+ */
+
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/ioport.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/pm_runtime.h>
+#include <linux/reset.h>
+#include <linux/slab.h>
+
+#include "rkvenc_hw.h"
+
+unsigned int rkvenc_debug;
+module_param_named(debug, rkvenc_debug, uint, 0644);
+MODULE_PARM_DESC(debug, "Debug level bitmask");
+
+/* ---- FD translation tables for VEPU580 ---- */
+static const u16 trans_tbl_h264e_v2[] = {
+	0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
+	10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
+	20, 21, 22, 23,
+};
+
+static const u16 trans_tbl_h265e_v2[] = {
+	0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
+	10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
+	20, 21, 22, 23,
+};
+
+static const u16 trans_tbl_jpege_v2[] = {
+	5, 6, 7, 8, 9, 10, 11, 12, 13, 14,
+	15, 16,
+};
+
+static const u16 trans_tbl_h264e_v2_osd[] = {
+	20, 21, 22, 23, 24, 25, 26, 27,
+};
+
+static const u16 trans_tbl_h265e_v2_osd[] = {
+	20, 21, 22, 23, 24, 25, 26, 27,
+};
+
+static const u16 trans_tbl_jpege_v2_osd[] = {
+	3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
+	13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
+	23, 24, 25, 26, 27, 28, 29, 30, 31, 32,
+};
+
+const struct rkvenc_trans_info trans_rkvenc_v2[] = {
+	[RKVENC_FMT_H264E] = {
+		.count = ARRAY_SIZE(trans_tbl_h264e_v2),
+		.table = trans_tbl_h264e_v2,
+	},
+	[RKVENC_FMT_H265E] = {
+		.count = ARRAY_SIZE(trans_tbl_h265e_v2),
+		.table = trans_tbl_h265e_v2,
+	},
+	[RKVENC_FMT_JPEGE] = {
+		.count = ARRAY_SIZE(trans_tbl_jpege_v2),
+		.table = trans_tbl_jpege_v2,
+	},
+	[RKVENC_FMT_H264E_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_h264e_v2_osd),
+		.table = trans_tbl_h264e_v2_osd,
+	},
+	[RKVENC_FMT_H265E_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_h265e_v2_osd),
+		.table = trans_tbl_h265e_v2_osd,
+	},
+	[RKVENC_FMT_JPEGE_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_jpege_v2_osd),
+		.table = trans_tbl_jpege_v2_osd,
+	},
+};
+
+/* ---- VEPU580 HW info ---- */
+struct rkvenc_hw_info rkvenc_v2_hw_info = {
+	.hw = {
+		.reg_num = 254,
+		.reg_id = 0,
+		.reg_en = 4,
+		.reg_start = 160,
+		.reg_end = 253,
+	},
+	.reg_class = RKVENC_CLASS_BUTT,
+	.reg_msg = {
+		[RKVENC_CLASS_BASE] = { .base_s = 0x0000, .base_e = 0x0058 },
+		[RKVENC_CLASS_PIC]  = { .base_s = 0x0280, .base_e = 0x03f4 },
+		[RKVENC_CLASS_RC]   = { .base_s = 0x1000, .base_e = 0x10e0 },
+		[RKVENC_CLASS_PAR]  = { .base_s = 0x1700, .base_e = 0x1cd4 },
+		[RKVENC_CLASS_SQI]  = { .base_s = 0x2000, .base_e = 0x21e4 },
+		[RKVENC_CLASS_SCL]  = { .base_s = 0x2200, .base_e = 0x2c98 },
+		[RKVENC_CLASS_OSD]  = { .base_s = 0x3000, .base_e = 0x347c },
+		[RKVENC_CLASS_ST]   = { .base_s = 0x4000, .base_e = 0x42cc },
+		[RKVENC_CLASS_DBG]  = { .base_s = 0x5000, .base_e = 0x5354 },
+	},
+	.fd_class = RKVENC_CLASS_FD_BUTT,
+	.fd_reg = {
+		[RKVENC_CLASS_FD_BASE] = {
+			.class = RKVENC_CLASS_PIC,
+			.base_fmt = RKVENC_FMT_BASE,
+		},
+		[RKVENC_CLASS_FD_OSD] = {
+			.class = RKVENC_CLASS_OSD,
+			.base_fmt = RKVENC_FMT_OSD_BASE,
+		},
+	},
+	.fmt_reg = {
+		.class = RKVENC_CLASS_PIC,
+		.base = 0x0300,
+		.bitpos = 0,
+		.bitlen = 1,
+	},
+	.enc_start_base = 0x0010,
+	.enc_clr_base = 0x0014,
+	.int_en_base = 0x0020,
+	.int_mask_base = 0x0024,
+	.int_clr_base = 0x0028,
+	.int_sta_base = 0x002c,
+	.enc_wdg_base = 0x0038,
+	.err_mask = 0x03f0,
+	.enc_rsl = 0x0310,
+	.dcsh_class_ofst = 33,
+	.vepu_type = RKVENC_VEPU_580,
+};
+
+/* ---- Timeout thresholds by resolution ---- */
+static const u32 rkvenc2_timeout_thd_by_rsl[][2] = {
+	{  3840 * 2160, 200 },
+	{  7680 * 4320, 500 },
+	{ 0xffffffff,   800 },
+};
+
+/* ---- Class register helpers ---- */
+static int rkvenc_get_class_msg(struct rkvenc_task *task,
+				u32 addr, struct mpp_request *msg)
+{
+	int i;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+
+	if (!msg)
+		return -EINVAL;
+
+	memset(msg, 0, sizeof(*msg));
+	for (i = 0; i < hw->reg_class; i++) {
+		u32 base_s = hw->reg_msg[i].base_s;
+		u32 base_e = hw->reg_msg[i].base_e;
+
+		if (addr >= base_s && addr < base_e) {
+			msg->offset = base_s;
+			msg->size = task->reg[i].size;
+			msg->data = task->reg[i].data;
+			return 0;
+		}
+	}
+
+	return -EINVAL;
+}
+
+static u32 *rkvenc_get_class_reg(struct rkvenc_task *task, u32 addr)
+{
+	int i;
+	u8 *reg = NULL;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+
+	for (i = 0; i < hw->reg_class; i++) {
+		u32 base_s = hw->reg_msg[i].base_s;
+		u32 base_e = hw->reg_msg[i].base_e;
+
+		if (addr >= base_s && addr < base_e) {
+			reg = (u8 *)task->reg[i].data + (addr - base_s);
+			break;
+		}
+	}
+
+	return (u32 *)reg;
+}
+
+/* ---- DCHS (Dual-Core Handshake) ---- */
+static void rkvenc2_patch_dchs(struct rkvenc_dev *enc, struct rkvenc_task *task)
+{
+	struct rkvenc_ccu *ccu;
+	union rkvenc2_dual_core_handshake_id *dchs;
+	union rkvenc2_dual_core_handshake_id *task_dchs = &task->dchs_id;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	int core_num;
+	int core_id = enc->core_id;
+	unsigned long flags;
+	int i;
+
+	if (!enc->ccu)
+		return;
+
+	if (core_id >= RKVENC_MAX_CORE_NUM) {
+		dev_err(enc->dev, "invalid core id %d max %d\n",
+			core_id, RKVENC_MAX_CORE_NUM);
+		return;
+	}
+
+	ccu = enc->ccu;
+	dchs = ccu->dchs;
+	core_num = ccu->core_num;
+
+	spin_lock_irqsave(&ccu->lock_dchs, flags);
+
+	if (dchs[core_id].working) {
+		spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+		rkvenc_err("can not config when core %d is still working\n", core_id);
+		return;
+	}
+
+	/* Find free TX/RX IDs */
+	{
+		unsigned long id_valid = (unsigned long)-1;
+		int txid_map = -1;
+		int rxid_map = -1;
+
+		for (i = 0; i < core_num; i++) {
+			if (!dchs[i].working)
+				continue;
+			clear_bit(dchs[i].txid_map, &id_valid);
+			clear_bit(dchs[i].rxid_map, &id_valid);
+		}
+
+		if (task_dchs->rxe) {
+			for (i = 0; i < core_num; i++) {
+				if (i == core_id || !dchs[i].working)
+					continue;
+				if (task_dchs->session_id != dchs[i].session_id)
+					continue;
+				if (task_dchs->rxid_orig != dchs[i].txid_orig)
+					continue;
+				rxid_map = dchs[i].txid_map;
+				break;
+			}
+		}
+
+		txid_map = find_first_bit(&id_valid, RKVENC_MAX_DCHS_ID);
+		if (txid_map == RKVENC_MAX_DCHS_ID) {
+			spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+			rkvenc_err("failed to find a txid\n");
+			return;
+		}
+
+		clear_bit(txid_map, &id_valid);
+		task_dchs->txid_map = txid_map;
+
+		if (rxid_map < 0) {
+			rxid_map = find_first_bit(&id_valid, RKVENC_MAX_DCHS_ID);
+			if (rxid_map == RKVENC_MAX_DCHS_ID) {
+				spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+				rkvenc_err("failed to find a rxid\n");
+				return;
+			}
+			task_dchs->rxe_map = 0;
+		}
+
+		task_dchs->rxid_map = rxid_map;
+	}
+
+	task_dchs->txid = task_dchs->txid_map;
+	task_dchs->rxid = task_dchs->rxid_map;
+	task_dchs->rxe = task_dchs->rxe_map;
+
+	dchs[core_id].val[0] = task_dchs->val[0];
+	dchs[core_id].val[1] = task_dchs->val[1];
+	task->reg[RKVENC_CLASS_PIC].data[hw->dcsh_class_ofst] = task_dchs->val[0];
+
+	dchs[core_id].working = 1;
+
+	spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+}
+
+static void __maybe_unused rkvenc2_update_dchs(struct rkvenc_dev *enc,
+					     struct rkvenc_task *task)
+{
+	struct rkvenc_ccu *ccu = enc->ccu;
+	int core_id = enc->core_id;
+	unsigned long flags;
+
+	if (!ccu)
+		return;
+
+	if (core_id >= RKVENC_MAX_CORE_NUM) {
+		dev_err(enc->dev, "invalid core id %d max %d\n",
+			core_id, RKVENC_MAX_CORE_NUM);
+		return;
+	}
+
+	spin_lock_irqsave(&ccu->lock_dchs, flags);
+	ccu->dchs[core_id].val[0] = 0;
+	ccu->dchs[core_id].val[1] = 0;
+	spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+}
+
+/* ---- Timeout threshold calculation ---- */
+static void rkvenc2_calc_timeout_thd(struct rkvenc_dev *enc)
+{
+	const struct rkvenc_hw_info *hw = enc->hw_info;
+	union rkvenc2_frame_resolution frm_rsl;
+	u32 timeout_ms = 0;
+	u32 timeout_thd = 0;
+	u32 timeout_thd_cnt;
+	u32 i;
+
+	timeout_thd = rkvenc_read(enc, RKVENC_WDG) & 0xff000000;
+	frm_rsl.val = rkvenc_read(enc, hw->enc_rsl);
+	frm_rsl.val = (frm_rsl.pic_wd8 + 1) * (frm_rsl.pic_hd8 + 1) * 64;
+
+	timeout_thd_cnt = ARRAY_SIZE(rkvenc2_timeout_thd_by_rsl);
+	for (i = 0; i < timeout_thd_cnt; i++) {
+		if (frm_rsl.val <= rkvenc2_timeout_thd_by_rsl[i][0]) {
+			timeout_ms = rkvenc2_timeout_thd_by_rsl[i][1];
+			break;
+		}
+	}
+
+	/* Use x1024 core clk cycles for VEPU580 */
+	if (enc->core_clk_info.clk)
+		timeout_thd |= timeout_ms * clk_get_rate(enc->core_clk_info.clk) / 1024000;
+
+	rkvenc_write(enc, RKVENC_WDG, timeout_thd);
+}
+
+/* ---- Slice reading ---- */
+static void rkvenc2_read_slice_len(struct rkvenc_dev *mpp,
+				   struct rkvenc_task *task,
+				   u32 *irq_status)
+{
+	const struct rkvenc_hw_info *hw = mpp->hw_info;
+	u32 sli_num = rkvenc_read_relaxed(mpp, RKVENC2_REG_SLICE_NUM_BASE) & 0x3f;
+	u32 new_irq_status = rkvenc_read(mpp, hw->int_sta_base);
+	union rkvenc2_slice_len_info slice_info;
+	u32 i;
+	u32 last = 0;
+	u32 split = task->task_split;
+
+	if ((new_irq_status != *irq_status) && (new_irq_status & INT_STA_ENC_DONE_STA)) {
+		*irq_status |= new_irq_status;
+		sli_num = rkvenc_read_relaxed(mpp, RKVENC2_REG_SLICE_NUM_BASE) & 0x3f;
+		rkvenc_write(mpp, hw->int_clr_base, new_irq_status);
+	}
+
+	last = *irq_status & INT_STA_ENC_DONE_STA;
+
+	for (i = 0; i < sli_num; i++) {
+		slice_info.val = rkvenc_read_relaxed(mpp, RKVENC2_REG_SLICE_LEN_BASE);
+		last |= slice_info.last;
+		if (last && i == sli_num - 1) {
+			task->last_slice_found = 1;
+			break;
+		}
+
+		if (split) {
+			kfifo_in(&task->slice_info, &slice_info, 1);
+			task->slice_wr_cnt++;
+		}
+	}
+
+	if (split) {
+		if (last && !task->last_slice_found) {
+			slice_info.last = 1;
+			slice_info.slice_len = 0;
+			kfifo_in(&task->slice_info, &slice_info, 1);
+		}
+	}
+}
+
+/* ---- Bitstream overflow handling ---- */
+static void rkvenc2_bs_overflow_handle(struct rkvenc_dev *mpp)
+{
+	struct rkvenc_mpp_task *mpp_task = mpp->cur_task;
+	u32 bs_rd, bs_wr, bs_top, bs_bot;
+
+	bs_rd = rkvenc_read(mpp, RKVENC580_REG_ADR_BSBR);
+	bs_wr = rkvenc_read(mpp, RKVENC2_REG_ST_BSB);
+	bs_top = rkvenc_read(mpp, RKVENC2_REG_ADR_BSBT);
+	bs_bot = rkvenc_read(mpp, RKVENC2_REG_ADR_BSBB);
+
+	bs_wr += 128;
+	if (bs_wr >= bs_top)
+		bs_wr = bs_bot;
+	rkvenc_write(mpp, RKVENC2_REG_ADR_BSBS, bs_wr);
+
+	if (mpp_task)
+		dev_err(mpp->dev, "task %d bitstream overflow [top=%#08x bot=%#08x wr=%#08x rd=%#08x]\n",
+			mpp_task->task_index, bs_top, bs_bot, bs_wr, bs_rd);
+}
+
+/* ---- IRQ handler ---- */
+irqreturn_t rkvenc_hw_irq(int irq, void *param)
+{
+	struct rkvenc_dev *enc = param;
+	const struct rkvenc_hw_info *hw = enc->hw_info;
+	struct rkvenc_mpp_task *mpp_task = NULL;
+	struct rkvenc_task *task = NULL;
+	u32 irq_status;
+	int ret = IRQ_NONE;
+
+	irq_status = rkvenc_read(enc, hw->int_sta_base);
+
+	if (!irq_status)
+		return ret;
+
+	/* clear int first */
+	rkvenc_write(enc, hw->int_clr_base, irq_status);
+
+	/* prevent watch dog irq storm */
+	if (irq_status & INT_STA_WDG_STA)
+		rkvenc_write(enc, hw->int_mask_base, INT_STA_WDG_STA);
+
+	if (enc->cur_task) {
+		mpp_task = enc->cur_task;
+		task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+	}
+
+	/* 1. slice split read */
+	if (task && task->task_split &&
+	    (irq_status & (INT_STA_SLC_DONE_STA | INT_STA_ENC_DONE_STA))) {
+		rkvenc2_read_slice_len(enc, task, &irq_status);
+		wake_up(&mpp_task->wait);
+	}
+
+	/* 2. process slice irq */
+	if (irq_status & INT_STA_SLC_DONE_STA)
+		ret = IRQ_HANDLED;
+
+	/* 3. process bitstream overflow */
+	if (irq_status & INT_STA_BSF_OFLW_STA) {
+		rkvenc2_bs_overflow_handle(enc);
+		enc->bs_overflow = 1;
+		ret = IRQ_HANDLED;
+	}
+
+	/* 4. process frame done irq */
+	if (irq_status & INT_STA_ENC_DONE_STA) {
+		enc->irq_status = irq_status;
+
+		if (enc->bs_overflow) {
+			enc->irq_status |= INT_STA_BSF_OFLW_STA;
+			enc->bs_overflow = 0;
+		}
+
+		if (mpp_task) {
+			if (test_and_set_bit(TASK_STATE_HANDLE, &mpp_task->state)) {
+				dev_err(enc->dev, "error, task %d already handled, irq %#x\n",
+					mpp_task->task_index, enc->irq_status);
+				ret = IRQ_HANDLED;
+				goto done;
+			}
+			cancel_delayed_work(&mpp_task->timeout_work);
+			set_bit(TASK_STATE_IRQ, &mpp_task->state);
+			mpp_task->irq_status = enc->irq_status;
+			rkvenc_iommu_dev_deactivate(enc->iommu_info, enc);
+
+			/* Trigger the worker to process the ISR bottom half */
+			kthread_queue_work(&enc->queue->worker, &enc->work);
+		}
+
+		ret = IRQ_HANDLED;
+	}
+
+	/* 5. process error irq */
+	if (irq_status & INT_STA_ERROR) {
+		enc->irq_status = irq_status;
+		dev_err(enc->dev, "error status %08x\n", irq_status);
+
+		if (mpp_task) {
+			if (!test_and_set_bit(TASK_STATE_HANDLE, &mpp_task->state)) {
+				cancel_delayed_work(&mpp_task->timeout_work);
+				set_bit(TASK_STATE_IRQ, &mpp_task->state);
+				mpp_task->irq_status = enc->irq_status;
+				rkvenc_iommu_dev_deactivate(enc->iommu_info, enc);
+				kthread_queue_work(&enc->queue->worker, &enc->work);
+			}
+		}
+
+		ret = IRQ_HANDLED;
+	}
+
+done:
+	return ret;
+}
+
+/* ---- Run task: write registers and start HW ---- */
+int rkvenc_hw_run(struct rkvenc_dev *enc, struct rkvenc_mpp_task *mpp_task)
+{
+	u32 i, j;
+	u32 start_val = 0;
+	int ret;
+	struct rkvenc_task *task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+	const struct rkvenc_hw_info *hw = enc->hw_info;
+
+	rkvenc_debug_enter();
+
+	/* Power on encoder and its IOMMU */
+	if (enc->iommu_info && enc->iommu_info->pdev)
+		pm_runtime_get_sync(&enc->iommu_info->pdev->dev);
+	pm_runtime_get_sync(enc->dev);
+	pm_stay_awake(enc->dev);
+	rkvenc_hw_clk_on(enc);
+
+	/* Reset group down_read if available */
+	if (enc->reset_group)
+		down_read(&enc->reset_group->rw_sem);
+
+	/* Match BSP: ensure IOMMU is attached and fault handler is active */
+	ret = rkvenc_iommu_attach(enc->iommu_info);
+	if (ret) {
+		rkvenc_err("iommu attach failed: %d\n", ret);
+		goto err_unlock;
+	}
+
+	ret = rkvenc_iommu_dev_activate(enc->iommu_info, enc);
+	if (ret) {
+		rkvenc_err("iommu activate failed: %d\n", ret);
+		goto err_unlock;
+	}
+
+	/* Add force clear to avoid pagefault (VEPU580 workaround) */
+	if (hw->vepu_type == RKVENC_VEPU_580) {
+		rkvenc_write(enc, hw->enc_clr_base, 0x2);
+		udelay(5);
+		rkvenc_write(enc, hw->enc_clr_base, 0x0);
+	}
+
+	/* Clear hardware counter */
+	rkvenc_write_relaxed(enc, 0x5300, 0x2);
+
+	/* Patch dual-core handshake IDs */
+	rkvenc2_patch_dchs(enc, task);
+
+	/* Write all class registers except enc_start */
+	for (i = 0; i < task->w_req_cnt; i++) {
+		u32 s, e;
+		u32 *regs;
+		struct mpp_request msg;
+		struct mpp_request *req = &task->w_reqs[i];
+
+		ret = rkvenc_get_class_msg(task, req->offset, &msg);
+		if (ret)
+			goto err_deactivate;
+
+		s = (req->offset - msg.offset) / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		regs = (u32 *)msg.data;
+		for (j = s; j < e; j++) {
+			u32 off = msg.offset + j * sizeof(u32);
+
+			if (off == hw->enc_start_base) {
+				start_val = regs[j];
+				continue;
+			}
+			rkvenc_write_relaxed(enc, off, regs[j]);
+		}
+	}
+
+	/* flush tlb before starting hardware */
+	rkvenc_iommu_flush_tlb(enc->iommu_info);
+
+	/* init current task */
+	enc->cur_task = mpp_task;
+
+	/* Calculate and write watchdog timeout threshold */
+	rkvenc2_calc_timeout_thd(enc);
+
+	/* Schedule timeout work */
+	INIT_DELAYED_WORK(&mpp_task->timeout_work, rkvenc_task_timeout_work);
+	schedule_delayed_work(&mpp_task->timeout_work,
+			      msecs_to_jiffies(MPP_WORK_TIMEOUT_DELAY));
+
+	/* memory barrier before starting HW */
+	wmb();
+	rkvenc_write(enc, hw->enc_start_base, start_val);
+
+	rkvenc_debug_leave();
+
+	return 0;
+
+err_deactivate:
+	rkvenc_iommu_dev_deactivate(enc->iommu_info, enc);
+err_unlock:
+	if (enc->reset_group)
+		up_read(&enc->reset_group->rw_sem);
+	rkvenc_hw_clk_off(enc);
+	pm_relax(enc->dev);
+	pm_runtime_mark_last_busy(enc->dev);
+	pm_runtime_put_autosuspend(enc->dev);
+	if (enc->iommu_info && enc->iommu_info->pdev)
+		pm_runtime_put_sync(&enc->iommu_info->pdev->dev);
+
+	return ret;
+}
+
+/* ---- Finish: read status registers back from HW ---- */
+int rkvenc_hw_finish(struct rkvenc_dev *mpp, struct rkvenc_mpp_task *mpp_task)
+{
+	u32 i, j;
+	u32 *reg;
+	struct rkvenc_task *task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+
+	rkvenc_debug_enter();
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		int ret;
+		int s, e;
+		struct mpp_request msg;
+		struct mpp_request *req = &task->r_reqs[i];
+
+		ret = rkvenc_get_class_msg(task, req->offset, &msg);
+		if (ret)
+			return -EINVAL;
+		s = (req->offset - msg.offset) / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		reg = (u32 *)msg.data;
+		for (j = s; j < e; j++)
+			reg[j] = rkvenc_read_relaxed(mpp, msg.offset + j * sizeof(u32));
+	}
+
+	/* Sync bitstream buffer if present */
+	if (task->bs_buf) {
+		u32 bs_size = rkvenc_read(mpp, 0x4064);
+
+		rkvenc_dma_buf_sync(task->bs_buf, 0, bs_size + task->offset_bs,
+				    DMA_FROM_DEVICE, true);
+	}
+
+	/* Revert irq status register for userspace readback */
+	reg = rkvenc_get_class_reg(task, task->hw_info->int_sta_base);
+	if (reg)
+		*reg = task->irq_status;
+
+	rkvenc_debug_leave();
+
+	return 0;
+}
+
+/* ---- Soft reset ---- */
+static int rkvenc_soft_reset(struct rkvenc_dev *enc)
+{
+	const struct rkvenc_hw_info *hw = enc->hw_info;
+	u32 rst_status = 0;
+	int ret;
+
+	/* safe reset */
+	rkvenc_write(enc, hw->int_mask_base, 0x3FF);
+	rkvenc_write(enc, hw->enc_clr_base, 0x1);
+	ret = readl_relaxed_poll_timeout(enc->reg_base + hw->int_sta_base,
+					 rst_status,
+					 rst_status & RKVENC_SCLR_DONE_STA,
+					 0, 5);
+	if (ret)
+		rkvenc_err("safe reset failed\n");
+	rkvenc_write(enc, hw->enc_clr_base, 0x2);
+	udelay(5);
+	rkvenc_write(enc, hw->enc_clr_base, 0);
+	rkvenc_write(enc, hw->int_clr_base, 0xffffffff);
+	rkvenc_write(enc, hw->int_sta_base, 0);
+
+	return ret;
+}
+
+/* ---- Full reset (soft + CRU fallback) ---- */
+int rkvenc_hw_reset(struct rkvenc_dev *enc)
+{
+	int ret;
+	struct rkvenc_taskqueue *queue = enc->queue;
+	struct rkvenc_ccu *ccu = enc->ccu;
+	unsigned long flags;
+
+	/* safe reset first */
+	ret = rkvenc_soft_reset(enc);
+
+	/* cru reset as fallback */
+	if (ret && enc->rst_a && enc->rst_h && enc->rst_core) {
+		rkvenc_err("soft reset timeout, use cru reset\n");
+		rkvenc_safe_reset(enc->rst_a);
+		rkvenc_safe_reset(enc->rst_h);
+		rkvenc_safe_reset(enc->rst_core);
+		udelay(5);
+		rkvenc_safe_unreset(enc->rst_a);
+		rkvenc_safe_unreset(enc->rst_h);
+		rkvenc_safe_unreset(enc->rst_core);
+
+		/*
+		 * CRU reset wipes the IOMMU registers (DTE, paging).
+		 * Force a suspendresume cycle on the IOMMU device to
+		 * re-program them via rk_iommu_enable.
+		 */
+		if (enc->iommu_info && enc->iommu_info->pdev) {
+			struct device *iommu_dev = &enc->iommu_info->pdev->dev;
+
+			pm_runtime_put_sync(iommu_dev);
+			pm_runtime_get_sync(iommu_dev);
+		}
+	}
+
+	set_bit(enc->core_id, &queue->core_idle);
+
+	if (ccu) {
+		spin_lock_irqsave(&ccu->lock_dchs, flags);
+		ccu->dchs[enc->core_id].val[0] = 0;
+		ccu->dchs[enc->core_id].val[1] = 0;
+		spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+	}
+
+	atomic_set(&enc->reset_request, 0);
+
+	return 0;
+}
+
+/* ---- Clock on/off ---- */
+void rkvenc_hw_clk_on(struct rkvenc_dev *enc)
+{
+	rkvenc_clk_safe_enable(enc->aclk_info.clk);
+	rkvenc_clk_safe_enable(enc->hclk_info.clk);
+	rkvenc_clk_safe_enable(enc->core_clk_info.clk);
+}
+
+void rkvenc_hw_clk_off(struct rkvenc_dev *enc)
+{
+	clk_disable_unprepare(enc->aclk_info.clk);
+	clk_disable_unprepare(enc->hclk_info.clk);
+	clk_disable_unprepare(enc->core_clk_info.clk);
+}
+
+/* ---- HW init during probe ---- */
+static int rkvenc_hw_init_clocks(struct rkvenc_dev *enc)
+{
+	int ret;
+	struct device *dev = enc->dev;
+
+	enc->aclk_info.clk = devm_clk_get(dev, "aclk_vcodec");
+	if (IS_ERR(enc->aclk_info.clk)) {
+		ret = PTR_ERR(enc->aclk_info.clk);
+		enc->aclk_info.clk = NULL;
+		dev_err(dev, "failed to get aclk_vcodec: %d\n", ret);
+	}
+
+	enc->hclk_info.clk = devm_clk_get(dev, "hclk_vcodec");
+	if (IS_ERR(enc->hclk_info.clk)) {
+		ret = PTR_ERR(enc->hclk_info.clk);
+		enc->hclk_info.clk = NULL;
+		dev_err(dev, "failed to get hclk_vcodec: %d\n", ret);
+	}
+
+	enc->core_clk_info.clk = devm_clk_get(dev, "clk_core");
+	if (IS_ERR(enc->core_clk_info.clk)) {
+		ret = PTR_ERR(enc->core_clk_info.clk);
+		enc->core_clk_info.clk = NULL;
+		dev_err(dev, "failed to get clk_core: %d\n", ret);
+	}
+
+	/* Set default rates */
+	enc->aclk_info.default_rate_hz = 300000000;
+	enc->core_clk_info.default_rate_hz = 600000000;
+
+	return 0;
+}
+
+static int rkvenc_hw_init_resets(struct rkvenc_dev *enc)
+{
+	struct device *dev = enc->dev;
+
+	enc->rst_a = devm_reset_control_get_optional(dev, "video_a");
+	if (IS_ERR(enc->rst_a)) {
+		enc->rst_a = NULL;
+		dev_warn(dev, "no video_a reset\n");
+	}
+
+	enc->rst_h = devm_reset_control_get_optional(dev, "video_h");
+	if (IS_ERR(enc->rst_h)) {
+		enc->rst_h = NULL;
+		dev_warn(dev, "no video_h reset\n");
+	}
+
+	enc->rst_core = devm_reset_control_get_optional(dev, "video_core");
+	if (IS_ERR(enc->rst_core)) {
+		enc->rst_core = NULL;
+		dev_warn(dev, "no video_core reset\n");
+	}
+
+	return 0;
+}
+
+int rkvenc_hw_probe(struct rkvenc_dev *enc, struct platform_device *pdev)
+{
+	int ret;
+	struct resource *res;
+	struct device *dev = &pdev->dev;
+
+	enc->dev = dev;
+
+	/* Read task-capacity from DTS */
+	ret = of_property_read_u32(dev->of_node, "rockchip,task-capacity",
+				   &enc->task_capacity);
+	if (ret)
+		enc->task_capacity = 1;
+
+	/* PM runtime */
+	pm_runtime_set_autosuspend_delay(dev, 2000);
+	pm_runtime_use_autosuspend(dev);
+	device_init_wakeup(dev, true);
+	pm_runtime_enable(dev);
+
+	/* IRQ */
+	enc->irq = platform_get_irq(pdev, 0);
+	if (enc->irq < 0) {
+		dev_err(dev, "no interrupt resource found\n");
+		ret = -ENODEV;
+		goto failed;
+	}
+
+	/* Register space */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(dev, "no memory resource defined\n");
+		ret = -ENODEV;
+		goto failed;
+	}
+	enc->reg_base = devm_ioremap(dev, res->start, resource_size(res));
+	if (!enc->reg_base) {
+		dev_err(dev, "ioremap failed for resource %pR\n", res);
+		ret = -ENOMEM;
+		goto failed;
+	}
+	enc->io_base = res->start;
+
+	/* Clocks and resets */
+	rkvenc_hw_init_clocks(enc);
+	rkvenc_hw_init_resets(enc);
+
+	/* Set HW info early so it's available for HW ID read */
+	enc->hw_info = &rkvenc_v2_hw_info;
+	enc->trans_info = trans_rkvenc_v2;
+
+	/* IOMMU */
+	enc->iommu_info = rkvenc_iommu_probe(dev);
+	if (IS_ERR(enc->iommu_info)) {
+		dev_err(dev, "failed to attach iommu\n");
+		enc->iommu_info = NULL;
+	}
+
+	/* Read hardware ID */
+	pm_runtime_get_sync(dev);
+	rkvenc_hw_clk_on(enc);
+	rkvenc_v2_hw_info.hw.hw_id = rkvenc_read(enc,
+					rkvenc_v2_hw_info.hw.reg_id * sizeof(u32));
+	rkvenc_hw_clk_off(enc);
+	pm_runtime_put_sync(dev);
+
+	/* Init state */
+	atomic_set(&enc->reset_request, 0);
+	atomic_set(&enc->session_index, 0);
+	atomic_set(&enc->task_count, 0);
+	atomic_set(&enc->task_index, 0);
+
+	enc->session_max_buffers = RKVENC_SESSION_MAX_BUFFERS;
+
+	return 0;
+
+failed:
+	device_init_wakeup(dev, false);
+	pm_runtime_disable(dev);
+	return ret;
+}
+
+int rkvenc_hw_remove(struct rkvenc_dev *enc)
+{
+	rkvenc_iommu_remove(enc->iommu_info);
+	device_init_wakeup(enc->dev, false);
+	pm_runtime_disable(enc->dev);
+	return 0;
+}
diff -urN a/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.h bb/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.h
--- a/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.h	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.h	2026-02-07 21:25:19
@@ -0,0 +1,881 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Rockchip VEPU580 encoder driver - Hardware definitions
+ * Ported from Rockchip BSP mpp_rkvenc2.c / mpp_common.h
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ * Copyright (C) 2026 Ross Cawston
+ */
+
+#ifndef __RKVENC_HW_H__
+#define __RKVENC_HW_H__
+
+#include <linux/clk.h>
+#include <linux/cdev.h>
+#include <linux/dma-direction.h>
+#include <linux/io.h>
+#include <linux/iommu.h>
+#include <linux/interrupt.h>
+#include <linux/kfifo.h>
+#include <linux/kthread.h>
+#include <linux/platform_device.h>
+#include <linux/pm_runtime.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include <linux/types.h>
+#include <linux/wait.h>
+#include <linux/workqueue.h>
+
+#include <uapi/linux/rkvenc.h>
+
+/* ---- Debug infrastructure ---- */
+extern unsigned int rkvenc_debug;
+
+#define DEBUG_IRQ_STATUS		0x00000004
+#define DEBUG_IOMMU			0x00000008
+#define DEBUG_IOCTL			0x00000010
+#define DEBUG_FUNCTION			0x00000020
+#define DEBUG_TASK_INFO			0x00000200
+#define DEBUG_DUMP_ERR_REG		0x00000400
+#define DEBUG_SRAM_INFO			0x00200000
+#define DEBUG_CCU			0x01000000
+#define DEBUG_CORE			0x02000000
+#define DEBUG_SLICE			0x00000002
+
+#define rkvenc_debug_unlikely(type)	(unlikely(rkvenc_debug & (type)))
+
+#define rkvenc_debug_func(type, fmt, args...)			\
+	do {							\
+		if (unlikely(rkvenc_debug & (type)))		\
+			pr_info("%s:%d: " fmt,			\
+				__func__, __LINE__, ##args);	\
+	} while (0)
+
+#define rkvenc_dbg(type, fmt, args...)				\
+	do {							\
+		if (unlikely(rkvenc_debug & (type)))		\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+#define rkvenc_debug_enter()					\
+	do {							\
+		if (unlikely(rkvenc_debug & DEBUG_FUNCTION))	\
+			pr_info("%s:%d: enter\n",		\
+				__func__, __LINE__);		\
+	} while (0)
+
+#define rkvenc_debug_leave()					\
+	do {							\
+		if (unlikely(rkvenc_debug & DEBUG_FUNCTION))	\
+			pr_info("%s:%d: leave\n",		\
+				__func__, __LINE__);		\
+	} while (0)
+
+#define rkvenc_err(fmt, args...)				\
+	pr_err("%s:%d: " fmt, __func__, __LINE__, ##args)
+
+#define rkvenc_dbg_core(fmt, args...)				\
+	do {							\
+		if (unlikely(rkvenc_debug & DEBUG_CORE))		\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+#define rkvenc_dbg_slice(fmt, args...)				\
+	do {							\
+		if (unlikely(rkvenc_debug & DEBUG_SLICE))	\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+/* ---- Constants ---- */
+#define MPP_DRIVER_NAME			"rkvenc"
+#define MPP_SERVICE_NAME		"mpp_service"
+#define MPP_CLASS_NAME			"mpp_class"
+
+#define MPP_MAX_CORE_NUM		4
+#define MPP_MAX_TASK_CAPACITY		16
+#define MPP_MAX_MSG_NUM			32
+#define MPP_MAX_REG_TRANS_NUM		128
+#define MPP_WORK_TIMEOUT_DELAY		200
+#define RKVENC_SESSION_MAX_BUFFERS	40
+#define RKVENC_MAX_CORE_NUM		2
+#define RKVENC_MAX_DCHS_ID		16
+
+/* ---- Device types (must match MPP userspace MppClientType enum) ---- */
+enum MPP_DEVICE_TYPE {
+	MPP_DEVICE_VDPU1		= 0,
+	MPP_DEVICE_VDPU2		= 1,
+	MPP_DEVICE_VDPU1_PP		= 2,
+	MPP_DEVICE_VDPU2_PP		= 3,
+	MPP_DEVICE_AV1DEC		= 4,
+
+	MPP_DEVICE_HEVC_DEC		= 8,
+	MPP_DEVICE_RKVDEC		= 9,
+
+	MPP_DEVICE_AVSPLUS_DEC		= 12,
+	MPP_DEVICE_RKJPEGD		= 13,
+
+	MPP_DEVICE_RKVENC		= 16,
+	MPP_DEVICE_VEPU1		= 17,
+	MPP_DEVICE_VEPU2		= 18,
+	MPP_DEVICE_VEPU2_JPEG		= 19,
+	MPP_DEVICE_RKJPEGE		= 20,
+
+	MPP_DEVICE_VEPU22		= 24,
+
+	MPP_DEVICE_IEP2			= 28,
+	MPP_DEVICE_VDPP			= 29,
+	MPP_DEVICE_BUTT			= 30,
+};
+
+/* ---- Register class definitions for VEPU580 ---- */
+enum RKVENC_CLASS_TYPE {
+	RKVENC_CLASS_BASE	= 0,
+	RKVENC_CLASS_PIC	= 1,
+	RKVENC_CLASS_RC		= 2,
+	RKVENC_CLASS_PAR	= 3,
+	RKVENC_CLASS_SQI	= 4,
+	RKVENC_CLASS_SCL	= 5,
+	RKVENC_CLASS_OSD	= 6,
+	RKVENC_CLASS_ST		= 7,
+	RKVENC_CLASS_DBG	= 8,
+	RKVENC_CLASS_BUTT,
+};
+
+enum RKVENC_CLASS_FD {
+	RKVENC_CLASS_FD_BASE	= 0,
+	RKVENC_CLASS_FD_OSD	= 1,
+	RKVENC_CLASS_FD_BUTT,
+};
+
+enum RKVENC_FORMAT_TYPE {
+	RKVENC_FMT_BASE		= 0x0000,
+	RKVENC_FMT_H264E	= RKVENC_FMT_BASE + 0,
+	RKVENC_FMT_H265E	= RKVENC_FMT_BASE + 1,
+	RKVENC_FMT_JPEGE	= RKVENC_FMT_BASE + 2,
+
+	RKVENC_FMT_OSD_BASE	= 0x1000,
+	RKVENC_FMT_H264E_OSD	= RKVENC_FMT_OSD_BASE + 0,
+	RKVENC_FMT_H265E_OSD	= RKVENC_FMT_OSD_BASE + 1,
+	RKVENC_FMT_JPEGE_OSD	= RKVENC_FMT_OSD_BASE + 2,
+	RKVENC_FMT_BUTT,
+};
+
+enum RKVENC_VEPU_TYPE {
+	RKVENC_VEPU_580		= 0,
+	RKVENC_VEPU_BUTT,
+};
+
+/* ---- Register offsets ---- */
+#define RKVENC_WDG			0x0038
+
+/* PIC class data array indices (relative to PIC class base 0x0280) */
+#define RKVENC2_PIC_BASE		0x0280
+#define RKVENC2_REG_ENC_PIC		((0x0300 - RKVENC2_PIC_BASE) / sizeof(u32))
+#define RKVENC2_REG_EXT_LINE_BUF_BASE	22
+#define RKVENC2_REG_SLI_SPLIT		56
+
+/* Absolute MMIO byte offsets used by runtime paths (match BSP mpp_rkvenc2.c) */
+#define RKVENC2_REG_ADR_BSBT		0x02b0
+#define RKVENC2_REG_ADR_BSBB		0x02b4
+#define RKVENC2_REG_ADR_BSBS		0x02b8
+#define RKVENC2_REG_ADR_BSBR		0x02bc
+
+#define RKVENC580_REG_ADR_BSBR		0x02b8
+
+#define RKVENC2_REG_ST_BSB		0x402c
+
+#define RKVENC2_REG_SLICE_NUM_BASE	0x4034
+#define RKVENC2_REG_SLICE_LEN_BASE	0x4038
+
+#define DCHS_REG_OFFSET			0x0084
+
+#define RKVENC2_BIT_ENC_STND		BIT(0)
+#define RKVENC2_BIT_SLEN_FIFO		BIT(30)
+#define RKVENC2_BIT_SLI_SPLIT		BIT(0)
+#define RKVENC2_BIT_SLI_FLUSH		BIT(15)
+#define RKVENC2_BIT_REC_FBC_DIS		BIT(31)
+#define RKVENC2_BIT_VAL_H264		0
+#define RKVENC2_BIT_VAL_H265		1
+
+#define RKVENC_SCLR_DONE_STA		BIT(4)
+
+/* Interrupt status bits */
+#define INT_STA_ENC_DONE_STA		BIT(0)
+#define INT_STA_SLC_DONE_STA		BIT(1)
+#define INT_STA_BSF_OFLW_STA		BIT(2)
+#define INT_STA_WDG_STA			BIT(5)
+#define INT_STA_ERROR			(0x03f0)
+
+/* Dual-core handshake */
+#define DCHS_TXE			BIT(8)
+
+/* ---- Clock mode ---- */
+enum MPP_CLOCK_MODE {
+	CLK_MODE_DEFAULT,
+	CLK_MODE_REDUCE,
+	CLK_MODE_NORMAL,
+	CLK_MODE_ADVANCED,
+	CLK_MODE_DEBUG,
+};
+
+/* ---- Task state bits ---- */
+enum {
+	TASK_STATE_PENDING	= 0,
+	TASK_STATE_RUNNING	= 1,
+	TASK_STATE_START	= 2,
+	TASK_STATE_IRQ		= 3,
+	TASK_STATE_HANDLE	= 4,
+	TASK_STATE_TIMEOUT	= 5,
+	TASK_STATE_FINISH	= 6,
+	TASK_STATE_DONE		= 7,
+	TASK_STATE_ABORT	= 8,
+};
+
+/* ---- Clock info ---- */
+struct rkvenc_clk_info {
+	struct clk *clk;
+	unsigned long debug_rate_hz;
+	unsigned long reduce_rate_hz;
+	unsigned long normal_rate_hz;
+	unsigned long advanced_rate_hz;
+	unsigned long default_rate_hz;
+	unsigned long used_rate_hz;
+	unsigned long real_rate_hz;
+};
+
+/* ---- HW info structure (register class map for VEPU580) ---- */
+struct mpp_hw_info {
+	s32 reg_num;
+	s32 reg_id;
+	s32 reg_en;
+	s32 reg_start;
+	s32 reg_end;
+	u32 hw_id;
+};
+
+struct rkvenc_reg_msg {
+	u32 base_s;
+	u32 base_e;
+};
+
+struct rkvenc_fmt_reg {
+	u32 class;
+	u32 base;
+	u32 bitpos;
+	u32 bitlen;
+};
+
+struct rkvenc_fd_reg {
+	u32 class;
+	u32 base_fmt;
+};
+
+struct rkvenc_hw_info {
+	struct mpp_hw_info hw;
+
+	u32 reg_class;
+	struct rkvenc_reg_msg reg_msg[RKVENC_CLASS_BUTT];
+
+	u32 fd_class;
+	struct rkvenc_fd_reg fd_reg[RKVENC_CLASS_FD_BUTT];
+
+	struct rkvenc_fmt_reg fmt_reg;
+
+	u32 enc_start_base;
+	u32 enc_clr_base;
+	u32 int_en_base;
+	u32 int_mask_base;
+	u32 int_clr_base;
+	u32 int_sta_base;
+	u32 enc_wdg_base;
+	u32 err_mask;
+	u32 enc_rsl;
+	u32 dcsh_class_ofst;
+	u32 vepu_type;
+};
+
+/* ---- FD translation table ---- */
+struct rkvenc_trans_info {
+	const int count;
+	const u16 *table;
+};
+
+/* ---- Memory region for DMA-buf tracking ---- */
+#define MPP_MAX_MEM_REGION		128
+
+struct rkvenc_mem_region {
+	struct list_head reg_link;
+	/* DMABUF information */
+	dma_addr_t iova;
+	unsigned long len;
+	int fd;
+	u32 reg_class;
+	u32 reg_idx;
+	bool is_dup;
+	/* handle from DMA session */
+	void *hdl;
+};
+
+/* ---- Offset info for register address offsets ---- */
+struct reg_offset_elem {
+	u32 index;
+	u32 offset;
+};
+
+struct reg_offset_info {
+	u32 cnt;
+	struct reg_offset_elem elem[MPP_MAX_REG_TRANS_NUM];
+};
+
+/* ---- DMA buffer management ---- */
+#define MPP_SESSION_MAX_BUFFERS		60
+
+struct rkvenc_dma_buffer {
+	struct list_head link;
+	struct rkvenc_dma_session *dma;
+	struct dma_buf *dmabuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	enum dma_data_direction dir;
+	dma_addr_t iova;
+	unsigned long size;
+	void *vaddr;
+	struct kref ref;
+	struct device *dev;
+};
+
+struct rkvenc_dma_session {
+	struct list_head unused_list;
+	struct list_head used_list;
+	struct list_head static_list;
+	struct rkvenc_dma_buffer dma_bufs[MPP_SESSION_MAX_BUFFERS];
+	struct mutex list_mutex;
+	u32 max_buffers;
+	int buffer_count;
+	struct device *dev;
+};
+
+/* ---- IOMMU info ---- */
+struct rkvenc_iommu_info {
+	struct rw_semaphore *rw_sem;
+	struct rw_semaphore rw_sem_self;
+	struct device *dev;
+	struct platform_device *pdev;
+	struct iommu_domain *domain;
+	struct iommu_group *group;
+	spinlock_t dev_lock;
+	struct rkvenc_dev *dev_active;
+	int irq;
+	int got_irq;
+};
+
+/* ---- Task queue ---- */
+struct rkvenc_taskqueue {
+	struct mutex session_lock;
+	struct mutex pending_lock;
+	spinlock_t running_lock;
+	struct mutex dev_lock;
+
+	struct list_head session_attach;
+	struct list_head session_detach;
+	struct list_head pending_list;
+	struct list_head running_list;
+	struct list_head dev_list;
+
+	struct kthread_worker worker;
+	struct task_struct *kworker_task;
+
+	u32 task_capacity;
+	atomic_t reset_request;
+	atomic_t detach_count;
+	atomic_t task_id;
+
+	/* Multi-core support */
+	struct rkvenc_dev *cores[MPP_MAX_CORE_NUM];
+	u32 core_count;
+	u32 core_id_max;
+	unsigned long core_idle;
+	unsigned long dev_active_flags;
+};
+
+/* ---- Reset group ---- */
+struct rkvenc_reset_group {
+	struct rw_semaphore rw_sem;
+	bool rw_sem_on;
+	struct reset_control *resets[3];
+	struct rkvenc_taskqueue *queue;
+};
+
+/* ---- GRF info ---- */
+#define MPP_GRF_VAL_MASK		0xffff
+
+struct rkvenc_grf_info {
+	struct regmap *grf;
+	u32 offset;
+	u32 val;
+};
+
+/* ---- MPP service ---- */
+struct rkvenc_service {
+	struct device *dev;
+	struct cdev mpp_cdev;
+	dev_t dev_id;
+	struct class *cls;
+	struct device *child_dev;
+
+	u32 taskqueue_cnt;
+	struct rkvenc_taskqueue *task_queues[MPP_DEVICE_BUTT];
+	u32 reset_group_cnt;
+	struct rkvenc_reset_group *reset_groups[MPP_DEVICE_BUTT];
+
+	struct rkvenc_dev *sub_devices[MPP_DEVICE_BUTT];
+	unsigned long hw_support;
+	struct rkvenc_grf_info grf_infos[MPP_DEVICE_BUTT];
+
+	struct mutex session_lock;
+	struct list_head session_list;
+
+	u32 timing_en;
+	atomic_t shutdown_request;
+};
+
+/* ---- MPP task ---- */
+struct rkvenc_mpp_task {
+	struct list_head pending_link;
+	struct list_head queue_link;
+	struct list_head mem_region_list;
+	unsigned long state;
+	u32 mem_count;
+	struct rkvenc_session *session;
+	struct rkvenc_dev *mpp;
+	s32 core_id;
+	const struct mpp_hw_info *hw_info;
+	u32 *reg;
+
+	struct kref ref;
+	wait_queue_head_t wait;
+	atomic_t abort_request;
+	u32 task_index;
+	u32 task_id;
+	struct delayed_work timeout_work;
+	u32 irq_status;
+	u32 hw_cycles;
+	s64 hw_time;
+
+	/* Memory regions */
+	struct rkvenc_mem_region mem_regions[MPP_MAX_MEM_REGION];
+
+	/* Timing */
+	ktime_t start;
+	ktime_t part;
+	ktime_t on_create;
+	ktime_t on_create_end;
+	ktime_t on_pending;
+	ktime_t on_run;
+	ktime_t on_sched_timeout;
+	ktime_t on_run_end;
+	ktime_t on_irq;
+	ktime_t on_cancel_timeout;
+	ktime_t on_finish;
+};
+
+/* ---- RKVENC-specific task ---- */
+struct rkvenc_class_msg {
+	u32 *data;
+	u32 size;
+	u32 valid;
+};
+
+union rkvenc2_dual_core_handshake_id {
+	u64 val[2];
+	struct {
+		/* val[0] */
+		u32 txe		: 1;
+		u32 reserved0	: 3;
+		u32 txid	: 4;
+		u32 rxe		: 1;
+		u32 reserved1	: 3;
+		u32 rxid	: 4;
+		u32 reserved2	: 16;
+		u32 session_id;
+		/* val[1] */
+		u32 txe_orig	: 1;
+		u32 reserved3	: 3;
+		u32 txid_orig	: 4;
+		u32 rxe_orig	: 1;
+		u32 reserved4	: 3;
+		u32 rxid_orig	: 4;
+		u32 txe_map	: 1;
+		u32 reserved5	: 3;
+		u32 txid_map	: 4;
+		u32 rxe_map	: 1;
+		u32 reserved6	: 3;
+		u32 rxid_map	: 4;
+		u32 working;
+	};
+};
+
+union rkvenc2_slice_len_info {
+	u32 val;
+	struct {
+		u32 slice_len	: 24;
+		u32 reserved	: 7;
+		u32 last	: 1;
+	};
+};
+
+union rkvenc2_frame_resolution {
+	u32 val;
+	struct {
+		u32 pic_wd8	: 13;
+		u32 reserved0	: 3;
+		u32 pic_hd8	: 13;
+		u32 reserved1	: 3;
+	};
+};
+
+struct rkvenc_poll_slice_cfg {
+	u32 count_max;
+	u32 count_ret;
+};
+
+#define RKVENC_SLICE_FIFO_LEN	512
+
+struct rkvenc_task {
+	struct rkvenc_mpp_task mpp_task;
+	const struct rkvenc_hw_info *hw_info;
+
+	/* Class-based register buffers */
+	struct rkvenc_class_msg reg[RKVENC_CLASS_BUTT];
+	struct mpp_request w_reqs[RKVENC_CLASS_BUTT];
+	struct mpp_request r_reqs[RKVENC_CLASS_BUTT];
+	u32 w_req_cnt;
+	u32 r_req_cnt;
+
+	/* Register offset info */
+	struct reg_offset_info off_inf;
+
+	/* Format and flags */
+	u32 fmt;
+	int clk_mode;
+	u32 irq_status;
+	u32 task_split;
+	u32 task_split_done;
+
+	/* Dual-core handshake ID */
+	union rkvenc2_dual_core_handshake_id dchs_id;
+
+	/* Slice split info */
+	DECLARE_KFIFO(slice_info, union rkvenc2_slice_len_info, RKVENC_SLICE_FIFO_LEN);
+	u32 slice_wr_cnt;
+	u32 slice_rd_cnt;
+	u32 last_slice_found;
+
+	/* Bitstream buffer tracking */
+	struct rkvenc_dma_buffer *bs_buf;
+	u32 offset_bs;
+};
+
+/* Forward declaration */
+struct rkvenc_task_msgs;
+
+/* ---- Session ---- */
+struct rkvenc_session {
+	int pid;
+	int index;
+
+	struct rkvenc_dev *mpp;
+	struct rkvenc_service *srv;
+	enum MPP_DEVICE_TYPE device_type;
+	struct rkvenc_dma_session *dma;
+
+	struct mutex pending_lock;
+	struct list_head pending_list;
+	struct list_head service_link;
+	struct list_head session_link;
+
+	atomic_t task_count;
+	atomic_t release_request;
+
+	/* FD translation table (from userspace) */
+	u16 trans_table[MPP_MAX_REG_TRANS_NUM];
+	u32 trans_count;
+	u32 msg_flags;
+
+	/* Session private data */
+	void *priv;
+
+};
+
+/* ---- Task messages ---- */
+struct rkvenc_task_msgs {
+	struct list_head list;
+	struct list_head list_session;
+
+	struct rkvenc_session *session;
+	struct rkvenc_taskqueue *queue;
+	struct rkvenc_mpp_task *task;
+	struct rkvenc_dev *mpp;
+
+	u32 flags;
+	u32 req_cnt;
+	u32 set_cnt;
+	u32 poll_cnt;
+	struct mpp_request *poll_req;
+	struct mpp_request reqs[MPP_MAX_MSG_NUM];
+
+	int ext_fd;
+};
+
+/* ---- Session private data ---- */
+struct rkvenc2_rcb_info_elem {
+	u32 index;
+	u32 size;
+};
+
+struct rkvenc2_rcb_info {
+	u32 cnt;
+	struct rkvenc2_rcb_info_elem elem[20];
+};
+
+enum {
+	ENC_INFO_BASE = 0,
+	ENC_INFO_WIDTH,
+	ENC_INFO_HEIGHT,
+	ENC_INFO_FORMAT,
+	ENC_INFO_FPS_IN,
+	ENC_INFO_FPS_OUT,
+	ENC_INFO_RC_MODE,
+	ENC_INFO_BITRATE,
+	ENC_INFO_GOP_SIZE,
+	ENC_INFO_FPS_CALC,
+	ENC_INFO_PROFILE,
+	ENC_INFO_BUTT,
+};
+
+enum {
+	CODEC_INFO_FLAG_NULL = 0,
+	CODEC_INFO_FLAG_NUMBER,
+	CODEC_INFO_FLAG_STRING,
+	CODEC_INFO_FLAG_BUTT,
+};
+
+struct codec_info_elem {
+	u32 type;
+	u32 flag;
+	u64 data;
+};
+
+struct rkvenc2_codec_info {
+	u32 flag;
+	u64 val;
+};
+
+struct rkvenc2_session_priv {
+	struct rw_semaphore rw_sem;
+	struct rkvenc2_rcb_info rcb_inf;
+	struct rkvenc2_codec_info codec_info[ENC_INFO_BUTT];
+};
+
+/* ---- CCU (Core Coordination Unit) ---- */
+struct rkvenc_ccu {
+	struct mutex lock;
+	struct list_head core_list;
+	int core_num;
+	struct rkvenc_dev *main_core;
+	spinlock_t lock_dchs;
+	union rkvenc2_dual_core_handshake_id dchs[RKVENC_MAX_CORE_NUM];
+};
+
+/* ---- Per-core encoder device ---- */
+struct rkvenc_dev {
+	struct device *dev;
+	void __iomem *reg_base;
+	resource_size_t io_base;
+	int irq;
+	u32 irq_status;
+
+	/* Framework */
+	struct rkvenc_service *srv;
+	struct rkvenc_taskqueue *queue;
+	struct rkvenc_iommu_info *iommu_info;
+	struct rkvenc_reset_group *reset_group;
+	struct rkvenc_grf_info *grf_info;
+	struct rkvenc_mpp_task *cur_task;
+	iommu_fault_handler_t fault_handler;
+
+	/* HW variant data */
+	const struct rkvenc_hw_info *hw_info;
+	const struct rkvenc_trans_info *trans_info;
+
+	s32 core_id;
+	u32 task_capacity;
+	u32 session_max_buffers;
+	u32 msgs_cap;
+	bool auto_freq_en;
+
+	/* Clocks and resets */
+	struct rkvenc_clk_info aclk_info;
+	struct rkvenc_clk_info hclk_info;
+	struct rkvenc_clk_info core_clk_info;
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+	struct reset_control *rst_core;
+
+	/* Multi-core */
+	struct rkvenc_ccu *ccu;
+	struct list_head core_link;
+	struct list_head queue_link;
+
+	/* Work */
+	struct kthread_work work;
+
+	/* State */
+	atomic_t reset_request;
+	atomic_t session_index;
+	atomic_t task_count;
+	atomic_t task_index;
+
+	/* SRAM RCB */
+	dma_addr_t sram_iova;
+	u32 sram_size;
+	u32 sram_used;
+	int sram_enabled;
+	struct page *rcb_page;
+
+	/* Bitstream overflow flag */
+	u32 bs_overflow;
+
+	/* PM */
+	u32 disable;
+};
+
+/* ---- Helper macros ---- */
+static inline u32 rkvenc_read(struct rkvenc_dev *mpp, u32 reg)
+{
+	u32 val = readl(mpp->reg_base + reg);
+	return val;
+}
+
+static inline u32 rkvenc_read_relaxed(struct rkvenc_dev *mpp, u32 reg)
+{
+	return readl_relaxed(mpp->reg_base + reg);
+}
+
+static inline void rkvenc_write(struct rkvenc_dev *mpp, u32 reg, u32 val)
+{
+	writel(val, mpp->reg_base + reg);
+}
+
+static inline void rkvenc_write_relaxed(struct rkvenc_dev *mpp, u32 reg, u32 val)
+{
+	writel_relaxed(val, mpp->reg_base + reg);
+}
+
+static inline void rkvenc_clk_safe_enable(struct clk *clk)
+{
+	if (clk)
+		clk_prepare_enable(clk);
+}
+
+static inline void rkvenc_clk_safe_disable(struct clk *clk)
+{
+	if (clk)
+		clk_disable_unprepare(clk);
+}
+
+static inline void rkvenc_safe_reset(struct reset_control *rst)
+{
+	if (rst)
+		reset_control_assert(rst);
+}
+
+static inline void rkvenc_safe_unreset(struct reset_control *rst)
+{
+	if (rst)
+		reset_control_deassert(rst);
+}
+
+/* ---- Extern declarations ---- */
+
+/* rkvenc_service.c */
+int rkvenc_service_probe(struct platform_device *pdev);
+int rkvenc_service_remove(struct platform_device *pdev);
+void rkvenc_task_worker_default(struct kthread_work *work);
+extern const struct file_operations rkvenc_fops;
+
+/* rkvenc_iommu.c */
+struct rkvenc_iommu_info *rkvenc_iommu_probe(struct device *dev);
+int rkvenc_iommu_remove(struct rkvenc_iommu_info *info);
+int rkvenc_iommu_attach(struct rkvenc_iommu_info *info);
+int rkvenc_iommu_detach(struct rkvenc_iommu_info *info);
+int rkvenc_iommu_flush_tlb(struct rkvenc_iommu_info *info);
+int rkvenc_iommu_dev_activate(struct rkvenc_iommu_info *info, struct rkvenc_dev *dev);
+int rkvenc_iommu_dev_deactivate(struct rkvenc_iommu_info *info, struct rkvenc_dev *dev);
+
+struct rkvenc_dma_session *rkvenc_dma_session_create(struct device *dev, u32 max_buffers);
+int rkvenc_dma_session_destroy(struct rkvenc_dma_session *dma);
+struct rkvenc_dma_buffer *rkvenc_dma_import_fd(struct rkvenc_iommu_info *iommu_info,
+					       struct rkvenc_dma_session *dma,
+					       int fd, int static_use);
+struct rkvenc_dma_buffer *rkvenc_dma_find_buffer_fd(struct rkvenc_dma_session *dma, int fd);
+int rkvenc_dma_release(struct rkvenc_dma_session *dma, struct rkvenc_dma_buffer *buffer);
+int rkvenc_dma_release_fd(struct rkvenc_dma_session *dma, int fd);
+void rkvenc_dma_buf_sync(struct rkvenc_dma_buffer *buffer, u32 offset, u32 length,
+			 enum dma_data_direction dir, bool for_cpu);
+
+static inline int rkvenc_iommu_down_read(struct rkvenc_iommu_info *info)
+{
+	if (info)
+		down_read(info->rw_sem);
+	return 0;
+}
+
+static inline int rkvenc_iommu_up_read(struct rkvenc_iommu_info *info)
+{
+	if (info)
+		up_read(info->rw_sem);
+	return 0;
+}
+
+static inline int rkvenc_iommu_down_write(struct rkvenc_iommu_info *info)
+{
+	if (info)
+		down_write(info->rw_sem);
+	return 0;
+}
+
+static inline int rkvenc_iommu_up_write(struct rkvenc_iommu_info *info)
+{
+	if (info)
+		up_write(info->rw_sem);
+	return 0;
+}
+
+/* rkvenc_task.c */
+void rkvenc_free_task_callback(struct kref *ref);
+int rkvenc_task_init(struct rkvenc_session *session, struct rkvenc_mpp_task *task);
+int rkvenc_task_finish(struct rkvenc_session *session, struct rkvenc_mpp_task *task);
+int rkvenc_task_finalize(struct rkvenc_session *session, struct rkvenc_mpp_task *task);
+int rkvenc_translate_reg_address(struct rkvenc_session *session,
+				  struct rkvenc_mpp_task *task, int fmt, u32 reg_class,
+				  u32 *reg, struct reg_offset_info *off_inf);
+int rkvenc_extract_reg_offset_info(struct reg_offset_info *off_inf,
+				   struct mpp_request *req);
+int rkvenc_query_reg_offset_info(struct reg_offset_info *off_inf, u32 index);
+void rkvenc_task_timeout_work(struct work_struct *work_s);
+
+/* rkvenc_hw.c */
+int rkvenc_hw_probe(struct rkvenc_dev *enc, struct platform_device *pdev);
+int rkvenc_hw_remove(struct rkvenc_dev *enc);
+irqreturn_t rkvenc_hw_irq(int irq, void *param);
+int rkvenc_hw_run(struct rkvenc_dev *mpp, struct rkvenc_mpp_task *mpp_task);
+int rkvenc_hw_finish(struct rkvenc_dev *mpp, struct rkvenc_mpp_task *mpp_task);
+int rkvenc_hw_reset(struct rkvenc_dev *enc);
+void rkvenc_hw_clk_on(struct rkvenc_dev *enc);
+void rkvenc_hw_clk_off(struct rkvenc_dev *enc);
+
+/* HW info for VEPU580 */
+extern struct rkvenc_hw_info rkvenc_v2_hw_info;
+extern const struct rkvenc_trans_info trans_rkvenc_v2[];
+
+#endif /* __RKVENC_HW_H__ */
diff -urN a/drivers/media/platform/rockchip/rkvenc/rkvenc_iommu.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_iommu.c
--- a/drivers/media/platform/rockchip/rkvenc/rkvenc_iommu.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_iommu.c	2026-02-08 03:09:11
@@ -0,0 +1,463 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - IOMMU and DMA buffer helpers
+ * Ported from Rockchip BSP mpp_iommu.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ * Copyright (C) 2026 Ross Cawston
+ */
+
+#include <linux/dma-buf.h>
+#include <linux/dma-mapping.h>
+#include <linux/iommu.h>
+#include <linux/interrupt.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/kref.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/pm_runtime.h>
+
+#include "rkvenc_hw.h"
+
+/* ---- DMA buffer find ---- */
+struct rkvenc_dma_buffer *
+rkvenc_dma_find_buffer_fd(struct rkvenc_dma_session *dma, int fd)
+{
+	struct dma_buf *dmabuf;
+	struct rkvenc_dma_buffer *out = NULL;
+	struct rkvenc_dma_buffer *buffer = NULL, *n;
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf))
+		return NULL;
+
+	mutex_lock(&dma->list_mutex);
+	list_for_each_entry_safe(buffer, n, &dma->used_list, link) {
+		if (buffer->dmabuf == dmabuf) {
+			out = buffer;
+			list_move_tail(&buffer->link, &buffer->dma->used_list);
+			break;
+		}
+	}
+	if (!out) {
+		list_for_each_entry_safe(buffer, n, &dma->static_list, link) {
+			if (buffer->dmabuf == dmabuf) {
+				out = buffer;
+				list_move_tail(&buffer->link, &buffer->dma->static_list);
+				break;
+			}
+		}
+	}
+	mutex_unlock(&dma->list_mutex);
+	dma_buf_put(dmabuf);
+
+	return out;
+}
+
+/* ---- DMA buffer release ---- */
+static void rkvenc_dma_release_buffer(struct kref *ref)
+{
+	struct rkvenc_dma_buffer *buffer =
+		container_of(ref, struct rkvenc_dma_buffer, ref);
+
+	buffer->dma->buffer_count--;
+	list_move_tail(&buffer->link, &buffer->dma->unused_list);
+
+	dma_buf_unmap_attachment(buffer->attach, buffer->sgt, buffer->dir);
+	dma_buf_detach(buffer->dmabuf, buffer->attach);
+	dma_buf_put(buffer->dmabuf);
+	buffer->dma = NULL;
+	buffer->dmabuf = NULL;
+	buffer->attach = NULL;
+	buffer->sgt = NULL;
+	buffer->iova = 0;
+	buffer->size = 0;
+	buffer->vaddr = NULL;
+}
+
+static int rkvenc_dma_remove_extra_buffer(struct rkvenc_dma_session *dma)
+{
+	struct rkvenc_dma_buffer *n;
+	struct rkvenc_dma_buffer *removable = NULL, *buffer = NULL;
+
+	if (dma->buffer_count > dma->max_buffers) {
+		mutex_lock(&dma->list_mutex);
+		list_for_each_entry_safe(buffer, n, &dma->used_list, link) {
+			if (kref_read(&buffer->ref) == 1) {
+				removable = buffer;
+				break;
+			}
+		}
+		if (removable)
+			kref_put(&removable->ref, rkvenc_dma_release_buffer);
+		mutex_unlock(&dma->list_mutex);
+	}
+
+	return 0;
+}
+
+int rkvenc_dma_release(struct rkvenc_dma_session *dma,
+		       struct rkvenc_dma_buffer *buffer)
+{
+	mutex_lock(&dma->list_mutex);
+	kref_put(&buffer->ref, rkvenc_dma_release_buffer);
+	mutex_unlock(&dma->list_mutex);
+
+	return 0;
+}
+
+int rkvenc_dma_release_fd(struct rkvenc_dma_session *dma, int fd)
+{
+	struct rkvenc_dma_buffer *buffer;
+
+	buffer = rkvenc_dma_find_buffer_fd(dma, fd);
+	if (IS_ERR_OR_NULL(buffer)) {
+		dev_err(dma->dev, "can not find %d buffer in list\n", fd);
+		return -EINVAL;
+	}
+
+	mutex_lock(&dma->list_mutex);
+	kref_put(&buffer->ref, rkvenc_dma_release_buffer);
+	mutex_unlock(&dma->list_mutex);
+
+	return 0;
+}
+
+/* ---- DMA buffer import ---- */
+struct rkvenc_dma_buffer *
+rkvenc_dma_import_fd(struct rkvenc_iommu_info *iommu_info,
+		     struct rkvenc_dma_session *dma,
+		     int fd, int static_use)
+{
+	int ret = 0;
+	struct sg_table *sgt;
+	struct dma_buf *dmabuf;
+	struct rkvenc_dma_buffer *buffer;
+	struct dma_buf_attachment *attach;
+
+	if (!dma) {
+		rkvenc_err("dma session is null\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	rkvenc_dma_remove_extra_buffer(dma);
+
+	/* Check whether in dma session */
+	buffer = rkvenc_dma_find_buffer_fd(dma, fd);
+	if (!IS_ERR_OR_NULL(buffer)) {
+		if (kref_get_unless_zero(&buffer->ref))
+			return buffer;
+	}
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf)) {
+		ret = PTR_ERR(dmabuf);
+		rkvenc_err("dma_buf_get fd %d failed(%d)\n", fd, ret);
+		return ERR_PTR(ret);
+	}
+
+	mutex_lock(&dma->list_mutex);
+	buffer = list_first_entry_or_null(&dma->unused_list,
+					  struct rkvenc_dma_buffer, link);
+	if (!buffer) {
+		ret = -ENOMEM;
+		mutex_unlock(&dma->list_mutex);
+		goto fail;
+	}
+	list_del_init(&buffer->link);
+	mutex_unlock(&dma->list_mutex);
+
+	buffer->dmabuf = dmabuf;
+	buffer->dir = DMA_BIDIRECTIONAL;
+
+	attach = dma_buf_attach(buffer->dmabuf, dma->dev);
+	if (IS_ERR(attach)) {
+		ret = PTR_ERR(attach);
+		rkvenc_err("dma_buf_attach fd %d failed(%d)\n", fd, ret);
+		goto fail_attach;
+	}
+
+	sgt = dma_buf_map_attachment(attach, buffer->dir);
+	if (IS_ERR(sgt)) {
+		ret = PTR_ERR(sgt);
+		rkvenc_err("dma_buf_map_attachment fd %d failed(%d)\n", fd, ret);
+		goto fail_map;
+	}
+	buffer->iova = sg_dma_address(sgt->sgl);
+	buffer->size = sg_dma_len(sgt->sgl);
+	buffer->attach = attach;
+	buffer->sgt = sgt;
+	buffer->dma = dma;
+
+	kref_init(&buffer->ref);
+
+	if (!static_use)
+		kref_get(&buffer->ref);
+
+	mutex_lock(&dma->list_mutex);
+	dma->buffer_count++;
+	if (static_use)
+		list_add_tail(&buffer->link, &dma->static_list);
+	else
+		list_add_tail(&buffer->link, &dma->used_list);
+	mutex_unlock(&dma->list_mutex);
+
+	return buffer;
+
+fail_map:
+	dma_buf_detach(buffer->dmabuf, attach);
+fail_attach:
+	mutex_lock(&dma->list_mutex);
+	list_add_tail(&buffer->link, &dma->unused_list);
+	mutex_unlock(&dma->list_mutex);
+fail:
+	dma_buf_put(dmabuf);
+	return ERR_PTR(ret);
+}
+
+/* ---- DMA buffer sync ---- */
+void rkvenc_dma_buf_sync(struct rkvenc_dma_buffer *buffer, u32 offset, u32 length,
+			 enum dma_data_direction dir, bool for_cpu)
+{
+	struct device *dev = buffer->dma->dev;
+	struct sg_table *sgt = buffer->sgt;
+	struct scatterlist *sg = sgt->sgl;
+	dma_addr_t sg_dma_addr = sg_dma_address(sg);
+	unsigned int len = 0;
+	int i;
+
+	for_each_sgtable_sg(sgt, sg, i) {
+		unsigned int sg_offset, sg_left, size = 0;
+
+		len += sg->length;
+		if (len <= offset) {
+			sg_dma_addr += sg->length;
+			continue;
+		}
+
+		sg_left = len - offset;
+		sg_offset = sg->length - sg_left;
+
+		size = (length < sg_left) ? length : sg_left;
+
+		if (for_cpu)
+			dma_sync_single_range_for_cpu(dev, sg_dma_addr,
+						      sg_offset, size, dir);
+		else
+			dma_sync_single_range_for_device(dev, sg_dma_addr,
+							 sg_offset, size, dir);
+
+		offset += size;
+		length -= size;
+		sg_dma_addr += sg->length;
+
+		if (length == 0)
+			break;
+	}
+}
+
+/* ---- DMA session management ---- */
+int rkvenc_dma_session_destroy(struct rkvenc_dma_session *dma)
+{
+	struct rkvenc_dma_buffer *n, *buffer = NULL;
+
+	if (!dma)
+		return -EINVAL;
+
+	mutex_lock(&dma->list_mutex);
+	list_for_each_entry_safe(buffer, n, &dma->used_list, link)
+		kref_put(&buffer->ref, rkvenc_dma_release_buffer);
+	list_for_each_entry_safe(buffer, n, &dma->static_list, link)
+		kref_put(&buffer->ref, rkvenc_dma_release_buffer);
+	mutex_unlock(&dma->list_mutex);
+
+	kfree(dma);
+	return 0;
+}
+
+struct rkvenc_dma_session *
+rkvenc_dma_session_create(struct device *dev, u32 max_buffers)
+{
+	int i;
+	struct rkvenc_dma_session *dma;
+	struct rkvenc_dma_buffer *buffer;
+
+	dma = kzalloc(sizeof(*dma), GFP_KERNEL);
+	if (!dma)
+		return NULL;
+
+	mutex_init(&dma->list_mutex);
+	INIT_LIST_HEAD(&dma->unused_list);
+	INIT_LIST_HEAD(&dma->used_list);
+	INIT_LIST_HEAD(&dma->static_list);
+
+	if (max_buffers > MPP_SESSION_MAX_BUFFERS)
+		dma->max_buffers = MPP_SESSION_MAX_BUFFERS;
+	else
+		dma->max_buffers = max_buffers;
+
+	for (i = 0; i < ARRAY_SIZE(dma->dma_bufs); i++) {
+		buffer = &dma->dma_bufs[i];
+		buffer->dma = dma;
+		INIT_LIST_HEAD(&buffer->link);
+		list_add_tail(&buffer->link, &dma->unused_list);
+	}
+	dma->dev = dev;
+
+	return dma;
+}
+
+/* ---- IOMMU helpers ---- */
+int rkvenc_iommu_detach(struct rkvenc_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	iommu_detach_group(info->domain, info->group);
+	return 0;
+}
+
+int rkvenc_iommu_attach(struct rkvenc_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	if (info->domain == iommu_get_domain_for_dev(info->dev))
+		return 0;
+
+	return iommu_attach_group(info->domain, info->group);
+}
+
+int rkvenc_iommu_flush_tlb(struct rkvenc_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	if (info->domain && info->domain->ops)
+		iommu_flush_iotlb_all(info->domain);
+
+	return 0;
+}
+
+static int rkvenc_iommu_fault_handler(struct iommu_domain *iommu,
+				      struct device *iommu_dev,
+				      unsigned long iova,
+				      int status, void *arg)
+{
+	dev_err(iommu_dev, "IOMMU fault addr 0x%08lx status %x\n",
+		iova, status);
+
+	return 0;
+}
+
+int rkvenc_iommu_dev_activate(struct rkvenc_iommu_info *info, struct rkvenc_dev *dev)
+{
+	unsigned long flags;
+
+	if (!info)
+		return 0;
+
+	spin_lock_irqsave(&info->dev_lock, flags);
+
+	if (info->dev_active || !dev) {
+		dev_err(info->dev, "can not activate\n");
+		spin_unlock_irqrestore(&info->dev_lock, flags);
+		return -EINVAL;
+	}
+
+	info->dev_active = dev;
+	if (info->domain && info->domain->cookie_type == IOMMU_COOKIE_NONE)
+		iommu_set_fault_handler(info->domain, rkvenc_iommu_fault_handler, dev);
+
+	spin_unlock_irqrestore(&info->dev_lock, flags);
+
+	return 0;
+}
+
+int rkvenc_iommu_dev_deactivate(struct rkvenc_iommu_info *info, struct rkvenc_dev *dev)
+{
+	unsigned long flags;
+
+	if (!info)
+		return 0;
+
+	spin_lock_irqsave(&info->dev_lock, flags);
+	info->dev_active = NULL;
+	spin_unlock_irqrestore(&info->dev_lock, flags);
+
+	return 0;
+}
+
+struct rkvenc_iommu_info *rkvenc_iommu_probe(struct device *dev)
+{
+	int ret = 0;
+	struct device_node *np;
+	struct platform_device *pdev;
+	struct rkvenc_iommu_info *info;
+	struct iommu_domain *domain;
+	struct iommu_group *group;
+
+	np = of_parse_phandle(dev->of_node, "iommus", 0);
+	if (!np || !of_device_is_available(np)) {
+		rkvenc_err("failed to get IOMMU device node\n");
+		return ERR_PTR(-ENODEV);
+	}
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev) {
+		rkvenc_err("failed to get IOMMU platform device\n");
+		return ERR_PTR(-ENODEV);
+	}
+
+	group = iommu_group_get(dev);
+	if (!group) {
+		ret = -EINVAL;
+		goto err_put_pdev;
+	}
+
+	domain = iommu_get_domain_for_dev(dev);
+	if (!domain) {
+		ret = -EINVAL;
+		goto err_put_group;
+	}
+
+	info = devm_kzalloc(dev, sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		ret = -ENOMEM;
+		goto err_put_group;
+	}
+
+	init_rwsem(&info->rw_sem_self);
+	info->rw_sem = &info->rw_sem_self;
+	spin_lock_init(&info->dev_lock);
+	info->dev = dev;
+	info->pdev = pdev;
+	info->group = group;
+	info->domain = domain;
+	info->dev_active = NULL;
+	info->irq = platform_get_irq(pdev, 0);
+	info->got_irq = (info->irq < 0) ? false : true;
+
+	return info;
+
+err_put_group:
+	if (group)
+		iommu_group_put(group);
+err_put_pdev:
+	if (pdev)
+		platform_device_put(pdev);
+
+	return ERR_PTR(ret);
+}
+
+int rkvenc_iommu_remove(struct rkvenc_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	iommu_group_put(info->group);
+	platform_device_put(info->pdev);
+
+	return 0;
+}
diff -urN a/drivers/media/platform/rockchip/rkvenc/rkvenc_service.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_service.c
--- a/drivers/media/platform/rockchip/rkvenc/rkvenc_service.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_service.c	2026-02-08 01:25:24
@@ -0,0 +1,1195 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - MPP service char device and ioctl dispatch
+ * Ported from Rockchip BSP mpp_service.c / mpp_common.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ * Copyright (C) 2026 Ross Cawston
+ */
+
+#include <linux/cdev.h>
+#include <linux/delay.h>
+#include <linux/dma-buf.h>
+#include <linux/device/class.h>
+#include <linux/fs.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/poll.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/wait.h>
+
+#include "rkvenc_hw.h"
+
+/* ---- MSG v1 structure (from userspace) ---- */
+struct mpp_msg_v1 {
+	__u32 cmd;
+	__u32 flags;
+	__u32 size;
+	__u32 offset;
+	__u64 data_ptr;
+};
+
+/* ---- Session helpers ---- */
+static struct rkvenc_session *rkvenc_session_init(void)
+{
+	struct rkvenc_session *session;
+
+	session = kzalloc(sizeof(*session), GFP_KERNEL);
+	if (!session)
+		return NULL;
+
+	session->pid = current->pid;
+	mutex_init(&session->pending_lock);
+	INIT_LIST_HEAD(&session->pending_list);
+	INIT_LIST_HEAD(&session->service_link);
+	INIT_LIST_HEAD(&session->session_link);
+	atomic_set(&session->task_count, 0);
+	atomic_set(&session->release_request, 0);
+
+	return session;
+}
+
+static void rkvenc_session_deinit(struct rkvenc_session *session)
+{
+	if (!session)
+		return;
+
+	if (session->dma) {
+		rkvenc_dma_session_destroy(session->dma);
+		session->dma = NULL;
+	}
+
+	if (session->priv) {
+		kfree(session->priv);
+		session->priv = NULL;
+	}
+
+	kfree(session);
+}
+
+/* ---- Attach session to encoder device ---- */
+static int rkvenc_session_attach_device(struct rkvenc_session *session,
+					struct rkvenc_dev *mpp)
+{
+	session->mpp = mpp;
+	session->dma = rkvenc_dma_session_create(mpp->dev,
+						 mpp->session_max_buffers);
+	if (!session->dma) {
+		rkvenc_err("failed to create dma session\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+/* ---- Class msg alloc/free ---- */
+static int rkvenc_alloc_class_msg(struct rkvenc_task *task, u32 class)
+{
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	u32 base_s = hw->reg_msg[class].base_s;
+	u32 base_e = hw->reg_msg[class].base_e;
+	/*
+	 * BSP treats base_e as the last valid dword (inclusive), unlike the usual
+	 * half-open [base_s, base_e) convention. Keep this to match BSP IOCTL layout.
+	 */
+	u32 size = base_e - base_s + sizeof(u32);
+
+	if (task->reg[class].data)
+		return 0;
+
+	task->reg[class].data = kzalloc(size, GFP_KERNEL);
+	if (!task->reg[class].data)
+		return -ENOMEM;
+
+	task->reg[class].size = size;
+	return 0;
+}
+
+static void rkvenc_free_class_msg(struct rkvenc_task *task)
+{
+	int i;
+
+	for (i = 0; i < RKVENC_CLASS_BUTT; i++) {
+		kfree(task->reg[i].data);
+		task->reg[i].data = NULL;
+		task->reg[i].size = 0;
+		task->reg[i].valid = 0;
+	}
+}
+
+/* ---- Check if request overlaps a class ---- */
+static bool req_over_class(struct mpp_request *req,
+			   struct rkvenc_task *task, u32 class)
+{
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	/* BSP overlap check is inclusive; use last dword of request (offset+size-4). */
+	u32 req_e = req->offset + req->size - sizeof(u32);
+	u32 base_s = hw->reg_msg[class].base_s;
+	u32 base_e = hw->reg_msg[class].base_e;
+
+	return (req->offset <= base_e && req_e >= base_s);
+}
+
+/* ---- Update request to fit within class boundaries ---- */
+static void rkvenc_update_req(struct rkvenc_task *task, u32 class,
+			      struct mpp_request *src, struct mpp_request *dst)
+{
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	u32 base_s = hw->reg_msg[class].base_s;
+	u32 base_e = hw->reg_msg[class].base_e;
+	u32 req_s = src->offset;
+	/* Clamp to BSP's inclusive end to avoid truncating the last register. */
+	u32 req_e = src->offset + src->size - sizeof(u32);
+	u32 s = max(req_s, base_s);
+	u32 e = min(req_e, base_e);
+
+	dst->cmd = src->cmd;
+	dst->flags = src->flags;
+	dst->offset = s;
+	dst->size = e - s + sizeof(u32);
+	/* Adjust data pointer for the offset */
+	dst->data = (void __user *)((u8 __user *)src->data + (s - req_s));
+}
+
+/* ---- Extract RCB info ---- */
+static int rkvenc2_extract_rcb_info(struct rkvenc2_rcb_info *rcb_inf,
+				    struct mpp_request *req)
+{
+	int max_size = ARRAY_SIZE(rcb_inf->elem);
+	int cnt = req->size / sizeof(rcb_inf->elem[0]);
+
+	if (req->size > sizeof(rcb_inf->elem)) {
+		rkvenc_err("count %d, max_size %d\n", cnt, max_size);
+		return -EINVAL;
+	}
+	if (copy_from_user(rcb_inf->elem, req->data, req->size)) {
+		rkvenc_err("copy_from_user failed\n");
+		return -EINVAL;
+	}
+	rcb_inf->cnt = cnt;
+
+	return 0;
+}
+
+/* ---- Extract task messages from userspace ---- */
+static int rkvenc_extract_task_msg(struct rkvenc_session *session,
+				   struct rkvenc_task *task,
+				   struct rkvenc_task_msgs *msgs)
+{
+	int ret;
+	u32 i, j;
+	struct mpp_request *req;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			void *data;
+			struct mpp_request *wreq;
+
+			for (j = 0; j < hw->reg_class; j++) {
+				if (!req_over_class(req, task, j))
+					continue;
+
+				ret = rkvenc_alloc_class_msg(task, j);
+				if (ret) {
+					rkvenc_err("alloc class msg %d fail.\n", j);
+					goto fail;
+				}
+				wreq = &task->w_reqs[task->w_req_cnt];
+				rkvenc_update_req(task, j, req, wreq);
+				data = (u8 *)task->reg[j].data + (wreq->offset - hw->reg_msg[j].base_s);
+				if (!data) {
+					rkvenc_err("get class reg fail, offset %08x\n", wreq->offset);
+					ret = -EINVAL;
+					goto fail;
+				}
+				if (copy_from_user(data, wreq->data, wreq->size)) {
+					rkvenc_err("copy_from_user fail, offset %08x\n", wreq->offset);
+					ret = -EIO;
+					goto fail;
+				}
+				task->reg[j].valid = 1;
+				task->w_req_cnt++;
+			}
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			struct mpp_request *rreq;
+
+			for (j = 0; j < hw->reg_class; j++) {
+				if (!req_over_class(req, task, j))
+					continue;
+
+				ret = rkvenc_alloc_class_msg(task, j);
+				if (ret) {
+					rkvenc_err("alloc class msg reg %d fail.\n", j);
+					goto fail;
+				}
+				rreq = &task->r_reqs[task->r_req_cnt];
+				rkvenc_update_req(task, j, req, rreq);
+				task->reg[j].valid = 1;
+				task->r_req_cnt++;
+			}
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			rkvenc_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		case MPP_CMD_SET_RCB_INFO: {
+			struct rkvenc2_session_priv *priv = session->priv;
+
+			if (priv)
+				rkvenc2_extract_rcb_info(&priv->rcb_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+
+	return 0;
+
+fail:
+	rkvenc_free_class_msg(task);
+	return ret;
+}
+
+/* ---- Get task format from register data ---- */
+static int rkvenc_task_get_format(struct rkvenc_dev *mpp,
+				  struct rkvenc_task *task)
+{
+	u32 offset, val;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	u32 class = hw->fmt_reg.class;
+	u32 *class_reg = task->reg[class].data;
+	u32 class_size = task->reg[class].size;
+	u32 class_base = hw->reg_msg[class].base_s;
+	u32 bitpos = hw->fmt_reg.bitpos;
+	u32 bitlen = hw->fmt_reg.bitlen;
+
+	if (!class_reg || !class_size)
+		return -EINVAL;
+
+	offset = hw->fmt_reg.base - class_base;
+	val = class_reg[offset / sizeof(u32)];
+	task->fmt = (val >> bitpos) & ((1 << bitlen) - 1);
+
+	return 0;
+}
+
+/* ---- Setup task DCHS ID ---- */
+static void rkvenc2_setup_task_id(u32 session_id, struct rkvenc_task *task)
+{
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	u32 val;
+
+	/* always enable tx */
+	val = task->reg[RKVENC_CLASS_PIC].data[hw->dcsh_class_ofst] | DCHS_TXE;
+	if (hw->dcsh_class_ofst)
+		task->reg[RKVENC_CLASS_PIC].data[hw->dcsh_class_ofst] = val;
+	task->dchs_id.val[0] = (((u64)session_id << 32) | val);
+
+	task->dchs_id.txid_orig = task->dchs_id.txid;
+	task->dchs_id.rxid_orig = task->dchs_id.rxid;
+	task->dchs_id.txid_map = task->dchs_id.txid;
+	task->dchs_id.rxid_map = task->dchs_id.rxid;
+
+	task->dchs_id.txe_orig = task->dchs_id.txe;
+	task->dchs_id.rxe_orig = task->dchs_id.rxe;
+	task->dchs_id.txe_map = task->dchs_id.txe;
+	task->dchs_id.rxe_map = task->dchs_id.rxe;
+}
+
+/* ---- Check for slice split task ---- */
+static void rkvenc2_check_split_task(struct rkvenc_dev *mpp, struct rkvenc_task *task)
+{
+	u32 slen_fifo_en = 0;
+	u32 sli_split_en = 0;
+
+	if (task->reg[RKVENC_CLASS_PIC].valid) {
+		u32 *reg = task->reg[RKVENC_CLASS_PIC].data;
+
+		slen_fifo_en = (reg[RKVENC2_REG_ENC_PIC] & RKVENC2_BIT_SLEN_FIFO) ? 1 : 0;
+		sli_split_en = (reg[RKVENC2_REG_SLI_SPLIT] & RKVENC2_BIT_SLI_SPLIT) ? 1 : 0;
+
+		/* H.264 bug: external line buffer + slice flush = bad */
+		if (sli_split_en && slen_fifo_en &&
+		    (reg[RKVENC2_REG_ENC_PIC] & RKVENC2_BIT_ENC_STND) == RKVENC2_BIT_VAL_H264 &&
+		    reg[RKVENC2_REG_EXT_LINE_BUF_BASE])
+			reg[RKVENC2_REG_SLI_SPLIT] &= ~RKVENC2_BIT_SLI_FLUSH;
+	}
+
+	task->task_split = sli_split_en && slen_fifo_en;
+
+	if (task->task_split)
+		INIT_KFIFO(task->slice_info);
+}
+
+/* ---- Set RCB buffer addresses from SRAM ---- */
+static void rkvenc2_set_rcbbuf(struct rkvenc_dev *enc,
+			       struct rkvenc_session *session,
+			       struct rkvenc_task *task)
+{
+	struct rkvenc2_session_priv *priv = session->priv;
+
+	if (priv && enc->sram_iova) {
+		int i;
+		u32 *reg;
+		u32 reg_idx, rcb_size, rcb_offset;
+		struct rkvenc2_rcb_info *rcb_inf = &priv->rcb_inf;
+
+		rcb_offset = 0;
+		for (i = 0; i < rcb_inf->cnt; i++) {
+			reg_idx = rcb_inf->elem[i].index;
+			rcb_size = rcb_inf->elem[i].size;
+
+			if (rcb_offset > enc->sram_size ||
+			    (rcb_offset + rcb_size) > enc->sram_used)
+				continue;
+
+			/* Get class reg for the RCB register index */
+			{
+				const struct rkvenc_hw_info *hw = task->hw_info;
+				int c;
+
+				for (c = 0; c < hw->reg_class; c++) {
+					u32 bs = hw->reg_msg[c].base_s;
+					u32 be = hw->reg_msg[c].base_e;
+					u32 addr = reg_idx * sizeof(u32);
+
+					if (addr >= bs && addr < be) {
+						reg = (u32 *)((u8 *)task->reg[c].data + (addr - bs));
+						*reg = enc->sram_iova + rcb_offset;
+						break;
+					}
+				}
+			}
+
+			rcb_offset += rcb_size;
+		}
+	}
+}
+
+/* ---- Alloc task from session ---- */
+static struct rkvenc_task *rkvenc_alloc_task(struct rkvenc_session *session,
+					     struct rkvenc_task_msgs *msgs)
+{
+	int ret;
+	struct rkvenc_task *task;
+	struct rkvenc_mpp_task *mpp_task;
+	struct rkvenc_dev *mpp = session->mpp;
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	rkvenc_task_init(session, mpp_task);
+	mpp_task->hw_info = &mpp->hw_info->hw;
+	task->hw_info = mpp->hw_info;
+
+	/* extract reqs for current task */
+	ret = rkvenc_extract_task_msg(session, task, msgs);
+	if (ret)
+		goto free_task;
+	mpp_task->reg = task->reg[0].data;
+
+	/* get format */
+	ret = rkvenc_task_get_format(mpp, task);
+	if (ret)
+		goto free_task;
+
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		u32 i, j;
+		int cnt;
+		u32 off;
+		const u16 *tbl;
+		const struct rkvenc_hw_info *hw = task->hw_info;
+		int fd_bs = -1;
+
+		for (i = 0; i < hw->fd_class; i++) {
+			u32 class = hw->fd_reg[i].class;
+			u32 fmt = hw->fd_reg[i].base_fmt + task->fmt;
+			u32 *reg = task->reg[class].data;
+			u32 ss = hw->reg_msg[class].base_s / sizeof(u32);
+			u32 mem_count_before;
+
+			if (!reg)
+				continue;
+
+			if (fmt == RKVENC_FMT_JPEGE && class == RKVENC_CLASS_PIC && fd_bs == -1) {
+				int bs_index = mpp->trans_info[fmt].table[2];
+
+				fd_bs = reg[bs_index];
+				task->offset_bs = rkvenc_query_reg_offset_info(&task->off_inf,
+									       bs_index + ss);
+			}
+
+			mem_count_before = mpp_task->mem_count;
+			ret = rkvenc_translate_reg_address(session, mpp_task, fmt, class, reg, NULL);
+			if (ret)
+				goto fail;
+
+			cnt = mpp->trans_info[fmt].count;
+			tbl = mpp->trans_info[fmt].table;
+			for (j = 0; j < cnt; j++) {
+				off = rkvenc_query_reg_offset_info(&task->off_inf, tbl[j] + ss);
+				reg[tbl[j]] += off;
+			}
+
+			/* Guardrail: ensure translated regs fall inside their mapped buffers */
+			{
+				u32 mc;
+
+				for (mc = mem_count_before; mc < mpp_task->mem_count; mc++) {
+					struct rkvenc_mem_region *mr = &mpp_task->mem_regions[mc];
+					u32 reg_size;
+					u32 reg_val;
+					unsigned long reg_off;
+					bool allow_end;
+					dma_addr_t end;
+					dma_addr_t reg_iova;
+
+					if (mr->reg_class != class)
+						continue;
+					if (!mr->len)
+						continue;
+
+					reg_size = task->reg[class].size / sizeof(u32);
+					if (mr->reg_idx >= reg_size) {
+						dev_err(mpp->dev,
+							"guardrail: class %u reg_idx %u out of range (%u dwords) fd %d\n",
+							class, mr->reg_idx, reg_size, mr->fd);
+						ret = -EINVAL;
+						goto fail;
+					}
+
+					reg_val = reg[mr->reg_idx];
+					reg_iova = (dma_addr_t)reg_val;
+					end = mr->iova + mr->len;
+					reg_off = rkvenc_query_reg_offset_info(&task->off_inf,
+									   mr->reg_idx + ss);
+					/* BSP encodes BSBT as base+size (end pointer); allow end-of-buffer when offset==len. */
+					allow_end = (reg_iova == end) && (reg_off == mr->len);
+					if (reg_iova < mr->iova || (!allow_end && reg_iova >= end)) {
+						dev_err(mpp->dev,
+							"guardrail: class %u reg[%u]=%#08x outside iova [%pad..%pad) fd %d\n",
+							class, mr->reg_idx, reg_val, &mr->iova, &end, mr->fd);
+						ret = -EINVAL;
+						goto fail;
+					}
+				}
+			}
+		}
+
+		if (fd_bs >= 0) {
+			struct rkvenc_dma_buffer *bs_buf =
+				rkvenc_dma_find_buffer_fd(session->dma, fd_bs);
+
+			if (bs_buf && task->offset_bs > 0)
+				rkvenc_dma_buf_sync(bs_buf, 0, task->offset_bs,
+						   DMA_TO_DEVICE, false);
+			task->bs_buf = bs_buf;
+		}
+	}
+
+	rkvenc2_setup_task_id(session->index, task);
+	task->clk_mode = CLK_MODE_NORMAL;
+	rkvenc2_check_split_task(mpp, task);
+
+	/* Init wait queue and reference count */
+	init_waitqueue_head(&mpp_task->wait);
+	kref_init(&mpp_task->ref);
+	atomic_set(&mpp_task->abort_request, 0);
+	mpp_task->task_index = atomic_inc_return(&mpp->task_index);
+	mpp_task->task_id = atomic_inc_return(&mpp->queue->task_id);
+	atomic_inc(&session->task_count);
+	atomic_inc(&mpp->task_count);
+
+	return task;
+
+fail:
+	rkvenc_task_finalize(session, mpp_task);
+	rkvenc_free_class_msg(task);
+free_task:
+	kfree(task);
+	return NULL;
+}
+
+/* ---- Result: copy status regs back to userspace ---- */
+static int rkvenc_result(struct rkvenc_dev *mpp,
+			 struct rkvenc_mpp_task *mpp_task)
+{
+	u32 i;
+	struct rkvenc_task *task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		struct mpp_request *req = &task->r_reqs[i];
+		const struct rkvenc_hw_info *hw = task->hw_info;
+		u32 class_base;
+		u32 *reg;
+		int c;
+
+		/* Find class for this read request offset */
+		for (c = 0; c < hw->reg_class; c++) {
+			if (req->offset >= hw->reg_msg[c].base_s &&
+			    req->offset < hw->reg_msg[c].base_e) {
+				class_base = hw->reg_msg[c].base_s;
+				reg = (u32 *)((u8 *)task->reg[c].data + (req->offset - class_base));
+				break;
+			}
+		}
+
+		if (c == hw->reg_class) {
+			rkvenc_err("read request offset %x not in any class\n", req->offset);
+			return -EINVAL;
+		}
+
+		if (copy_to_user(req->data, reg, req->size)) {
+			rkvenc_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+/* ---- Task ISR bottom half (called from kthread worker) ---- */
+void rkvenc_task_worker_default(struct kthread_work *work)
+{
+	struct rkvenc_dev *enc = container_of(work, struct rkvenc_dev, work);
+	struct rkvenc_taskqueue *queue = enc->queue;
+	struct rkvenc_mpp_task *mpp_task;
+	struct rkvenc_task *task;
+	struct rkvenc_session *session;
+
+	/* Process finished tasks (ISR bottom half) */
+	mpp_task = enc->cur_task;
+	if (mpp_task && test_bit(TASK_STATE_IRQ, &mpp_task->state)) {
+		enc->cur_task = NULL;
+
+		task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+		session = mpp_task->session;
+
+		task->irq_status = enc->irq_status;
+
+		/* Update DCHS state */
+		if (enc->ccu) {
+			unsigned long flags;
+
+			spin_lock_irqsave(&enc->ccu->lock_dchs, flags);
+			enc->ccu->dchs[enc->core_id].val[0] = 0;
+			enc->ccu->dchs[enc->core_id].val[1] = 0;
+			spin_unlock_irqrestore(&enc->ccu->lock_dchs, flags);
+		}
+
+		/* Check for errors */
+		if (task->irq_status & enc->hw_info->err_mask)
+			atomic_inc(&enc->reset_request);
+
+		rkvenc_task_finish(session, mpp_task);
+
+		set_bit(enc->core_id, &queue->core_idle);
+	}
+
+	/* Process timed-out tasks */
+	if (mpp_task && test_bit(TASK_STATE_TIMEOUT, &mpp_task->state)) {
+		enc->cur_task = NULL;
+
+		session = mpp_task->session;
+
+		rkvenc_err("task %d timeout, reset\n", mpp_task->task_index);
+		atomic_inc(&enc->reset_request);
+
+		rkvenc_task_finish(session, mpp_task);
+		set_bit(enc->core_id, &queue->core_idle);
+	}
+
+	/* Trigger pending tasks if core is idle */
+	{
+		struct rkvenc_mpp_task *pending_task = NULL;
+		unsigned long flags;
+		unsigned long core_idle;
+		s32 core_id;
+
+		spin_lock_irqsave(&queue->running_lock, flags);
+		core_idle = queue->core_idle;
+		core_id = find_first_bit(&core_idle, queue->core_id_max + 1);
+
+		if (core_id <= queue->core_id_max && queue->cores[core_id]) {
+			mutex_lock(&queue->pending_lock);
+			pending_task = list_first_entry_or_null(&queue->pending_list,
+								struct rkvenc_mpp_task,
+								queue_link);
+			if (pending_task) {
+				list_del_init(&pending_task->queue_link);
+				clear_bit(core_id, &queue->core_idle);
+				pending_task->mpp = queue->cores[core_id];
+				pending_task->core_id = core_id;
+			}
+			mutex_unlock(&queue->pending_lock);
+		}
+		spin_unlock_irqrestore(&queue->running_lock, flags);
+
+		if (pending_task) {
+			struct rkvenc_dev *target = pending_task->mpp;
+			struct rkvenc_task *enc_task = container_of(pending_task,
+								   struct rkvenc_task,
+								   mpp_task);
+
+			/* Set RCB buffers if SRAM available */
+			rkvenc2_set_rcbbuf(target, pending_task->session, enc_task);
+
+			/* Run on hardware */
+			set_bit(TASK_STATE_RUNNING, &pending_task->state);
+			rkvenc_hw_run(target, pending_task);
+		}
+	}
+}
+
+/* ---- Process ioctl requests ---- */
+static int rkvenc_check_cmd(unsigned int cmd)
+{
+	if (cmd >= MPP_CMD_BUTT)
+		return -EINVAL;
+	return 0;
+}
+
+static int rkvenc_process_request(struct rkvenc_session *session,
+				  struct mpp_request *req,
+				  struct rkvenc_task_msgs *msgs)
+{
+	rkvenc_dbg(DEBUG_IOCTL, "cmd %x, size %d, offset %x\n",
+		   req->cmd, req->size, req->offset);
+
+	switch (req->cmd) {
+	case MPP_CMD_QUERY_HW_SUPPORT: {
+		u32 val = session->srv->hw_support;
+
+		if (put_user(val, (u32 __user *)req->data))
+			return -EFAULT;
+	} break;
+	case MPP_CMD_QUERY_HW_ID: {
+		struct rkvenc_dev *mpp = session->mpp;
+		u32 val = mpp ? mpp->hw_info->hw.hw_id : 0;
+
+		if (put_user(val, (u32 __user *)req->data))
+			return -EFAULT;
+	} break;
+	case MPP_CMD_INIT_CLIENT_TYPE: {
+		u32 client_type;
+
+		if (get_user(client_type, (u32 __user *)req->data))
+			return -EFAULT;
+
+		session->device_type = client_type;
+
+		/* Attach to the encoder device */
+		if (!session->mpp) {
+			struct rkvenc_dev *dev =
+				session->srv->sub_devices[MPP_DEVICE_RKVENC];
+
+			if (dev)
+				rkvenc_session_attach_device(session, dev);
+		}
+
+		/* Init session private data */
+		if (!session->priv) {
+			struct rkvenc2_session_priv *priv;
+
+			priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+			if (priv) {
+				init_rwsem(&priv->rw_sem);
+				session->priv = priv;
+			}
+		}
+	} break;
+	case MPP_CMD_INIT_TRANS_TABLE: {
+		int cnt = req->size / sizeof(u16);
+
+		if (cnt > MPP_MAX_REG_TRANS_NUM) {
+			rkvenc_err("trans count %d too large\n", cnt);
+			return -EINVAL;
+		}
+		if (copy_from_user(session->trans_table, req->data, req->size)) {
+			rkvenc_err("copy_from_user trans_table failed\n");
+			return -EFAULT;
+		}
+		session->trans_count = cnt;
+	} break;
+	case MPP_CMD_SET_REG_WRITE:
+	case MPP_CMD_SET_REG_READ:
+	case MPP_CMD_SET_REG_ADDR_OFFSET:
+	case MPP_CMD_SET_RCB_INFO: {
+		msgs->set_cnt++;
+	} break;
+	case MPP_CMD_POLL_HW_FINISH: {
+		msgs->poll_cnt++;
+		msgs->poll_req = req;
+	} break;
+	case MPP_CMD_TRANS_FD_TO_IOVA: {
+		/* Static FD->IOVA translation for buffer pre-import */
+		if (session->mpp && session->dma) {
+			u32 fd;
+
+			if (get_user(fd, (u32 __user *)req->data))
+				return -EFAULT;
+
+			rkvenc_iommu_down_read(session->mpp->iommu_info);
+			rkvenc_dma_import_fd(session->mpp->iommu_info, session->dma, fd, 1);
+			rkvenc_iommu_up_read(session->mpp->iommu_info);
+		}
+	} break;
+	case MPP_CMD_RELEASE_FD: {
+		if (session->dma) {
+			u32 fd;
+
+			if (get_user(fd, (u32 __user *)req->data))
+				return -EFAULT;
+			rkvenc_dma_release_fd(session->dma, fd);
+		}
+	} break;
+	case MPP_CMD_SEND_CODEC_INFO: {
+		/* Codec info is optional, just store it */
+		if (session->priv) {
+			int ci;
+			int cnt;
+			struct codec_info_elem elem;
+			struct rkvenc2_session_priv *priv = session->priv;
+
+			cnt = req->size / sizeof(elem);
+			cnt = (cnt > ENC_INFO_BUTT) ? ENC_INFO_BUTT : cnt;
+			for (ci = 0; ci < cnt; ci++) {
+				if (copy_from_user(&elem,
+						   (void __user *)req->data + ci * sizeof(elem),
+						   sizeof(elem)))
+					continue;
+				if (elem.type > ENC_INFO_BASE && elem.type < ENC_INFO_BUTT) {
+					priv->codec_info[elem.type].flag = elem.flag;
+					priv->codec_info[elem.type].val = elem.data;
+				}
+			}
+		}
+	} break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+/* ---- Collect messages from one ioctl call ---- */
+static int rkvenc_collect_msgs(struct rkvenc_session *session,
+			       unsigned int cmd, void __user *msg,
+			       struct rkvenc_task_msgs *msgs)
+{
+	struct mpp_msg_v1 msg_v1;
+	struct mpp_request *req;
+	int last = 1;
+	int ret;
+
+	if (cmd != MPP_IOC_CFG_V1) {
+		rkvenc_err("unknown ioctl cmd %x\n", cmd);
+		return -EINVAL;
+	}
+
+next:
+	if (copy_from_user(&msg_v1, msg, sizeof(msg_v1)))
+		return -EFAULT;
+
+	msg += sizeof(msg_v1);
+
+	if (rkvenc_check_cmd(msg_v1.cmd)) {
+		rkvenc_err("cmd %x not supported\n", msg_v1.cmd);
+		return -EFAULT;
+	}
+
+	if (msg_v1.flags & MPP_FLAGS_MULTI_MSG)
+		last = (msg_v1.flags & MPP_FLAGS_LAST_MSG) ? 1 : 0;
+	else
+		last = 1;
+
+	if (msgs->req_cnt >= MPP_MAX_MSG_NUM) {
+		rkvenc_err("message count %d more than %d\n",
+			   msgs->req_cnt, MPP_MAX_MSG_NUM);
+		return -EINVAL;
+	}
+
+	req = &msgs->reqs[msgs->req_cnt++];
+	req->cmd = msg_v1.cmd;
+	req->flags = msg_v1.flags;
+	req->size = msg_v1.size;
+	req->offset = msg_v1.offset;
+	req->data = (void __user *)(unsigned long)msg_v1.data_ptr;
+
+	/* Update session flags */
+	session->msg_flags = msg_v1.flags;
+	msgs->flags = msg_v1.flags;
+
+	ret = rkvenc_process_request(session, req, msgs);
+	if (ret) {
+		rkvenc_err("process cmd %x ret %d\n", req->cmd, ret);
+		return ret;
+	}
+
+	if (!last)
+		goto next;
+
+	return 0;
+}
+
+/* ---- Wait for task result ---- */
+static int rkvenc_wait_result(struct rkvenc_session *session,
+			      struct rkvenc_task_msgs *msgs)
+{
+	struct rkvenc_mpp_task *task;
+	struct rkvenc_task *enc_task;
+	union rkvenc2_slice_len_info slice_info;
+	int ret = 0;
+
+	mutex_lock(&session->pending_lock);
+	task = list_first_entry_or_null(&session->pending_list,
+					struct rkvenc_mpp_task,
+					pending_link);
+	mutex_unlock(&session->pending_lock);
+
+	if (!task) {
+		rkvenc_err("session %p pending list is empty!\n", session);
+		return -EIO;
+	}
+
+	enc_task = container_of(task, struct rkvenc_task, mpp_task);
+
+	if (!enc_task->task_split || enc_task->task_split_done) {
+task_done_ret:
+		ret = wait_event_interruptible(task->wait,
+					       test_bit(TASK_STATE_DONE, &task->state));
+		if (ret == -ERESTARTSYS)
+			rkvenc_err("wait task break by signal\n");
+
+		/* Copy results to userspace */
+		rkvenc_result(task->mpp, task);
+
+		/* Pop from pending list */
+		mutex_lock(&session->pending_lock);
+		list_del_init(&task->pending_link);
+		mutex_unlock(&session->pending_lock);
+
+		kref_put(&task->ref, rkvenc_free_task_callback);
+
+		return ret;
+	}
+
+	/* Slice split mode: wait for all slices */
+	do {
+		ret = wait_event_interruptible(task->wait,
+					       kfifo_out(&enc_task->slice_info,
+							 &slice_info, 1));
+		if (ret == -ERESTARTSYS) {
+			rkvenc_err("wait task break by signal in slice mode\n");
+			return 0;
+		}
+
+		enc_task->slice_rd_cnt++;
+
+		if (slice_info.last)
+			goto task_done_ret;
+	} while (1);
+}
+
+/* ---- Main ioctl handler ---- */
+static long rkvenc_dev_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct rkvenc_session *session = filp->private_data;
+	struct rkvenc_task_msgs msgs;
+	struct rkvenc_task *task;
+	int ret = 0;
+
+	if (!session || !session->srv) {
+		rkvenc_err("session %p\n", session);
+		return -EINVAL;
+	}
+
+	if (atomic_read(&session->release_request) > 0)
+		return -EBUSY;
+	if (atomic_read(&session->srv->shutdown_request) > 0)
+		return -EBUSY;
+
+	/* Init msgs */
+	memset(&msgs, 0, sizeof(msgs));
+	INIT_LIST_HEAD(&msgs.list);
+	msgs.session = session;
+
+	/* Phase 1: Collect all messages from the ioctl */
+	ret = rkvenc_collect_msgs(session, cmd, (void __user *)arg, &msgs);
+	if (ret) {
+		rkvenc_err("collect msgs failed %d\n", ret);
+		return ret;
+	}
+
+	/* Phase 2: If we have SET requests, create a task and submit */
+	if (msgs.set_cnt && session->mpp) {
+		struct rkvenc_taskqueue *queue = session->mpp->queue;
+
+		task = rkvenc_alloc_task(session, &msgs);
+		if (!task) {
+			rkvenc_err("alloc task failed\n");
+			return -ENOMEM;
+		}
+
+		msgs.task = &task->mpp_task;
+		msgs.mpp = session->mpp;
+		msgs.queue = queue;
+
+		/* Add to pending list */
+		kref_get(&task->mpp_task.ref);
+		mutex_lock(&session->pending_lock);
+		list_add_tail(&task->mpp_task.pending_link, &session->pending_list);
+		mutex_unlock(&session->pending_lock);
+
+		/* Add to task queue and trigger worker */
+		set_bit(TASK_STATE_PENDING, &task->mpp_task.state);
+		mutex_lock(&queue->pending_lock);
+		list_add_tail(&task->mpp_task.queue_link, &queue->pending_list);
+		mutex_unlock(&queue->pending_lock);
+
+		/* Trigger the worker to run the task */
+		kthread_queue_work(&queue->worker, &session->mpp->work);
+	}
+
+	/* Phase 3: If we have POLL requests, wait for result */
+	if (msgs.poll_cnt) {
+		ret = rkvenc_wait_result(session, &msgs);
+		if (ret)
+			rkvenc_err("wait result ret %d\n", ret);
+	}
+
+	return ret;
+}
+
+/* ---- File operations ---- */
+static int rkvenc_dev_open(struct inode *inode, struct file *filp)
+{
+	struct rkvenc_service *srv = container_of(inode->i_cdev,
+						  struct rkvenc_service,
+						  mpp_cdev);
+	struct rkvenc_session *session;
+
+	session = rkvenc_session_init();
+	if (!session)
+		return -ENOMEM;
+
+	session->srv = srv;
+
+	if (!srv->sub_devices[MPP_DEVICE_RKVENC]) {
+		rkvenc_session_deinit(session);
+		return -ENODEV;
+	}
+
+	mutex_lock(&srv->session_lock);
+	list_add_tail(&session->service_link, &srv->session_list);
+	session->index = atomic_inc_return(&srv->sub_devices[MPP_DEVICE_RKVENC]->session_index);
+	mutex_unlock(&srv->session_lock);
+
+	filp->private_data = session;
+
+	return nonseekable_open(inode, filp);
+}
+
+static int rkvenc_dev_release(struct inode *inode, struct file *filp)
+{
+	struct rkvenc_session *session = filp->private_data;
+
+	if (!session) {
+		rkvenc_err("session is null\n");
+		return -EINVAL;
+	}
+
+	atomic_inc(&session->release_request);
+
+	/* Remove from service list */
+	if (session->srv) {
+		mutex_lock(&session->srv->session_lock);
+		list_del_init(&session->service_link);
+		mutex_unlock(&session->srv->session_lock);
+	}
+
+	/* Wait for all tasks to complete */
+	if (atomic_read(&session->task_count) > 0) {
+		struct rkvenc_mpp_task *task, *n;
+
+		mutex_lock(&session->pending_lock);
+		list_for_each_entry_safe(task, n, &session->pending_list, pending_link) {
+			/* Wait for task completion with timeout */
+			wait_event_timeout(task->wait,
+					   test_bit(TASK_STATE_DONE, &task->state),
+					   msecs_to_jiffies(2000));
+			list_del_init(&task->pending_link);
+			kref_put(&task->ref, rkvenc_free_task_callback);
+		}
+		mutex_unlock(&session->pending_lock);
+	}
+
+	rkvenc_session_deinit(session);
+	filp->private_data = NULL;
+
+	return 0;
+}
+
+const struct file_operations rkvenc_fops = {
+	.open		= rkvenc_dev_open,
+	.release	= rkvenc_dev_release,
+	.unlocked_ioctl = rkvenc_dev_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl   = rkvenc_dev_ioctl,
+#endif
+};
+
+/* ---- Service probe/remove ---- */
+int rkvenc_service_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct rkvenc_service *srv;
+	struct device *dev = &pdev->dev;
+	u32 taskqueue_cnt = 0;
+	u32 resetgroup_cnt = 0;
+
+	srv = devm_kzalloc(dev, sizeof(*srv), GFP_KERNEL);
+	if (!srv)
+		return -ENOMEM;
+
+	srv->dev = dev;
+	mutex_init(&srv->session_lock);
+	INIT_LIST_HEAD(&srv->session_list);
+	atomic_set(&srv->shutdown_request, 0);
+
+	/* Read DTS properties */
+	of_property_read_u32(dev->of_node, "rockchip,taskqueue-count", &taskqueue_cnt);
+	of_property_read_u32(dev->of_node, "rockchip,resetgroup-count", &resetgroup_cnt);
+	srv->taskqueue_cnt = taskqueue_cnt;
+	srv->reset_group_cnt = resetgroup_cnt;
+
+	/* Create the task queue for RKVENC */
+	{
+		struct rkvenc_taskqueue *queue;
+
+		queue = devm_kzalloc(dev, sizeof(*queue), GFP_KERNEL);
+		if (!queue)
+			return -ENOMEM;
+
+		mutex_init(&queue->session_lock);
+		mutex_init(&queue->pending_lock);
+		spin_lock_init(&queue->running_lock);
+		mutex_init(&queue->dev_lock);
+		INIT_LIST_HEAD(&queue->session_attach);
+		INIT_LIST_HEAD(&queue->session_detach);
+		INIT_LIST_HEAD(&queue->pending_list);
+		INIT_LIST_HEAD(&queue->running_list);
+		INIT_LIST_HEAD(&queue->dev_list);
+		atomic_set(&queue->reset_request, 0);
+		atomic_set(&queue->detach_count, 0);
+		atomic_set(&queue->task_id, 0);
+		queue->core_idle = (unsigned long)-1;
+
+		/* Create kthread worker */
+		kthread_init_worker(&queue->worker);
+		queue->kworker_task = kthread_run(kthread_worker_fn,
+						  &queue->worker,
+						  "rkvenc-worker");
+		if (IS_ERR(queue->kworker_task)) {
+			dev_err(dev, "failed to create kthread worker\n");
+			return PTR_ERR(queue->kworker_task);
+		}
+
+		srv->task_queues[MPP_DEVICE_RKVENC] = queue;
+	}
+
+	/* Create reset group */
+	if (resetgroup_cnt > 0) {
+		struct rkvenc_reset_group *rg;
+
+		rg = devm_kzalloc(dev, sizeof(*rg), GFP_KERNEL);
+		if (!rg)
+			return -ENOMEM;
+
+		init_rwsem(&rg->rw_sem);
+		rg->rw_sem_on = true;
+		srv->reset_groups[0] = rg;
+	}
+
+	/* Allocate char device */
+	ret = alloc_chrdev_region(&srv->dev_id, 0, 1, MPP_SERVICE_NAME);
+	if (ret) {
+		dev_err(dev, "alloc_chrdev_region failed: %d\n", ret);
+		return ret;
+	}
+
+	cdev_init(&srv->mpp_cdev, &rkvenc_fops);
+	srv->mpp_cdev.owner = THIS_MODULE;
+
+	ret = cdev_add(&srv->mpp_cdev, srv->dev_id, 1);
+	if (ret) {
+		dev_err(dev, "cdev_add failed: %d\n", ret);
+		goto err_cdev;
+	}
+
+	srv->cls = class_create(MPP_CLASS_NAME);
+	if (IS_ERR(srv->cls)) {
+		ret = PTR_ERR(srv->cls);
+		dev_err(dev, "class_create failed: %d\n", ret);
+		goto err_class;
+	}
+
+	srv->child_dev = device_create(srv->cls, dev, srv->dev_id,
+				       NULL, MPP_SERVICE_NAME);
+	if (IS_ERR(srv->child_dev)) {
+		ret = PTR_ERR(srv->child_dev);
+		dev_err(dev, "device_create failed: %d\n", ret);
+		goto err_device;
+	}
+
+	platform_set_drvdata(pdev, srv);
+	dev_info(dev, "mpp_service probe success\n");
+
+	return 0;
+
+err_device:
+	class_destroy(srv->cls);
+err_class:
+	cdev_del(&srv->mpp_cdev);
+err_cdev:
+	unregister_chrdev_region(srv->dev_id, 1);
+	return ret;
+}
+
+int rkvenc_service_remove(struct platform_device *pdev)
+{
+	struct rkvenc_service *srv = platform_get_drvdata(pdev);
+
+	if (!srv)
+		return 0;
+
+	/* Stop kthread worker */
+	if (srv->task_queues[MPP_DEVICE_RKVENC]) {
+		struct rkvenc_taskqueue *queue = srv->task_queues[MPP_DEVICE_RKVENC];
+
+		if (queue->kworker_task) {
+			kthread_flush_worker(&queue->worker);
+			kthread_stop(queue->kworker_task);
+		}
+	}
+
+	device_destroy(srv->cls, srv->dev_id);
+	class_destroy(srv->cls);
+	cdev_del(&srv->mpp_cdev);
+	unregister_chrdev_region(srv->dev_id, 1);
+
+	return 0;
+}
diff -urN a/drivers/media/platform/rockchip/rkvenc/rkvenc_task.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_task.c
--- a/drivers/media/platform/rockchip/rkvenc/rkvenc_task.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_task.c	2026-02-07 21:27:09
@@ -0,0 +1,300 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - Task lifecycle management
+ * Ported from Rockchip BSP mpp_common.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ * Copyright (C) 2026 Ross Cawston
+ */
+
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+
+#include "rkvenc_hw.h"
+
+/* ---- Task init/finish/finalize ---- */
+int rkvenc_task_init(struct rkvenc_session *session, struct rkvenc_mpp_task *task)
+{
+	INIT_LIST_HEAD(&task->pending_link);
+	INIT_LIST_HEAD(&task->queue_link);
+	INIT_LIST_HEAD(&task->mem_region_list);
+	task->state = 0;
+	task->mem_count = 0;
+	task->session = session;
+
+	return 0;
+}
+
+static struct rkvenc_dev *rkvenc_get_task_used_device(struct rkvenc_mpp_task *task,
+						     struct rkvenc_session *session)
+{
+	if (task->mpp)
+		return task->mpp;
+	return session->mpp;
+}
+
+void rkvenc_free_task_callback(struct kref *ref)
+{
+	struct rkvenc_mpp_task *task = container_of(ref, struct rkvenc_mpp_task, ref);
+	struct rkvenc_session *session;
+	struct rkvenc_dev *mpp;
+	struct rkvenc_task *enc_task = container_of(task, struct rkvenc_task, mpp_task);
+
+	if (!task->session) {
+		rkvenc_err("task %p, task->session is null.\n", task);
+		return;
+	}
+	session = task->session;
+	mpp = rkvenc_get_task_used_device(task, session);
+
+	/* Release memory regions and free class register buffers */
+	rkvenc_task_finalize(session, task);
+
+	/* Free class register buffers */
+	{
+		int i;
+
+		for (i = 0; i < RKVENC_CLASS_BUTT; i++) {
+			kfree(enc_task->reg[i].data);
+			enc_task->reg[i].data = NULL;
+			enc_task->reg[i].size = 0;
+			enc_task->reg[i].valid = 0;
+		}
+	}
+
+	atomic_dec(&session->task_count);
+	atomic_dec(&mpp->task_count);
+
+	kfree(enc_task);
+}
+
+int rkvenc_task_finish(struct rkvenc_session *session,
+		       struct rkvenc_mpp_task *task)
+{
+	struct rkvenc_dev *mpp = rkvenc_get_task_used_device(task, session);
+	struct rkvenc_taskqueue *queue = mpp->queue;
+
+	/* Read status registers back from HW */
+	rkvenc_hw_finish(mpp, task);
+
+	/* Handle reset if needed */
+	if (mpp->reset_group) {
+		up_read(&mpp->reset_group->rw_sem);
+		if (atomic_read(&mpp->reset_request) > 0)
+			rkvenc_hw_reset(mpp);
+	}
+
+	/* Power off encoder and its IOMMU */
+	rkvenc_hw_clk_off(mpp);
+	pm_relax(mpp->dev);
+	pm_runtime_mark_last_busy(mpp->dev);
+	pm_runtime_put_autosuspend(mpp->dev);
+	if (mpp->iommu_info && mpp->iommu_info->pdev)
+		pm_runtime_put_sync(&mpp->iommu_info->pdev->dev);
+
+	set_bit(TASK_STATE_FINISH, &task->state);
+	set_bit(TASK_STATE_DONE, &task->state);
+
+	/* Wake up the GET thread */
+	wake_up(&task->wait);
+
+	/* Pop from running queue */
+	{
+		unsigned long flags;
+
+		spin_lock_irqsave(&queue->running_lock, flags);
+		list_del_init(&task->queue_link);
+		spin_unlock_irqrestore(&queue->running_lock, flags);
+		kref_put(&task->ref, rkvenc_free_task_callback);
+	}
+
+	return 0;
+}
+
+int rkvenc_task_finalize(struct rkvenc_session *session,
+			 struct rkvenc_mpp_task *task)
+{
+	struct rkvenc_mem_region *mem_region = NULL, *n;
+	struct rkvenc_dev *mpp = rkvenc_get_task_used_device(task, session);
+
+	list_for_each_entry_safe(mem_region, n, &task->mem_region_list, reg_link) {
+		if (!mem_region->is_dup) {
+			rkvenc_iommu_down_read(mpp->iommu_info);
+			rkvenc_dma_release(session->dma, mem_region->hdl);
+			rkvenc_iommu_up_read(mpp->iommu_info);
+		}
+		list_del_init(&mem_region->reg_link);
+	}
+
+	return 0;
+}
+
+/* ---- Memory region attach ---- */
+static struct rkvenc_mem_region *
+rkvenc_task_attach_fd(struct rkvenc_mpp_task *task, int fd)
+{
+	struct rkvenc_mem_region *mem_region = NULL, *loop = NULL, *n;
+	struct rkvenc_dma_buffer *buffer = NULL;
+	struct rkvenc_dev *mpp = task->session->mpp;
+	struct rkvenc_dma_session *dma = task->session->dma;
+	u32 mem_num = ARRAY_SIZE(task->mem_regions);
+	bool found = false;
+
+	if (fd <= 0 || !dma || !mpp)
+		return ERR_PTR(-EINVAL);
+
+	if (task->mem_count > mem_num) {
+		rkvenc_err("mem_count %d must less than %d\n", task->mem_count, mem_num);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	/* find fd whether had import */
+	list_for_each_entry_safe_reverse(loop, n, &task->mem_region_list, reg_link) {
+		if (loop->fd == fd) {
+			found = true;
+			break;
+		}
+	}
+
+	mem_region = &task->mem_regions[task->mem_count];
+	if (found) {
+		memcpy(mem_region, loop, sizeof(*loop));
+		mem_region->reg_class = 0;
+		mem_region->is_dup = true;
+	} else {
+		rkvenc_iommu_down_read(mpp->iommu_info);
+		buffer = rkvenc_dma_import_fd(mpp->iommu_info, dma, fd, 0);
+		rkvenc_iommu_up_read(mpp->iommu_info);
+		if (IS_ERR(buffer)) {
+			rkvenc_err("can't import dma-buf %d\n", fd);
+			return ERR_CAST(buffer);
+		}
+
+		mem_region->hdl = buffer;
+		mem_region->iova = buffer->iova;
+		mem_region->len = buffer->size;
+		mem_region->fd = fd;
+		mem_region->reg_class = 0;
+		mem_region->is_dup = false;
+	}
+	task->mem_count++;
+	INIT_LIST_HEAD(&mem_region->reg_link);
+	list_add_tail(&mem_region->reg_link, &task->mem_region_list);
+
+	return mem_region;
+}
+
+/* ---- FD translation ---- */
+int rkvenc_translate_reg_address(struct rkvenc_session *session,
+				 struct rkvenc_mpp_task *task, int fmt, u32 reg_class,
+				 u32 *reg, struct reg_offset_info *off_inf)
+{
+	int i;
+	int cnt;
+	const u16 *tbl;
+	struct rkvenc_dev *mpp = rkvenc_get_task_used_device(task, session);
+
+	if (session->trans_count > 0) {
+		cnt = session->trans_count;
+		tbl = session->trans_table;
+	} else {
+		cnt = mpp->trans_info[fmt].count;
+		tbl = mpp->trans_info[fmt].table;
+	}
+
+	for (i = 0; i < cnt; i++) {
+		int usr_fd;
+		u32 offset;
+		struct rkvenc_mem_region *mem_region;
+
+		if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET) {
+			usr_fd = reg[tbl[i]];
+			offset = 0;
+		} else {
+			usr_fd = reg[tbl[i]] & 0x3ff;
+			offset = reg[tbl[i]] >> 10;
+		}
+
+		if (usr_fd == 0)
+			continue;
+
+		mem_region = rkvenc_task_attach_fd(task, usr_fd);
+		if (IS_ERR(mem_region)) {
+			rkvenc_err("reg[%3d]: 0x%08x fd %d failed\n",
+				   tbl[i], reg[tbl[i]], usr_fd);
+			return PTR_ERR(mem_region);
+		}
+		mem_region->reg_class = reg_class;
+		mem_region->reg_idx = tbl[i];
+		reg[tbl[i]] = mem_region->iova + offset;
+	}
+
+	return 0;
+}
+
+int rkvenc_extract_reg_offset_info(struct reg_offset_info *off_inf,
+				   struct mpp_request *req)
+{
+	int max_size = ARRAY_SIZE(off_inf->elem);
+	int cnt = req->size / sizeof(off_inf->elem[0]);
+
+	if ((cnt + off_inf->cnt) > max_size) {
+		rkvenc_err("count %d, total %d, max_size %d\n",
+			   cnt, off_inf->cnt, max_size);
+		return -EINVAL;
+	}
+	if (copy_from_user(&off_inf->elem[off_inf->cnt], req->data, req->size)) {
+		rkvenc_err("copy_from_user failed\n");
+		return -EINVAL;
+	}
+	off_inf->cnt += cnt;
+
+	return 0;
+}
+
+int rkvenc_query_reg_offset_info(struct reg_offset_info *off_inf, u32 index)
+{
+	if (off_inf) {
+		int i;
+
+		for (i = 0; i < off_inf->cnt; i++) {
+			if (off_inf->elem[i].index == index)
+				return off_inf->elem[i].offset;
+		}
+	}
+
+	return 0;
+}
+
+/* ---- Task timeout ---- */
+void rkvenc_task_timeout_work(struct work_struct *work_s)
+{
+	struct rkvenc_mpp_task *task = container_of(to_delayed_work(work_s),
+						   struct rkvenc_mpp_task,
+						   timeout_work);
+	struct rkvenc_dev *mpp;
+	struct rkvenc_session *session = task->session;
+
+	if (!session) {
+		rkvenc_err("task %p, task->session is null.\n", task);
+		return;
+	}
+
+	mpp = rkvenc_get_task_used_device(task, session);
+	if (!mpp) {
+		rkvenc_err("mpp is null\n");
+		return;
+	}
+
+	disable_irq(mpp->irq);
+	if (test_and_set_bit(TASK_STATE_HANDLE, &task->state)) {
+		enable_irq(mpp->irq);
+		return;
+	}
+	rkvenc_err("task %d processing time out!\n", task->task_index);
+	set_bit(TASK_STATE_TIMEOUT, &task->state);
+	enable_irq(mpp->irq);
+
+	kthread_queue_work(&mpp->queue->worker, &mpp->work);
+}
