Binary files aa/.DS_Store and bb/.DS_Store differ
diff -ruN aa/arch/arm64/boot/dts/rockchip/rk3588-base.dtsi bb/arch/arm64/boot/dts/rockchip/rk3588-base.dtsi
--- aa/arch/arm64/boot/dts/rockchip/rk3588-base.dtsi	2026-02-09 18:12:23
+++ bb/arch/arm64/boot/dts/rockchip/rk3588-base.dtsi	2026-02-07 09:40:49
@@ -50,6 +50,8 @@
 		spi2 = &spi2;
 		spi3 = &spi3;
 		spi4 = &spi4;
+		rkvenc0 = &rkvenc0;
+		rkvenc1 = &rkvenc1;
 	};
 
 	cpus {
@@ -1351,6 +1353,92 @@
 		clock-names = "aclk", "iface";
 		power-domains = <&power RK3588_PD_VDPU>;
 		#iommu-cells = <0>;
+	};
+
+	mpp_srv: mpp-srv {
+		compatible = "rockchip,mpp-service";
+		rockchip,taskqueue-count = <12>;
+		rockchip,resetgroup-count = <1>;
+		status = "okay";
+	};
+
+	rkvenc_ccu: rkvenc-ccu {
+		compatible = "rockchip,rkv-encoder-v2-ccu";
+		status = "okay";
+	};
+
+	rkvenc0: rkvenc-core@fdbd0000 {
+		compatible = "rockchip,rkv-encoder-v2-core";
+		reg = <0x0 0xfdbd0000 0x0 0x6000>;
+		interrupts = <GIC_SPI 101 IRQ_TYPE_LEVEL_HIGH 0>;
+		interrupt-names = "irq_rkvenc0";
+		clocks = <&cru ACLK_RKVENC0>, <&cru HCLK_RKVENC0>,
+			 <&cru CLK_RKVENC0_CORE>;
+		clock-names = "aclk_vcodec", "hclk_vcodec", "clk_core";
+		rockchip,normal-rates = <500000000>, <0>, <800000000>;
+		assigned-clocks = <&cru ACLK_RKVENC0>, <&cru CLK_RKVENC0_CORE>;
+		assigned-clock-rates = <500000000>, <800000000>;
+		resets = <&cru SRST_A_RKVENC0>, <&cru SRST_H_RKVENC0>,
+			 <&cru SRST_RKVENC0_CORE>;
+		reset-names = "video_a", "video_h", "video_core";
+		iommus = <&rkvenc0_mmu>;
+		rockchip,srv = <&mpp_srv>;
+		rockchip,ccu = <&rkvenc_ccu>;
+		rockchip,taskqueue-node = <7>;
+		rockchip,resetgroup-node = <0>;
+		rockchip,task-capacity = <8>;
+		power-domains = <&power RK3588_PD_VENC0>;
+		status = "okay";
+	};
+
+	rkvenc0_mmu: iommu@fdbdf000 {
+		compatible = "rockchip,rk3588-iommu", "rockchip,rk3568-iommu";
+		reg = <0x0 0xfdbdf000 0x0 0x40>, <0x0 0xfdbdf040 0x0 0x40>;
+		interrupts = <GIC_SPI 99 IRQ_TYPE_LEVEL_HIGH 0>,
+			     <GIC_SPI 100 IRQ_TYPE_LEVEL_HIGH 0>;
+		clocks = <&cru ACLK_RKVENC0>, <&cru HCLK_RKVENC0>;
+		clock-names = "aclk", "iface";
+		rockchip,disable-mmu-reset;
+		#iommu-cells = <0>;
+		power-domains = <&power RK3588_PD_VENC0>;
+		status = "okay";
+	};
+
+	rkvenc1: rkvenc-core@fdbe0000 {
+		compatible = "rockchip,rkv-encoder-v2-core";
+		reg = <0x0 0xfdbe0000 0x0 0x6000>;
+		interrupts = <GIC_SPI 104 IRQ_TYPE_LEVEL_HIGH 0>;
+		interrupt-names = "irq_rkvenc1";
+		clocks = <&cru ACLK_RKVENC1>, <&cru HCLK_RKVENC1>,
+			 <&cru CLK_RKVENC1_CORE>;
+		clock-names = "aclk_vcodec", "hclk_vcodec", "clk_core";
+		rockchip,normal-rates = <500000000>, <0>, <800000000>;
+		assigned-clocks = <&cru ACLK_RKVENC1>, <&cru CLK_RKVENC1_CORE>;
+		assigned-clock-rates = <500000000>, <800000000>;
+		resets = <&cru SRST_A_RKVENC1>, <&cru SRST_H_RKVENC1>,
+			 <&cru SRST_RKVENC1_CORE>;
+		reset-names = "video_a", "video_h", "video_core";
+		iommus = <&rkvenc1_mmu>;
+		rockchip,srv = <&mpp_srv>;
+		rockchip,ccu = <&rkvenc_ccu>;
+		rockchip,taskqueue-node = <7>;
+		rockchip,resetgroup-node = <0>;
+		rockchip,task-capacity = <8>;
+		power-domains = <&power RK3588_PD_VENC1>;
+		status = "okay";
+	};
+
+	rkvenc1_mmu: iommu@fdbef000 {
+		compatible = "rockchip,rk3588-iommu", "rockchip,rk3568-iommu";
+		reg = <0x0 0xfdbef000 0x0 0x40>, <0x0 0xfdbef040 0x0 0x40>;
+		interrupts = <GIC_SPI 102 IRQ_TYPE_LEVEL_HIGH 0>,
+			     <GIC_SPI 103 IRQ_TYPE_LEVEL_HIGH 0>;
+		clocks = <&cru ACLK_RKVENC1>, <&cru HCLK_RKVENC1>;
+		clock-names = "aclk", "iface";
+		rockchip,disable-mmu-reset;
+		#iommu-cells = <0>;
+		power-domains = <&power RK3588_PD_VENC1>;
+		status = "okay";
 	};
 
 	av1d: video-codec@fdc70000 {
Binary files aa/drivers/.DS_Store and bb/drivers/.DS_Store differ
diff -ruN aa/drivers/iommu/rockchip-iommu.c bb/drivers/iommu/rockchip-iommu.c
--- aa/drivers/iommu/rockchip-iommu.c	2026-02-08 22:38:08
+++ bb/drivers/iommu/rockchip-iommu.c	2026-02-07 23:38:39
@@ -27,6 +27,7 @@
 #include <linux/spinlock.h>
 #include <linux/string_choices.h>
 
+#include "dma-iommu.h"
 #include "iommu-pages.h"
 
 /** MMU register offsets */
@@ -1182,6 +1183,7 @@
 	.probe_device = rk_iommu_probe_device,
 	.release_device = rk_iommu_release_device,
 	.device_group = generic_single_device_group,
+	.get_resv_regions = iommu_dma_get_resv_regions,
 	.of_xlate = rk_iommu_of_xlate,
 	.default_domain_ops = &(const struct iommu_domain_ops) {
 		.attach_dev	= rk_iommu_attach_device,
Binary files aa/drivers/media/.DS_Store and bb/drivers/media/.DS_Store differ
Binary files aa/drivers/media/platform/.DS_Store and bb/drivers/media/platform/.DS_Store differ
Binary files aa/drivers/media/platform/rockchip/.DS_Store and bb/drivers/media/platform/rockchip/.DS_Store differ
diff -ruN aa/drivers/media/platform/rockchip/Kconfig bb/drivers/media/platform/rockchip/Kconfig
--- aa/drivers/media/platform/rockchip/Kconfig	2026-02-08 22:43:45
+++ bb/drivers/media/platform/rockchip/Kconfig	2026-02-07 01:43:23
@@ -6,3 +6,4 @@
 source "drivers/media/platform/rockchip/rkcif/Kconfig"
 source "drivers/media/platform/rockchip/rkisp1/Kconfig"
 source "drivers/media/platform/rockchip/rkvdec/Kconfig"
+source "drivers/media/platform/rockchip/rkvenc/Kconfig"
diff -ruN aa/drivers/media/platform/rockchip/Makefile bb/drivers/media/platform/rockchip/Makefile
--- aa/drivers/media/platform/rockchip/Makefile	2026-02-08 22:43:57
+++ bb/drivers/media/platform/rockchip/Makefile	2026-02-07 01:43:24
@@ -3,3 +3,4 @@
 obj-y += rkcif/
 obj-y += rkisp1/
 obj-y += rkvdec/
+obj-y += rkvenc/
diff -ruN aa/drivers/media/platform/rockchip/rkvenc/Kconfig bb/drivers/media/platform/rockchip/rkvenc/Kconfig
--- aa/drivers/media/platform/rockchip/rkvenc/Kconfig	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/Kconfig	2026-02-07 01:32:53
@@ -0,0 +1,17 @@
+config VIDEO_ROCKCHIP_RKVENC
+	tristate "Rockchip RKVENC (VEPU580) encoder driver"
+	depends on ARCH_ROCKCHIP || COMPILE_TEST
+	depends on VIDEO_DEV
+	depends on IOMMU_API
+	select VIDEOBUF2_DMA_CONTIG
+	help
+	  Support for the Rockchip VEPU580 hardware video encoder found on
+	  RK3588 SoCs. This encoder supports H.264, H.265/HEVC, and JPEG
+	  encoding at up to 8K resolution.
+
+	  This driver provides the /dev/mpp_service character device interface
+	  used by the Rockchip MPP userspace library for hardware-accelerated
+	  video encoding.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called rkvenc.
diff -ruN aa/drivers/media/platform/rockchip/rkvenc/Makefile bb/drivers/media/platform/rockchip/rkvenc/Makefile
--- aa/drivers/media/platform/rockchip/rkvenc/Makefile	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/Makefile	2026-02-07 01:32:58
@@ -0,0 +1,2 @@
+obj-$(CONFIG_VIDEO_ROCKCHIP_RKVENC) += rkvenc.o
+rkvenc-objs := rkvenc_drv.o rkvenc_hw.o rkvenc_service.o rkvenc_task.o rkvenc_iommu.o
diff -ruN aa/drivers/media/platform/rockchip/rkvenc/rkvenc_drv.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_drv.c
--- aa/drivers/media/platform/rockchip/rkvenc/rkvenc_drv.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_drv.c	2026-02-09 09:17:29
@@ -0,0 +1,481 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - Platform driver
+ * Ported from Rockchip BSP mpp_rkvenc2.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ * Copyright (C) 2026 Ross Cawston
+ */
+
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/iommu.h>
+#include <linux/iopoll.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/pm_runtime.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+
+#include "rkvenc_hw.h"
+
+/* ---- PM ops ---- */
+static int __maybe_unused rkvenc_runtime_suspend(struct device *dev)
+{
+	return 0;
+}
+
+static int __maybe_unused rkvenc_runtime_resume(struct device *dev)
+{
+	return 0;
+}
+
+static const struct dev_pm_ops rkvenc_pm_ops = {
+	SET_RUNTIME_PM_OPS(rkvenc_runtime_suspend, rkvenc_runtime_resume, NULL)
+	SET_SYSTEM_SLEEP_PM_OPS(pm_runtime_force_suspend, pm_runtime_force_resume)
+};
+
+/* ---- CCU probe ---- */
+static int rkvenc_ccu_probe(struct platform_device *pdev)
+{
+	struct rkvenc_ccu *ccu;
+	struct device *dev = &pdev->dev;
+
+	ccu = devm_kzalloc(dev, sizeof(*ccu), GFP_KERNEL);
+	if (!ccu)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ccu);
+
+	mutex_init(&ccu->lock);
+	INIT_LIST_HEAD(&ccu->core_list);
+	spin_lock_init(&ccu->lock_dchs);
+
+	dev_info(dev, "rkvenc ccu probe success\n");
+	return 0;
+}
+
+/* ---- Attach core to CCU ---- */
+static int rkvenc_attach_ccu(struct device *dev, struct rkvenc_dev *enc)
+{
+	struct device_node *np;
+	struct platform_device *pdev;
+	struct rkvenc_ccu *ccu;
+
+	np = of_parse_phandle(dev->of_node, "rockchip,ccu", 0);
+	if (!np || !of_device_is_available(np))
+		return -ENODEV;
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev)
+		return -ENODEV;
+
+	ccu = platform_get_drvdata(pdev);
+	if (!ccu)
+		return -EPROBE_DEFER;
+
+	INIT_LIST_HEAD(&enc->core_link);
+	mutex_lock(&ccu->lock);
+	ccu->core_num++;
+	list_add_tail(&enc->core_link, &ccu->core_list);
+	mutex_unlock(&ccu->lock);
+
+	/* First core becomes the main core */
+	if (!ccu->main_core) {
+		ccu->main_core = enc;
+	} else {
+		struct rkvenc_iommu_info *main_iommu =
+			ccu->main_core ? ccu->main_core->iommu_info : NULL;
+		struct rkvenc_iommu_info *sec_iommu = enc->iommu_info;
+		struct iommu_domain *shared;
+		int ret;
+
+		shared = main_iommu ? main_iommu->domain : NULL;
+		if (!shared || !sec_iommu || !sec_iommu->group) {
+			dev_err(dev, "missing IOMMU info for shared domain\n");
+			return -ENODEV;
+		}
+
+		/*
+		 * Share the main core's IOMMU domain with the secondary core.
+		 * This programs the secondary IOMMU hardware to use the same
+		 * page tables as the main core (matching BSP mpp_rkvenc2.c).
+		 *
+		 * All DMA buffer imports go through the main core's device
+		 * (via session->mpp which is always the main core), so mappings
+		 * land in main_core->default_domain.  We do NOT need to modify
+		 * iommu_get_dma_domain() because no DMA API calls are ever
+		 * made through the secondary core's device.
+		 */
+		sec_iommu->domain = shared;
+		sec_iommu->rw_sem = main_iommu->rw_sem;
+
+		ret = iommu_attach_group(shared, sec_iommu->group);
+		if (ret) {
+			dev_err(dev, "attach shared IOMMU domain failed: %d\n", ret);
+			sec_iommu->domain = NULL;
+			sec_iommu->rw_sem = &sec_iommu->rw_sem_self;
+			return ret;
+		}
+	}
+	enc->ccu = ccu;
+
+	dev_info(dev, "attach ccu as core %d%s\n", enc->core_id,
+		 enc == ccu->main_core ? " [main]" :
+		 (enc->queue && enc->core_id >= 0 &&
+		  enc->core_id < MPP_MAX_CORE_NUM &&
+		  enc->queue->cores[enc->core_id] == enc ?
+		  " [secondary, active]" : " [secondary, inactive]"));
+	return 0;
+}
+
+/* ---- Attach core to service ---- */
+static int rkvenc_attach_service(struct rkvenc_dev *enc)
+{
+	struct device_node *np;
+	struct platform_device *pdev;
+	struct rkvenc_service *srv;
+	struct rkvenc_taskqueue *queue;
+	u32 taskqueue_node = 0;
+	u32 resetgroup_node = 0;
+
+	np = of_parse_phandle(enc->dev->of_node, "rockchip,srv", 0);
+	if (!np || !of_device_is_available(np))
+		return -ENODEV;
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev)
+		return -ENODEV;
+
+	srv = platform_get_drvdata(pdev);
+	if (!srv)
+		return -EPROBE_DEFER;
+
+	enc->srv = srv;
+
+	of_property_read_u32(enc->dev->of_node, "rockchip,taskqueue-node", &taskqueue_node);
+	of_property_read_u32(enc->dev->of_node, "rockchip,resetgroup-node", &resetgroup_node);
+
+	/* Attach to task queue */
+	queue = srv->task_queues[MPP_DEVICE_RKVENC];
+	if (!queue) {
+		dev_err(enc->dev, "no task queue for RKVENC\n");
+		return -ENODEV;
+	}
+	enc->queue = queue;
+
+	/* Register core in the task queue */
+	if (enc->core_id >= 0 && enc->core_id < MPP_MAX_CORE_NUM) {
+		queue->cores[enc->core_id] = enc;
+		queue->core_count++;
+		if (enc->core_id > queue->core_id_max)
+			queue->core_id_max = enc->core_id;
+		set_bit(enc->core_id, &queue->core_idle);
+	}
+
+	/* Attach to reset group */
+	if (resetgroup_node < srv->reset_group_cnt)
+		enc->reset_group = srv->reset_groups[resetgroup_node];
+
+	/* Init kthread work */
+	kthread_init_work(&enc->work, rkvenc_task_worker_default);
+
+	return 0;
+}
+
+/* ---- SRAM RCB allocation ---- */
+static int rkvenc2_alloc_rcbbuf(struct platform_device *pdev, struct rkvenc_dev *enc)
+{
+	int ret;
+	u32 vals[2];
+	dma_addr_t iova;
+	u32 sram_used, sram_size;
+	struct device_node *sram_np;
+	struct resource sram_res;
+	resource_size_t sram_start, sram_end;
+	struct iommu_domain *domain;
+	struct device *dev = &pdev->dev;
+
+	ret = device_property_read_u32_array(dev, "rockchip,rcb-iova", vals, 2);
+	if (ret)
+		return ret;
+
+	iova = PAGE_ALIGN(vals[0]);
+	sram_used = PAGE_ALIGN(vals[1]);
+	if (!sram_used) {
+		dev_err(dev, "sram rcb invalid\n");
+		return -EINVAL;
+	}
+
+	sram_np = of_parse_phandle(dev->of_node, "rockchip,sram", 0);
+	if (!sram_np) {
+		dev_err(dev, "could not find phandle sram\n");
+		return -ENODEV;
+	}
+
+	ret = of_address_to_resource(sram_np, 0, &sram_res);
+	of_node_put(sram_np);
+	if (ret) {
+		dev_err(dev, "find sram res error\n");
+		return ret;
+	}
+
+	sram_start = round_up(sram_res.start, PAGE_SIZE);
+	sram_end = round_down(sram_res.start + resource_size(&sram_res), PAGE_SIZE);
+	if (sram_end <= sram_start) {
+		dev_err(dev, "no available sram\n");
+		return -ENOMEM;
+	}
+	sram_size = sram_end - sram_start;
+	sram_size = sram_used < sram_size ? sram_used : sram_size;
+
+	if (!enc->iommu_info || !enc->iommu_info->domain)
+		return -ENODEV;
+
+	domain = enc->iommu_info->domain;
+	ret = iommu_map(domain, iova, sram_start, sram_size, IOMMU_READ | IOMMU_WRITE,
+			GFP_KERNEL);
+	if (ret) {
+		dev_err(dev, "sram iommu_map error\n");
+		return ret;
+	}
+
+	if (sram_size < sram_used) {
+		struct page *page;
+		size_t page_size = PAGE_ALIGN(sram_used - sram_size);
+
+		page = alloc_pages(GFP_KERNEL | __GFP_ZERO, get_order(page_size));
+		if (!page) {
+			dev_err(dev, "unable to allocate pages\n");
+			iommu_unmap(domain, iova, sram_size);
+			return -ENOMEM;
+		}
+		ret = iommu_map(domain, iova + sram_size, page_to_phys(page),
+				page_size, IOMMU_READ | IOMMU_WRITE, GFP_KERNEL);
+		if (ret) {
+			dev_err(dev, "page iommu_map error\n");
+			__free_pages(page, get_order(page_size));
+			iommu_unmap(domain, iova, sram_size);
+			return ret;
+		}
+		enc->rcb_page = page;
+	}
+
+	enc->sram_size = sram_size;
+	enc->sram_used = sram_used;
+	enc->sram_iova = iova;
+	enc->sram_enabled = -1;
+	dev_info(dev, "sram iova %pad size %u used %u\n",
+		 &enc->sram_iova, enc->sram_size, enc->sram_used);
+
+	return 0;
+}
+
+/* ---- Core probe ---- */
+static int rkvenc_core_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct device *dev = &pdev->dev;
+	struct rkvenc_dev *enc;
+
+	enc = devm_kzalloc(dev, sizeof(*enc), GFP_KERNEL);
+	if (!enc)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, enc);
+
+	/* Get core ID from alias, falling back to DTS property */
+	enc->core_id = of_alias_get_id(dev->of_node, "rkvenc");
+	if (enc->core_id < 0) {
+		u32 core_id = 0;
+
+		of_property_read_u32(dev->of_node, "rockchip,core-id", &core_id);
+		enc->core_id = core_id;
+	}
+
+	/* HW probe: clocks, resets, IOMMU, register space */
+	ret = rkvenc_hw_probe(enc, pdev);
+	if (ret)
+		return ret;
+
+	/* Attach to service */
+	ret = rkvenc_attach_service(enc);
+	if (ret) {
+		dev_err_probe(dev, ret, "failed to attach service\n");
+		goto err_hw;
+	}
+
+	/* Attach core to CCU */
+	ret = rkvenc_attach_ccu(dev, enc);
+	if (ret) {
+		dev_err_probe(dev, ret, "attach ccu failed\n");
+		goto err_hw;
+	}
+
+	/*
+	 * Try SRAM allocation (optional, non-fatal).
+	 * For secondary cores sharing a CCU domain, the main core has
+	 * already mapped the SRAM into the shared domain.  Just inherit
+	 * the SRAM info so rkvenc2_set_rcbbuf() works on both cores.
+	 */
+	if (enc->ccu && enc != enc->ccu->main_core &&
+	    enc->ccu->main_core->sram_iova) {
+		enc->sram_iova = enc->ccu->main_core->sram_iova;
+		enc->sram_size = enc->ccu->main_core->sram_size;
+		enc->sram_used = enc->ccu->main_core->sram_used;
+		enc->sram_enabled = -1;
+		dev_info(dev, "sram inherited from main core: iova %pad size %u used %u\n",
+			 &enc->sram_iova, enc->sram_size, enc->sram_used);
+	} else {
+		rkvenc2_alloc_rcbbuf(pdev, enc);
+	}
+
+	/* Register IRQ */
+	ret = devm_request_irq(dev, enc->irq, rkvenc_hw_irq,
+			       IRQF_SHARED, dev_name(dev), enc);
+	if (ret) {
+		dev_err(dev, "register interrupt failed: %d\n", ret);
+		goto err_hw;
+	}
+
+	/* If this is the main core, register with the service */
+	if (enc->ccu && enc == enc->ccu->main_core) {
+		enc->srv->sub_devices[MPP_DEVICE_RKVENC] = enc;
+		set_bit(MPP_DEVICE_RKVENC, &enc->srv->hw_support);
+	}
+
+	dev_info(dev, "rkvenc core %d probe success (hw_id: %08x)\n",
+		 enc->core_id, enc->hw_info->hw.hw_id);
+	return 0;
+
+err_hw:
+	rkvenc_hw_remove(enc);
+	return ret;
+}
+
+/* ---- Core remove ---- */
+static int rkvenc_core_remove(struct platform_device *pdev)
+{
+	struct rkvenc_dev *enc = platform_get_drvdata(pdev);
+
+	if (!enc)
+		return 0;
+
+	if (enc->ccu) {
+		mutex_lock(&enc->ccu->lock);
+		list_del_init(&enc->core_link);
+		enc->ccu->core_num--;
+		mutex_unlock(&enc->ccu->lock);
+	}
+
+	/* Free SRAM — only the core that mapped it (main or standalone) */
+	if (enc->sram_iova && enc->iommu_info && enc->iommu_info->domain &&
+	    (!enc->ccu || enc == enc->ccu->main_core)) {
+		struct iommu_domain *domain = enc->iommu_info->domain;
+
+		if (enc->rcb_page) {
+			size_t page_size = PAGE_ALIGN(enc->sram_used - enc->sram_size);
+
+			iommu_unmap(domain, enc->sram_iova + enc->sram_size, page_size);
+			__free_pages(enc->rcb_page, get_order(page_size));
+		}
+		iommu_unmap(domain, enc->sram_iova, enc->sram_size);
+	}
+
+	rkvenc_hw_remove(enc);
+	dev_info(&pdev->dev, "rkvenc core removed\n");
+	return 0;
+}
+
+/* ---- Top-level probe dispatcher ---- */
+static int rkvenc_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	dev_info(dev, "probing start\n");
+
+	if (strstr(np->name, "ccu"))
+		return rkvenc_ccu_probe(pdev);
+	else if (strstr(np->name, "core"))
+		return rkvenc_core_probe(pdev);
+	else if (of_device_is_compatible(np, "rockchip,mpp-service"))
+		return rkvenc_service_probe(pdev);
+
+	dev_err(dev, "unknown node type: %s\n", np->name);
+	return -ENODEV;
+}
+
+static void rkvenc_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+
+	if (strstr(np->name, "ccu")) {
+		dev_info(dev, "remove ccu\n");
+	} else if (strstr(np->name, "core")) {
+		rkvenc_core_remove(pdev);
+	} else if (of_device_is_compatible(np, "rockchip,mpp-service")) {
+		rkvenc_service_remove(pdev);
+	}
+}
+
+static void rkvenc_shutdown(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+	struct rkvenc_dev *enc;
+	int ret, val;
+
+	if (strstr(np->name, "ccu"))
+		return;
+	if (of_device_is_compatible(np, "rockchip,mpp-service"))
+		return;
+
+	enc = platform_get_drvdata(pdev);
+	if (!enc || !enc->srv)
+		return;
+
+	dev_info(dev, "shutdown device\n");
+	atomic_inc(&enc->srv->shutdown_request);
+
+	ret = readx_poll_timeout(atomic_read, &enc->task_count,
+				 val, val == 0, 20000, 200000);
+	if (ret == -ETIMEDOUT)
+		dev_err(dev, "wait total %d running time out\n",
+			atomic_read(&enc->task_count));
+	else
+		dev_info(dev, "shutdown success\n");
+}
+
+/* ---- OF match table ---- */
+static const struct of_device_id rkvenc_dt_match[] = {
+	{ .compatible = "rockchip,mpp-service" },
+	{ .compatible = "rockchip,rkv-encoder-v2-ccu" },
+	{ .compatible = "rockchip,rkv-encoder-v2-core" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, rkvenc_dt_match);
+
+static struct platform_driver rkvenc_driver = {
+	.probe = rkvenc_probe,
+	.remove = rkvenc_remove,
+	.shutdown = rkvenc_shutdown,
+	.driver = {
+		.name = MPP_DRIVER_NAME,
+		.of_match_table = rkvenc_dt_match,
+		.pm = &rkvenc_pm_ops,
+	},
+};
+
+module_platform_driver(rkvenc_driver);
+
+MODULE_DESCRIPTION("Rockchip VEPU580 (RKVENC v2) H.265/H.264/JPEG encoder driver");
+MODULE_LICENSE("Dual MIT/GPL");
+MODULE_AUTHOR("Rockchip Electronics Co., Ltd.");
+MODULE_IMPORT_NS("DMA_BUF");
diff -ruN aa/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.c
--- aa/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.c	2026-02-09 09:19:14
@@ -0,0 +1,888 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - Hardware operations
+ * Ported from Rockchip BSP mpp_rkvenc2.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ */
+
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/iopoll.h>
+#include <linux/ioport.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/pm_runtime.h>
+#include <linux/reset.h>
+#include <linux/slab.h>
+
+#include "rkvenc_hw.h"
+
+unsigned int rkvenc_debug;
+module_param_named(debug, rkvenc_debug, uint, 0644);
+MODULE_PARM_DESC(debug, "Debug level bitmask");
+
+/* ---- FD translation tables for VEPU580 ---- */
+static const u16 trans_tbl_h264e_v2[] = {
+	0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
+	10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
+	20, 21, 22, 23,
+};
+
+static const u16 trans_tbl_h265e_v2[] = {
+	0, 1, 2, 3, 4, 5, 6, 7, 8, 9,
+	10, 11, 12, 13, 14, 15, 16, 17, 18, 19,
+	20, 21, 22, 23,
+};
+
+static const u16 trans_tbl_jpege_v2[] = {
+	5, 6, 7, 8, 9, 10, 11, 12, 13, 14,
+	15, 16,
+};
+
+static const u16 trans_tbl_h264e_v2_osd[] = {
+	20, 21, 22, 23, 24, 25, 26, 27,
+};
+
+static const u16 trans_tbl_h265e_v2_osd[] = {
+	20, 21, 22, 23, 24, 25, 26, 27,
+};
+
+static const u16 trans_tbl_jpege_v2_osd[] = {
+	3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
+	13, 14, 15, 16, 17, 18, 19, 20, 21, 22,
+	23, 24, 25, 26, 27, 28, 29, 30, 31, 32,
+};
+
+const struct rkvenc_trans_info trans_rkvenc_v2[] = {
+	[RKVENC_FMT_H264E] = {
+		.count = ARRAY_SIZE(trans_tbl_h264e_v2),
+		.table = trans_tbl_h264e_v2,
+	},
+	[RKVENC_FMT_H265E] = {
+		.count = ARRAY_SIZE(trans_tbl_h265e_v2),
+		.table = trans_tbl_h265e_v2,
+	},
+	[RKVENC_FMT_JPEGE] = {
+		.count = ARRAY_SIZE(trans_tbl_jpege_v2),
+		.table = trans_tbl_jpege_v2,
+	},
+	[RKVENC_FMT_H264E_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_h264e_v2_osd),
+		.table = trans_tbl_h264e_v2_osd,
+	},
+	[RKVENC_FMT_H265E_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_h265e_v2_osd),
+		.table = trans_tbl_h265e_v2_osd,
+	},
+	[RKVENC_FMT_JPEGE_OSD] = {
+		.count = ARRAY_SIZE(trans_tbl_jpege_v2_osd),
+		.table = trans_tbl_jpege_v2_osd,
+	},
+};
+
+/* ---- VEPU580 HW info ---- */
+struct rkvenc_hw_info rkvenc_v2_hw_info = {
+	.hw = {
+		.reg_num = 254,
+		.reg_id = 0,
+		.reg_en = 4,
+		.reg_start = 160,
+		.reg_end = 253,
+	},
+	.reg_class = RKVENC_CLASS_BUTT,
+	.reg_msg = {
+		[RKVENC_CLASS_BASE] = { .base_s = 0x0000, .base_e = 0x0058 },
+		[RKVENC_CLASS_PIC]  = { .base_s = 0x0280, .base_e = 0x03f4 },
+		[RKVENC_CLASS_RC]   = { .base_s = 0x1000, .base_e = 0x10e0 },
+		[RKVENC_CLASS_PAR]  = { .base_s = 0x1700, .base_e = 0x1cd4 },
+		[RKVENC_CLASS_SQI]  = { .base_s = 0x2000, .base_e = 0x21e4 },
+		[RKVENC_CLASS_SCL]  = { .base_s = 0x2200, .base_e = 0x2c98 },
+		[RKVENC_CLASS_OSD]  = { .base_s = 0x3000, .base_e = 0x347c },
+		[RKVENC_CLASS_ST]   = { .base_s = 0x4000, .base_e = 0x42cc },
+		[RKVENC_CLASS_DBG]  = { .base_s = 0x5000, .base_e = 0x5354 },
+	},
+	.fd_class = RKVENC_CLASS_FD_BUTT,
+	.fd_reg = {
+		[RKVENC_CLASS_FD_BASE] = {
+			.class = RKVENC_CLASS_PIC,
+			.base_fmt = RKVENC_FMT_BASE,
+		},
+		[RKVENC_CLASS_FD_OSD] = {
+			.class = RKVENC_CLASS_OSD,
+			.base_fmt = RKVENC_FMT_OSD_BASE,
+		},
+	},
+	.fmt_reg = {
+		.class = RKVENC_CLASS_PIC,
+		.base = 0x0300,
+		.bitpos = 0,
+		.bitlen = 1,
+	},
+	.enc_start_base = 0x0010,
+	.enc_clr_base = 0x0014,
+	.int_en_base = 0x0020,
+	.int_mask_base = 0x0024,
+	.int_clr_base = 0x0028,
+	.int_sta_base = 0x002c,
+	.enc_wdg_base = 0x0038,
+	.err_mask = 0x03f0,
+	.enc_rsl = 0x0310,
+	.dcsh_class_ofst = 33,
+	.vepu_type = RKVENC_VEPU_580,
+};
+
+/* ---- Timeout thresholds by resolution ---- */
+static const u32 rkvenc2_timeout_thd_by_rsl[][2] = {
+	{  3840 * 2160, 200 },
+	{  7680 * 4320, 500 },
+	{ 0xffffffff,   800 },
+};
+
+/* ---- Class register helpers ---- */
+static int rkvenc_get_class_msg(struct rkvenc_task *task,
+				u32 addr, struct mpp_request *msg)
+{
+	int i;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+
+	if (!msg)
+		return -EINVAL;
+
+	memset(msg, 0, sizeof(*msg));
+	for (i = 0; i < hw->reg_class; i++) {
+		u32 base_s = hw->reg_msg[i].base_s;
+		u32 base_e = hw->reg_msg[i].base_e;
+
+		if (addr >= base_s && addr < base_e) {
+			msg->offset = base_s;
+			msg->size = task->reg[i].size;
+			msg->data = task->reg[i].data;
+			return 0;
+		}
+	}
+
+	return -EINVAL;
+}
+
+static u32 *rkvenc_get_class_reg(struct rkvenc_task *task, u32 addr)
+{
+	int i;
+	u8 *reg = NULL;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+
+	for (i = 0; i < hw->reg_class; i++) {
+		u32 base_s = hw->reg_msg[i].base_s;
+		u32 base_e = hw->reg_msg[i].base_e;
+
+		if (addr >= base_s && addr < base_e) {
+			reg = (u8 *)task->reg[i].data + (addr - base_s);
+			break;
+		}
+	}
+
+	return (u32 *)reg;
+}
+
+/* ---- DCHS (Dual-Core Handshake) ---- */
+static void rkvenc2_patch_dchs(struct rkvenc_dev *enc, struct rkvenc_task *task)
+{
+	struct rkvenc_ccu *ccu;
+	union rkvenc2_dual_core_handshake_id *dchs;
+	union rkvenc2_dual_core_handshake_id *task_dchs = &task->dchs_id;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	int core_num;
+	int core_id = enc->core_id;
+	unsigned long flags;
+	int i;
+
+	if (!enc->ccu)
+		return;
+
+	if (core_id >= RKVENC_MAX_CORE_NUM) {
+		dev_err(enc->dev, "invalid core id %d max %d\n",
+			core_id, RKVENC_MAX_CORE_NUM);
+		return;
+	}
+
+	ccu = enc->ccu;
+	dchs = ccu->dchs;
+	core_num = ccu->core_num;
+
+	spin_lock_irqsave(&ccu->lock_dchs, flags);
+
+	if (dchs[core_id].working) {
+		spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+		rkvenc_err("can not config when core %d is still working\n", core_id);
+		return;
+	}
+
+	/* Find free TX/RX IDs */
+	{
+		unsigned long id_valid = (unsigned long)-1;
+		int txid_map = -1;
+		int rxid_map = -1;
+
+		for (i = 0; i < core_num; i++) {
+			if (!dchs[i].working)
+				continue;
+			clear_bit(dchs[i].txid_map, &id_valid);
+			clear_bit(dchs[i].rxid_map, &id_valid);
+		}
+
+		if (task_dchs->rxe) {
+			for (i = 0; i < core_num; i++) {
+				if (i == core_id || !dchs[i].working)
+					continue;
+				if (task_dchs->session_id != dchs[i].session_id)
+					continue;
+				if (task_dchs->rxid_orig != dchs[i].txid_orig)
+					continue;
+				rxid_map = dchs[i].txid_map;
+				break;
+			}
+		}
+
+		txid_map = find_first_bit(&id_valid, RKVENC_MAX_DCHS_ID);
+		if (txid_map == RKVENC_MAX_DCHS_ID) {
+			spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+			rkvenc_err("failed to find a txid\n");
+			return;
+		}
+
+		clear_bit(txid_map, &id_valid);
+		task_dchs->txid_map = txid_map;
+
+		if (rxid_map < 0) {
+			rxid_map = find_first_bit(&id_valid, RKVENC_MAX_DCHS_ID);
+			if (rxid_map == RKVENC_MAX_DCHS_ID) {
+				spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+				rkvenc_err("failed to find a rxid\n");
+				return;
+			}
+			task_dchs->rxe_map = 0;
+		}
+
+		task_dchs->rxid_map = rxid_map;
+	}
+
+	task_dchs->txid = task_dchs->txid_map;
+	task_dchs->rxid = task_dchs->rxid_map;
+	task_dchs->rxe = task_dchs->rxe_map;
+
+	dchs[core_id].val[0] = task_dchs->val[0];
+	dchs[core_id].val[1] = task_dchs->val[1];
+	task->reg[RKVENC_CLASS_PIC].data[hw->dcsh_class_ofst] = task_dchs->val[0];
+
+	dchs[core_id].working = 1;
+
+	spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+}
+
+static void rkvenc2_update_dchs(struct rkvenc_dev *enc,
+				struct rkvenc_task *task)
+{
+	struct rkvenc_ccu *ccu = enc->ccu;
+	int core_id = enc->core_id;
+	unsigned long flags;
+
+	if (!ccu)
+		return;
+
+	if (core_id >= RKVENC_MAX_CORE_NUM) {
+		dev_err(enc->dev, "invalid core id %d max %d\n",
+			core_id, RKVENC_MAX_CORE_NUM);
+		return;
+	}
+
+	spin_lock_irqsave(&ccu->lock_dchs, flags);
+	ccu->dchs[core_id].val[0] = 0;
+	ccu->dchs[core_id].val[1] = 0;
+	spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+}
+
+/* ---- Timeout threshold calculation ---- */
+static void rkvenc2_calc_timeout_thd(struct rkvenc_dev *enc)
+{
+	const struct rkvenc_hw_info *hw = enc->hw_info;
+	union rkvenc2_frame_resolution frm_rsl;
+	u32 timeout_ms = 0;
+	u32 timeout_thd = 0;
+	u32 timeout_thd_cnt;
+	u32 i;
+
+	timeout_thd = rkvenc_read(enc, RKVENC_WDG) & 0xff000000;
+	frm_rsl.val = rkvenc_read(enc, hw->enc_rsl);
+	frm_rsl.val = (frm_rsl.pic_wd8 + 1) * (frm_rsl.pic_hd8 + 1) * 64;
+
+	timeout_thd_cnt = ARRAY_SIZE(rkvenc2_timeout_thd_by_rsl);
+	for (i = 0; i < timeout_thd_cnt; i++) {
+		if (frm_rsl.val <= rkvenc2_timeout_thd_by_rsl[i][0]) {
+			timeout_ms = rkvenc2_timeout_thd_by_rsl[i][1];
+			break;
+		}
+	}
+
+	/* Use x1024 core clk cycles for VEPU580 */
+	if (enc->core_clk_info.clk)
+		timeout_thd |= timeout_ms * clk_get_rate(enc->core_clk_info.clk) / 1024000;
+
+	rkvenc_write(enc, RKVENC_WDG, timeout_thd);
+}
+
+/* ---- Slice reading ---- */
+static void rkvenc2_read_slice_len(struct rkvenc_dev *mpp,
+				   struct rkvenc_task *task,
+				   u32 *irq_status)
+{
+	const struct rkvenc_hw_info *hw = mpp->hw_info;
+	u32 sli_num = rkvenc_read_relaxed(mpp, RKVENC2_REG_SLICE_NUM_BASE) & 0x3f;
+	u32 new_irq_status = rkvenc_read(mpp, hw->int_sta_base);
+	union rkvenc2_slice_len_info slice_info;
+	u32 i;
+	u32 last = 0;
+	u32 split = task->task_split;
+
+	if ((new_irq_status != *irq_status) && (new_irq_status & INT_STA_ENC_DONE_STA)) {
+		*irq_status |= new_irq_status;
+		sli_num = rkvenc_read_relaxed(mpp, RKVENC2_REG_SLICE_NUM_BASE) & 0x3f;
+		rkvenc_write(mpp, hw->int_clr_base, new_irq_status);
+	}
+
+	last = *irq_status & INT_STA_ENC_DONE_STA;
+
+	for (i = 0; i < sli_num; i++) {
+		slice_info.val = rkvenc_read_relaxed(mpp, RKVENC2_REG_SLICE_LEN_BASE);
+		last |= slice_info.last;
+		if (last && i == sli_num - 1) {
+			task->last_slice_found = 1;
+			break;
+		}
+
+		if (split) {
+			kfifo_in(&task->slice_info, &slice_info, 1);
+			task->slice_wr_cnt++;
+		}
+	}
+
+	if (split) {
+		if (last && !task->last_slice_found) {
+			slice_info.last = 1;
+			slice_info.slice_len = 0;
+			kfifo_in(&task->slice_info, &slice_info, 1);
+		}
+	}
+}
+
+/* ---- Bitstream overflow handling ---- */
+static void rkvenc2_bs_overflow_handle(struct rkvenc_dev *mpp)
+{
+	struct rkvenc_mpp_task *mpp_task = mpp->cur_task;
+	u32 bs_rd, bs_wr, bs_top, bs_bot;
+
+	bs_rd = rkvenc_read(mpp, RKVENC580_REG_ADR_BSBR);
+	bs_wr = rkvenc_read(mpp, RKVENC2_REG_ST_BSB);
+	bs_top = rkvenc_read(mpp, RKVENC2_REG_ADR_BSBT);
+	bs_bot = rkvenc_read(mpp, RKVENC2_REG_ADR_BSBB);
+
+	bs_wr += 128;
+	if (bs_wr >= bs_top)
+		bs_wr = bs_bot;
+	rkvenc_write(mpp, RKVENC2_REG_ADR_BSBS, bs_wr);
+
+	if (mpp_task)
+		dev_err(mpp->dev, "task %d bitstream overflow [top=%#08x bot=%#08x wr=%#08x rd=%#08x]\n",
+			mpp_task->task_index, bs_top, bs_bot, bs_wr, bs_rd);
+}
+
+/* ---- IRQ handler ---- */
+irqreturn_t rkvenc_hw_irq(int irq, void *param)
+{
+	struct rkvenc_dev *enc = param;
+	const struct rkvenc_hw_info *hw = enc->hw_info;
+	struct rkvenc_mpp_task *mpp_task = NULL;
+	struct rkvenc_task *task = NULL;
+	u32 irq_status;
+	int ret = IRQ_NONE;
+
+	irq_status = rkvenc_read(enc, hw->int_sta_base);
+
+	if (!irq_status)
+		return ret;
+
+	/* clear int first */
+	rkvenc_write(enc, hw->int_clr_base, irq_status);
+
+	/* prevent watch dog irq storm */
+	if (irq_status & INT_STA_WDG_STA)
+		rkvenc_write(enc, hw->int_mask_base, INT_STA_WDG_STA);
+
+	if (enc->cur_task) {
+		mpp_task = enc->cur_task;
+		task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+	}
+
+	/* 1. slice split read */
+	if (task && task->task_split &&
+	    (irq_status & (INT_STA_SLC_DONE_STA | INT_STA_ENC_DONE_STA))) {
+		rkvenc2_read_slice_len(enc, task, &irq_status);
+		wake_up(&mpp_task->wait);
+	}
+
+	/* 2. process slice irq */
+	if (irq_status & INT_STA_SLC_DONE_STA)
+		ret = IRQ_HANDLED;
+
+	/* 3. process bitstream overflow */
+	if (irq_status & INT_STA_BSF_OFLW_STA) {
+		rkvenc2_bs_overflow_handle(enc);
+		enc->bs_overflow = 1;
+		ret = IRQ_HANDLED;
+	}
+
+	/* 4. process frame done irq */
+	if (irq_status & INT_STA_ENC_DONE_STA) {
+		enc->irq_status = irq_status;
+
+		if (enc->bs_overflow) {
+			enc->irq_status |= INT_STA_BSF_OFLW_STA;
+			enc->bs_overflow = 0;
+		}
+
+		if (mpp_task) {
+			if (test_and_set_bit(TASK_STATE_HANDLE, &mpp_task->state)) {
+				dev_err(enc->dev, "error, task %d already handled, irq %#x\n",
+					mpp_task->task_index, enc->irq_status);
+				ret = IRQ_HANDLED;
+				goto done;
+			}
+			cancel_delayed_work(&mpp_task->timeout_work);
+			set_bit(TASK_STATE_IRQ, &mpp_task->state);
+			mpp_task->irq_status = enc->irq_status;
+			rkvenc_iommu_dev_deactivate(enc->iommu_info, enc);
+
+			/* Trigger the worker to process the ISR bottom half */
+			kthread_queue_work(&enc->queue->worker, &enc->work);
+		}
+
+		ret = IRQ_HANDLED;
+	}
+
+	/* 5. process error irq */
+	if (irq_status & INT_STA_ERROR) {
+		enc->irq_status = irq_status;
+		dev_err(enc->dev, "error status %08x\n", irq_status);
+
+		if (mpp_task) {
+			if (!test_and_set_bit(TASK_STATE_HANDLE, &mpp_task->state)) {
+				cancel_delayed_work(&mpp_task->timeout_work);
+				set_bit(TASK_STATE_IRQ, &mpp_task->state);
+				mpp_task->irq_status = enc->irq_status;
+				rkvenc_iommu_dev_deactivate(enc->iommu_info, enc);
+				kthread_queue_work(&enc->queue->worker, &enc->work);
+			}
+		}
+
+		ret = IRQ_HANDLED;
+	}
+
+done:
+	return ret;
+}
+
+/* ---- Run task: write registers and start HW ---- */
+int rkvenc_hw_run(struct rkvenc_dev *enc, struct rkvenc_mpp_task *mpp_task)
+{
+	u32 i, j;
+	u32 start_val = 0;
+	int ret;
+	struct rkvenc_task *task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+	const struct rkvenc_hw_info *hw = enc->hw_info;
+
+	rkvenc_debug_enter();
+
+	/* Power on encoder and its IOMMU */
+	if (enc->iommu_info && enc->iommu_info->pdev)
+		pm_runtime_get_sync(&enc->iommu_info->pdev->dev);
+	pm_runtime_get_sync(enc->dev);
+	pm_stay_awake(enc->dev);
+	rkvenc_hw_clk_on(enc);
+
+	/* Reset group down_read if available */
+	if (enc->reset_group)
+		down_read(&enc->reset_group->rw_sem);
+
+	/* Match BSP: ensure IOMMU is attached and fault handler is active */
+	ret = rkvenc_iommu_attach(enc->iommu_info);
+	if (ret) {
+		rkvenc_err("iommu attach failed: %d\n", ret);
+		goto err_unlock;
+	}
+
+	ret = rkvenc_iommu_dev_activate(enc->iommu_info, enc);
+	if (ret) {
+		rkvenc_err("iommu activate failed: %d\n", ret);
+		goto err_unlock;
+	}
+
+	/* Add force clear to avoid pagefault (VEPU580 workaround) */
+	if (hw->vepu_type == RKVENC_VEPU_580) {
+		rkvenc_write(enc, hw->enc_clr_base, 0x2);
+		udelay(5);
+		rkvenc_write(enc, hw->enc_clr_base, 0x0);
+	}
+
+	/* Clear hardware counter */
+	rkvenc_write_relaxed(enc, 0x5300, 0x2);
+
+	/* Patch dual-core handshake IDs */
+	rkvenc2_patch_dchs(enc, task);
+
+	/* Write all class registers except enc_start */
+	for (i = 0; i < task->w_req_cnt; i++) {
+		u32 s, e;
+		u32 *regs;
+		struct mpp_request msg;
+		struct mpp_request *req = &task->w_reqs[i];
+
+		ret = rkvenc_get_class_msg(task, req->offset, &msg);
+		if (ret)
+			goto err_deactivate;
+
+		s = (req->offset - msg.offset) / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		regs = (u32 *)msg.data;
+		for (j = s; j < e; j++) {
+			u32 off = msg.offset + j * sizeof(u32);
+
+			if (off == hw->enc_start_base) {
+				start_val = regs[j];
+				continue;
+			}
+			rkvenc_write_relaxed(enc, off, regs[j]);
+		}
+	}
+
+	/* flush tlb before starting hardware */
+	rkvenc_iommu_flush_tlb(enc->iommu_info);
+
+	/* init current task */
+	enc->cur_task = mpp_task;
+
+	/* Calculate and write watchdog timeout threshold */
+	rkvenc2_calc_timeout_thd(enc);
+
+	/* Schedule timeout work */
+	INIT_DELAYED_WORK(&mpp_task->timeout_work, rkvenc_task_timeout_work);
+	schedule_delayed_work(&mpp_task->timeout_work,
+			      msecs_to_jiffies(MPP_WORK_TIMEOUT_DELAY));
+
+	/* memory barrier before starting HW */
+	wmb();
+	rkvenc_write(enc, hw->enc_start_base, start_val);
+
+	rkvenc_debug_leave();
+
+	return 0;
+
+err_deactivate:
+	rkvenc_iommu_dev_deactivate(enc->iommu_info, enc);
+err_unlock:
+	if (enc->reset_group)
+		up_read(&enc->reset_group->rw_sem);
+	rkvenc_hw_clk_off(enc);
+	pm_relax(enc->dev);
+	pm_runtime_mark_last_busy(enc->dev);
+	pm_runtime_put_autosuspend(enc->dev);
+	if (enc->iommu_info && enc->iommu_info->pdev)
+		pm_runtime_put_sync(&enc->iommu_info->pdev->dev);
+
+	return ret;
+}
+
+/* ---- Finish: read status registers back from HW ---- */
+int rkvenc_hw_finish(struct rkvenc_dev *mpp, struct rkvenc_mpp_task *mpp_task)
+{
+	u32 i, j;
+	u32 *reg;
+	struct rkvenc_task *task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+
+	rkvenc_debug_enter();
+
+	/* Clear DCHS working flag so this core can accept the next task */
+	rkvenc2_update_dchs(mpp, task);
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		int ret;
+		int s, e;
+		struct mpp_request msg;
+		struct mpp_request *req = &task->r_reqs[i];
+
+		ret = rkvenc_get_class_msg(task, req->offset, &msg);
+		if (ret)
+			return -EINVAL;
+		s = (req->offset - msg.offset) / sizeof(u32);
+		e = s + req->size / sizeof(u32);
+		reg = (u32 *)msg.data;
+		for (j = s; j < e; j++)
+			reg[j] = rkvenc_read_relaxed(mpp, msg.offset + j * sizeof(u32));
+	}
+
+	/* Sync bitstream buffer if present */
+	if (task->bs_buf) {
+		u32 bs_size = rkvenc_read(mpp, 0x4064);
+
+		rkvenc_dma_buf_sync(task->bs_buf, 0, bs_size + task->offset_bs,
+				    DMA_FROM_DEVICE, true);
+	}
+
+	/* Revert irq status register for userspace readback */
+	reg = rkvenc_get_class_reg(task, task->hw_info->int_sta_base);
+	if (reg)
+		*reg = task->irq_status;
+
+	rkvenc_debug_leave();
+
+	return 0;
+}
+
+/* ---- Soft reset ---- */
+static int rkvenc_soft_reset(struct rkvenc_dev *enc)
+{
+	const struct rkvenc_hw_info *hw = enc->hw_info;
+	u32 rst_status = 0;
+	int ret;
+
+	/* safe reset */
+	rkvenc_write(enc, hw->int_mask_base, 0x3FF);
+	rkvenc_write(enc, hw->enc_clr_base, 0x1);
+	ret = readl_relaxed_poll_timeout(enc->reg_base + hw->int_sta_base,
+					 rst_status,
+					 rst_status & RKVENC_SCLR_DONE_STA,
+					 0, 5);
+	if (ret)
+		rkvenc_err("safe reset failed\n");
+	rkvenc_write(enc, hw->enc_clr_base, 0x2);
+	udelay(5);
+	rkvenc_write(enc, hw->enc_clr_base, 0);
+	rkvenc_write(enc, hw->int_clr_base, 0xffffffff);
+	rkvenc_write(enc, hw->int_sta_base, 0);
+
+	return ret;
+}
+
+/* ---- Full reset (soft + CRU fallback) ---- */
+int rkvenc_hw_reset(struct rkvenc_dev *enc)
+{
+	int ret;
+	struct rkvenc_taskqueue *queue = enc->queue;
+	struct rkvenc_ccu *ccu = enc->ccu;
+	unsigned long flags;
+
+	/* safe reset first */
+	ret = rkvenc_soft_reset(enc);
+
+	/* cru reset as fallback */
+	if (ret && enc->rst_a && enc->rst_h && enc->rst_core) {
+		rkvenc_err("soft reset timeout, use cru reset\n");
+		rkvenc_safe_reset(enc->rst_a);
+		rkvenc_safe_reset(enc->rst_h);
+		rkvenc_safe_reset(enc->rst_core);
+		udelay(5);
+		rkvenc_safe_unreset(enc->rst_a);
+		rkvenc_safe_unreset(enc->rst_h);
+		rkvenc_safe_unreset(enc->rst_core);
+
+		/*
+		 * CRU reset wipes the IOMMU registers (DTE, paging).
+		 * Force a suspend→resume cycle on the IOMMU device to
+		 * re-program them via rk_iommu_enable.
+		 */
+		if (enc->iommu_info && enc->iommu_info->pdev) {
+			struct device *iommu_dev = &enc->iommu_info->pdev->dev;
+
+			pm_runtime_put_sync(iommu_dev);
+			pm_runtime_get_sync(iommu_dev);
+		}
+	}
+
+	set_bit(enc->core_id, &queue->core_idle);
+
+	if (ccu) {
+		spin_lock_irqsave(&ccu->lock_dchs, flags);
+		ccu->dchs[enc->core_id].val[0] = 0;
+		ccu->dchs[enc->core_id].val[1] = 0;
+		spin_unlock_irqrestore(&ccu->lock_dchs, flags);
+	}
+
+	atomic_set(&enc->reset_request, 0);
+
+	return 0;
+}
+
+/* ---- Clock on/off ---- */
+void rkvenc_hw_clk_on(struct rkvenc_dev *enc)
+{
+	rkvenc_clk_safe_enable(enc->aclk_info.clk);
+	rkvenc_clk_safe_enable(enc->hclk_info.clk);
+	rkvenc_clk_safe_enable(enc->core_clk_info.clk);
+}
+
+void rkvenc_hw_clk_off(struct rkvenc_dev *enc)
+{
+	clk_disable_unprepare(enc->aclk_info.clk);
+	clk_disable_unprepare(enc->hclk_info.clk);
+	clk_disable_unprepare(enc->core_clk_info.clk);
+}
+
+/* ---- HW init during probe ---- */
+static int rkvenc_hw_init_clocks(struct rkvenc_dev *enc)
+{
+	int ret;
+	struct device *dev = enc->dev;
+
+	enc->aclk_info.clk = devm_clk_get(dev, "aclk_vcodec");
+	if (IS_ERR(enc->aclk_info.clk)) {
+		ret = PTR_ERR(enc->aclk_info.clk);
+		enc->aclk_info.clk = NULL;
+		dev_err(dev, "failed to get aclk_vcodec: %d\n", ret);
+	}
+
+	enc->hclk_info.clk = devm_clk_get(dev, "hclk_vcodec");
+	if (IS_ERR(enc->hclk_info.clk)) {
+		ret = PTR_ERR(enc->hclk_info.clk);
+		enc->hclk_info.clk = NULL;
+		dev_err(dev, "failed to get hclk_vcodec: %d\n", ret);
+	}
+
+	enc->core_clk_info.clk = devm_clk_get(dev, "clk_core");
+	if (IS_ERR(enc->core_clk_info.clk)) {
+		ret = PTR_ERR(enc->core_clk_info.clk);
+		enc->core_clk_info.clk = NULL;
+		dev_err(dev, "failed to get clk_core: %d\n", ret);
+	}
+
+	/* Set default rates */
+	enc->aclk_info.default_rate_hz = 300000000;
+	enc->core_clk_info.default_rate_hz = 600000000;
+
+	return 0;
+}
+
+static int rkvenc_hw_init_resets(struct rkvenc_dev *enc)
+{
+	struct device *dev = enc->dev;
+
+	enc->rst_a = devm_reset_control_get_optional(dev, "video_a");
+	if (IS_ERR(enc->rst_a)) {
+		enc->rst_a = NULL;
+		dev_warn(dev, "no video_a reset\n");
+	}
+
+	enc->rst_h = devm_reset_control_get_optional(dev, "video_h");
+	if (IS_ERR(enc->rst_h)) {
+		enc->rst_h = NULL;
+		dev_warn(dev, "no video_h reset\n");
+	}
+
+	enc->rst_core = devm_reset_control_get_optional(dev, "video_core");
+	if (IS_ERR(enc->rst_core)) {
+		enc->rst_core = NULL;
+		dev_warn(dev, "no video_core reset\n");
+	}
+
+	return 0;
+}
+
+int rkvenc_hw_probe(struct rkvenc_dev *enc, struct platform_device *pdev)
+{
+	int ret;
+	struct resource *res;
+	struct device *dev = &pdev->dev;
+
+	enc->dev = dev;
+
+	/* Read task-capacity from DTS */
+	ret = of_property_read_u32(dev->of_node, "rockchip,task-capacity",
+				   &enc->task_capacity);
+	if (ret)
+		enc->task_capacity = 1;
+
+	/* PM runtime */
+	pm_runtime_set_autosuspend_delay(dev, 2000);
+	pm_runtime_use_autosuspend(dev);
+	device_init_wakeup(dev, true);
+	pm_runtime_enable(dev);
+
+	/* IRQ */
+	enc->irq = platform_get_irq(pdev, 0);
+	if (enc->irq < 0) {
+		dev_err(dev, "no interrupt resource found\n");
+		ret = -ENODEV;
+		goto failed;
+	}
+
+	/* Register space */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(dev, "no memory resource defined\n");
+		ret = -ENODEV;
+		goto failed;
+	}
+	enc->reg_base = devm_ioremap(dev, res->start, resource_size(res));
+	if (!enc->reg_base) {
+		dev_err(dev, "ioremap failed for resource %pR\n", res);
+		ret = -ENOMEM;
+		goto failed;
+	}
+	enc->io_base = res->start;
+
+	/* Clocks and resets */
+	rkvenc_hw_init_clocks(enc);
+	rkvenc_hw_init_resets(enc);
+
+	/* Set HW info early so it's available for HW ID read */
+	enc->hw_info = &rkvenc_v2_hw_info;
+	enc->trans_info = trans_rkvenc_v2;
+
+	/* IOMMU */
+	enc->iommu_info = rkvenc_iommu_probe(dev);
+	if (IS_ERR(enc->iommu_info)) {
+		dev_err(dev, "failed to attach iommu\n");
+		enc->iommu_info = NULL;
+	}
+
+	/* Read hardware ID */
+	pm_runtime_get_sync(dev);
+	rkvenc_hw_clk_on(enc);
+	rkvenc_v2_hw_info.hw.hw_id = rkvenc_read(enc,
+					rkvenc_v2_hw_info.hw.reg_id * sizeof(u32));
+	rkvenc_hw_clk_off(enc);
+	pm_runtime_put_sync(dev);
+
+	/* Init state */
+	atomic_set(&enc->reset_request, 0);
+	atomic_set(&enc->session_index, 0);
+	atomic_set(&enc->task_count, 0);
+	atomic_set(&enc->task_index, 0);
+
+	enc->session_max_buffers = RKVENC_SESSION_MAX_BUFFERS;
+
+	return 0;
+
+failed:
+	device_init_wakeup(dev, false);
+	pm_runtime_disable(dev);
+	return ret;
+}
+
+int rkvenc_hw_remove(struct rkvenc_dev *enc)
+{
+	rkvenc_iommu_remove(enc->iommu_info);
+	device_init_wakeup(enc->dev, false);
+	pm_runtime_disable(enc->dev);
+	return 0;
+}
diff -ruN aa/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.h bb/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.h
--- aa/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.h	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_hw.h	2026-02-07 21:25:19
@@ -0,0 +1,881 @@
+/* SPDX-License-Identifier: (GPL-2.0+ OR MIT) */
+/*
+ * Rockchip VEPU580 encoder driver - Hardware definitions
+ * Ported from Rockchip BSP mpp_rkvenc2.c / mpp_common.h
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ */
+
+#ifndef __RKVENC_HW_H__
+#define __RKVENC_HW_H__
+
+#include <linux/clk.h>
+#include <linux/cdev.h>
+#include <linux/dma-direction.h>
+#include <linux/io.h>
+#include <linux/iommu.h>
+#include <linux/interrupt.h>
+#include <linux/kfifo.h>
+#include <linux/kthread.h>
+#include <linux/platform_device.h>
+#include <linux/pm_runtime.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include <linux/types.h>
+#include <linux/wait.h>
+#include <linux/workqueue.h>
+
+#include <uapi/linux/rkvenc.h>
+
+/* ---- Debug infrastructure ---- */
+extern unsigned int rkvenc_debug;
+
+#define DEBUG_IRQ_STATUS		0x00000004
+#define DEBUG_IOMMU			0x00000008
+#define DEBUG_IOCTL			0x00000010
+#define DEBUG_FUNCTION			0x00000020
+#define DEBUG_TASK_INFO			0x00000200
+#define DEBUG_DUMP_ERR_REG		0x00000400
+#define DEBUG_SRAM_INFO			0x00200000
+#define DEBUG_CCU			0x01000000
+#define DEBUG_CORE			0x02000000
+#define DEBUG_SLICE			0x00000002
+
+#define rkvenc_debug_unlikely(type)	(unlikely(rkvenc_debug & (type)))
+
+#define rkvenc_debug_func(type, fmt, args...)			\
+	do {							\
+		if (unlikely(rkvenc_debug & (type)))		\
+			pr_info("%s:%d: " fmt,			\
+				__func__, __LINE__, ##args);	\
+	} while (0)
+
+#define rkvenc_dbg(type, fmt, args...)				\
+	do {							\
+		if (unlikely(rkvenc_debug & (type)))		\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+#define rkvenc_debug_enter()					\
+	do {							\
+		if (unlikely(rkvenc_debug & DEBUG_FUNCTION))	\
+			pr_info("%s:%d: enter\n",		\
+				__func__, __LINE__);		\
+	} while (0)
+
+#define rkvenc_debug_leave()					\
+	do {							\
+		if (unlikely(rkvenc_debug & DEBUG_FUNCTION))	\
+			pr_info("%s:%d: leave\n",		\
+				__func__, __LINE__);		\
+	} while (0)
+
+#define rkvenc_err(fmt, args...)				\
+	pr_err("%s:%d: " fmt, __func__, __LINE__, ##args)
+
+#define rkvenc_dbg_core(fmt, args...)				\
+	do {							\
+		if (unlikely(rkvenc_debug & DEBUG_CORE))		\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+#define rkvenc_dbg_slice(fmt, args...)				\
+	do {							\
+		if (unlikely(rkvenc_debug & DEBUG_SLICE))	\
+			pr_info(fmt, ##args);			\
+	} while (0)
+
+/* ---- Constants ---- */
+#define MPP_DRIVER_NAME			"rkvenc"
+#define MPP_SERVICE_NAME		"mpp_service"
+#define MPP_CLASS_NAME			"mpp_class"
+
+#define MPP_MAX_CORE_NUM		4
+#define MPP_MAX_TASK_CAPACITY		16
+#define MPP_MAX_MSG_NUM			32
+#define MPP_MAX_REG_TRANS_NUM		128
+#define MPP_WORK_TIMEOUT_DELAY		200
+#define RKVENC_SESSION_MAX_BUFFERS	40
+#define RKVENC_MAX_CORE_NUM		2
+#define RKVENC_MAX_DCHS_ID		16
+
+/* ---- Device types (must match MPP userspace MppClientType enum) ---- */
+enum MPP_DEVICE_TYPE {
+	MPP_DEVICE_VDPU1		= 0,
+	MPP_DEVICE_VDPU2		= 1,
+	MPP_DEVICE_VDPU1_PP		= 2,
+	MPP_DEVICE_VDPU2_PP		= 3,
+	MPP_DEVICE_AV1DEC		= 4,
+
+	MPP_DEVICE_HEVC_DEC		= 8,
+	MPP_DEVICE_RKVDEC		= 9,
+
+	MPP_DEVICE_AVSPLUS_DEC		= 12,
+	MPP_DEVICE_RKJPEGD		= 13,
+
+	MPP_DEVICE_RKVENC		= 16,
+	MPP_DEVICE_VEPU1		= 17,
+	MPP_DEVICE_VEPU2		= 18,
+	MPP_DEVICE_VEPU2_JPEG		= 19,
+	MPP_DEVICE_RKJPEGE		= 20,
+
+	MPP_DEVICE_VEPU22		= 24,
+
+	MPP_DEVICE_IEP2			= 28,
+	MPP_DEVICE_VDPP			= 29,
+	MPP_DEVICE_BUTT			= 30,
+};
+
+/* ---- Register class definitions for VEPU580 ---- */
+enum RKVENC_CLASS_TYPE {
+	RKVENC_CLASS_BASE	= 0,
+	RKVENC_CLASS_PIC	= 1,
+	RKVENC_CLASS_RC		= 2,
+	RKVENC_CLASS_PAR	= 3,
+	RKVENC_CLASS_SQI	= 4,
+	RKVENC_CLASS_SCL	= 5,
+	RKVENC_CLASS_OSD	= 6,
+	RKVENC_CLASS_ST		= 7,
+	RKVENC_CLASS_DBG	= 8,
+	RKVENC_CLASS_BUTT,
+};
+
+enum RKVENC_CLASS_FD {
+	RKVENC_CLASS_FD_BASE	= 0,
+	RKVENC_CLASS_FD_OSD	= 1,
+	RKVENC_CLASS_FD_BUTT,
+};
+
+enum RKVENC_FORMAT_TYPE {
+	RKVENC_FMT_BASE		= 0x0000,
+	RKVENC_FMT_H264E	= RKVENC_FMT_BASE + 0,
+	RKVENC_FMT_H265E	= RKVENC_FMT_BASE + 1,
+	RKVENC_FMT_JPEGE	= RKVENC_FMT_BASE + 2,
+
+	RKVENC_FMT_OSD_BASE	= 0x1000,
+	RKVENC_FMT_H264E_OSD	= RKVENC_FMT_OSD_BASE + 0,
+	RKVENC_FMT_H265E_OSD	= RKVENC_FMT_OSD_BASE + 1,
+	RKVENC_FMT_JPEGE_OSD	= RKVENC_FMT_OSD_BASE + 2,
+	RKVENC_FMT_BUTT,
+};
+
+enum RKVENC_VEPU_TYPE {
+	RKVENC_VEPU_580		= 0,
+	RKVENC_VEPU_BUTT,
+};
+
+/* ---- Register offsets ---- */
+#define RKVENC_WDG			0x0038
+
+/* PIC class data array indices (relative to PIC class base 0x0280) */
+#define RKVENC2_PIC_BASE		0x0280
+#define RKVENC2_REG_ENC_PIC		((0x0300 - RKVENC2_PIC_BASE) / sizeof(u32))
+#define RKVENC2_REG_EXT_LINE_BUF_BASE	22
+#define RKVENC2_REG_SLI_SPLIT		56
+
+/* Absolute MMIO byte offsets used by runtime paths (match BSP mpp_rkvenc2.c) */
+#define RKVENC2_REG_ADR_BSBT		0x02b0
+#define RKVENC2_REG_ADR_BSBB		0x02b4
+#define RKVENC2_REG_ADR_BSBS		0x02b8
+#define RKVENC2_REG_ADR_BSBR		0x02bc
+
+#define RKVENC580_REG_ADR_BSBR		0x02b8
+
+#define RKVENC2_REG_ST_BSB		0x402c
+
+#define RKVENC2_REG_SLICE_NUM_BASE	0x4034
+#define RKVENC2_REG_SLICE_LEN_BASE	0x4038
+
+#define DCHS_REG_OFFSET			0x0084
+
+#define RKVENC2_BIT_ENC_STND		BIT(0)
+#define RKVENC2_BIT_SLEN_FIFO		BIT(30)
+#define RKVENC2_BIT_SLI_SPLIT		BIT(0)
+#define RKVENC2_BIT_SLI_FLUSH		BIT(15)
+#define RKVENC2_BIT_REC_FBC_DIS		BIT(31)
+#define RKVENC2_BIT_VAL_H264		0
+#define RKVENC2_BIT_VAL_H265		1
+
+#define RKVENC_SCLR_DONE_STA		BIT(4)
+
+/* Interrupt status bits */
+#define INT_STA_ENC_DONE_STA		BIT(0)
+#define INT_STA_SLC_DONE_STA		BIT(1)
+#define INT_STA_BSF_OFLW_STA		BIT(2)
+#define INT_STA_WDG_STA			BIT(5)
+#define INT_STA_ERROR			(0x03f0)
+
+/* Dual-core handshake */
+#define DCHS_TXE			BIT(8)
+
+/* ---- Clock mode ---- */
+enum MPP_CLOCK_MODE {
+	CLK_MODE_DEFAULT,
+	CLK_MODE_REDUCE,
+	CLK_MODE_NORMAL,
+	CLK_MODE_ADVANCED,
+	CLK_MODE_DEBUG,
+};
+
+/* ---- Task state bits ---- */
+enum {
+	TASK_STATE_PENDING	= 0,
+	TASK_STATE_RUNNING	= 1,
+	TASK_STATE_START	= 2,
+	TASK_STATE_IRQ		= 3,
+	TASK_STATE_HANDLE	= 4,
+	TASK_STATE_TIMEOUT	= 5,
+	TASK_STATE_FINISH	= 6,
+	TASK_STATE_DONE		= 7,
+	TASK_STATE_ABORT	= 8,
+};
+
+/* ---- Clock info ---- */
+struct rkvenc_clk_info {
+	struct clk *clk;
+	unsigned long debug_rate_hz;
+	unsigned long reduce_rate_hz;
+	unsigned long normal_rate_hz;
+	unsigned long advanced_rate_hz;
+	unsigned long default_rate_hz;
+	unsigned long used_rate_hz;
+	unsigned long real_rate_hz;
+};
+
+/* ---- HW info structure (register class map for VEPU580) ---- */
+struct mpp_hw_info {
+	s32 reg_num;
+	s32 reg_id;
+	s32 reg_en;
+	s32 reg_start;
+	s32 reg_end;
+	u32 hw_id;
+};
+
+struct rkvenc_reg_msg {
+	u32 base_s;
+	u32 base_e;
+};
+
+struct rkvenc_fmt_reg {
+	u32 class;
+	u32 base;
+	u32 bitpos;
+	u32 bitlen;
+};
+
+struct rkvenc_fd_reg {
+	u32 class;
+	u32 base_fmt;
+};
+
+struct rkvenc_hw_info {
+	struct mpp_hw_info hw;
+
+	u32 reg_class;
+	struct rkvenc_reg_msg reg_msg[RKVENC_CLASS_BUTT];
+
+	u32 fd_class;
+	struct rkvenc_fd_reg fd_reg[RKVENC_CLASS_FD_BUTT];
+
+	struct rkvenc_fmt_reg fmt_reg;
+
+	u32 enc_start_base;
+	u32 enc_clr_base;
+	u32 int_en_base;
+	u32 int_mask_base;
+	u32 int_clr_base;
+	u32 int_sta_base;
+	u32 enc_wdg_base;
+	u32 err_mask;
+	u32 enc_rsl;
+	u32 dcsh_class_ofst;
+	u32 vepu_type;
+};
+
+/* ---- FD translation table ---- */
+struct rkvenc_trans_info {
+	const int count;
+	const u16 *table;
+};
+
+/* ---- Memory region for DMA-buf tracking ---- */
+#define MPP_MAX_MEM_REGION		128
+
+struct rkvenc_mem_region {
+	struct list_head reg_link;
+	/* DMABUF information */
+	dma_addr_t iova;
+	unsigned long len;
+	int fd;
+	u32 reg_class;
+	u32 reg_idx;
+	bool is_dup;
+	/* handle from DMA session */
+	void *hdl;
+};
+
+/* ---- Offset info for register address offsets ---- */
+struct reg_offset_elem {
+	u32 index;
+	u32 offset;
+};
+
+struct reg_offset_info {
+	u32 cnt;
+	struct reg_offset_elem elem[MPP_MAX_REG_TRANS_NUM];
+};
+
+/* ---- DMA buffer management ---- */
+#define MPP_SESSION_MAX_BUFFERS		60
+
+struct rkvenc_dma_buffer {
+	struct list_head link;
+	struct rkvenc_dma_session *dma;
+	struct dma_buf *dmabuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	enum dma_data_direction dir;
+	dma_addr_t iova;
+	unsigned long size;
+	void *vaddr;
+	struct kref ref;
+	struct device *dev;
+};
+
+struct rkvenc_dma_session {
+	struct list_head unused_list;
+	struct list_head used_list;
+	struct list_head static_list;
+	struct rkvenc_dma_buffer dma_bufs[MPP_SESSION_MAX_BUFFERS];
+	struct mutex list_mutex;
+	u32 max_buffers;
+	int buffer_count;
+	struct device *dev;
+};
+
+/* ---- IOMMU info ---- */
+struct rkvenc_iommu_info {
+	struct rw_semaphore *rw_sem;
+	struct rw_semaphore rw_sem_self;
+	struct device *dev;
+	struct platform_device *pdev;
+	struct iommu_domain *domain;
+	struct iommu_group *group;
+	spinlock_t dev_lock;
+	struct rkvenc_dev *dev_active;
+	int irq;
+	int got_irq;
+};
+
+/* ---- Task queue ---- */
+struct rkvenc_taskqueue {
+	struct mutex session_lock;
+	struct mutex pending_lock;
+	spinlock_t running_lock;
+	struct mutex dev_lock;
+
+	struct list_head session_attach;
+	struct list_head session_detach;
+	struct list_head pending_list;
+	struct list_head running_list;
+	struct list_head dev_list;
+
+	struct kthread_worker worker;
+	struct task_struct *kworker_task;
+
+	u32 task_capacity;
+	atomic_t reset_request;
+	atomic_t detach_count;
+	atomic_t task_id;
+
+	/* Multi-core support */
+	struct rkvenc_dev *cores[MPP_MAX_CORE_NUM];
+	u32 core_count;
+	u32 core_id_max;
+	unsigned long core_idle;
+	unsigned long dev_active_flags;
+};
+
+/* ---- Reset group ---- */
+struct rkvenc_reset_group {
+	struct rw_semaphore rw_sem;
+	bool rw_sem_on;
+	struct reset_control *resets[3];
+	struct rkvenc_taskqueue *queue;
+};
+
+/* ---- GRF info ---- */
+#define MPP_GRF_VAL_MASK		0xffff
+
+struct rkvenc_grf_info {
+	struct regmap *grf;
+	u32 offset;
+	u32 val;
+};
+
+/* ---- MPP service ---- */
+struct rkvenc_service {
+	struct device *dev;
+	struct cdev mpp_cdev;
+	dev_t dev_id;
+	struct class *cls;
+	struct device *child_dev;
+
+	u32 taskqueue_cnt;
+	struct rkvenc_taskqueue *task_queues[MPP_DEVICE_BUTT];
+	u32 reset_group_cnt;
+	struct rkvenc_reset_group *reset_groups[MPP_DEVICE_BUTT];
+
+	struct rkvenc_dev *sub_devices[MPP_DEVICE_BUTT];
+	unsigned long hw_support;
+	struct rkvenc_grf_info grf_infos[MPP_DEVICE_BUTT];
+
+	struct mutex session_lock;
+	struct list_head session_list;
+
+	u32 timing_en;
+	atomic_t shutdown_request;
+};
+
+/* ---- MPP task ---- */
+struct rkvenc_mpp_task {
+	struct list_head pending_link;
+	struct list_head queue_link;
+	struct list_head mem_region_list;
+	unsigned long state;
+	u32 mem_count;
+	struct rkvenc_session *session;
+	struct rkvenc_dev *mpp;
+	s32 core_id;
+	const struct mpp_hw_info *hw_info;
+	u32 *reg;
+
+	struct kref ref;
+	wait_queue_head_t wait;
+	atomic_t abort_request;
+	u32 task_index;
+	u32 task_id;
+	struct delayed_work timeout_work;
+	u32 irq_status;
+	u32 hw_cycles;
+	s64 hw_time;
+
+	/* Memory regions */
+	struct rkvenc_mem_region mem_regions[MPP_MAX_MEM_REGION];
+
+	/* Timing */
+	ktime_t start;
+	ktime_t part;
+	ktime_t on_create;
+	ktime_t on_create_end;
+	ktime_t on_pending;
+	ktime_t on_run;
+	ktime_t on_sched_timeout;
+	ktime_t on_run_end;
+	ktime_t on_irq;
+	ktime_t on_cancel_timeout;
+	ktime_t on_finish;
+};
+
+/* ---- RKVENC-specific task ---- */
+struct rkvenc_class_msg {
+	u32 *data;
+	u32 size;
+	u32 valid;
+};
+
+union rkvenc2_dual_core_handshake_id {
+	u64 val[2];
+	struct {
+		/* val[0] */
+		u32 txe		: 1;
+		u32 reserved0	: 3;
+		u32 txid	: 4;
+		u32 rxe		: 1;
+		u32 reserved1	: 3;
+		u32 rxid	: 4;
+		u32 reserved2	: 16;
+		u32 session_id;
+		/* val[1] */
+		u32 txe_orig	: 1;
+		u32 reserved3	: 3;
+		u32 txid_orig	: 4;
+		u32 rxe_orig	: 1;
+		u32 reserved4	: 3;
+		u32 rxid_orig	: 4;
+		u32 txe_map	: 1;
+		u32 reserved5	: 3;
+		u32 txid_map	: 4;
+		u32 rxe_map	: 1;
+		u32 reserved6	: 3;
+		u32 rxid_map	: 4;
+		u32 working;
+	};
+};
+
+union rkvenc2_slice_len_info {
+	u32 val;
+	struct {
+		u32 slice_len	: 24;
+		u32 reserved	: 7;
+		u32 last	: 1;
+	};
+};
+
+union rkvenc2_frame_resolution {
+	u32 val;
+	struct {
+		u32 pic_wd8	: 13;
+		u32 reserved0	: 3;
+		u32 pic_hd8	: 13;
+		u32 reserved1	: 3;
+	};
+};
+
+struct rkvenc_poll_slice_cfg {
+	u32 count_max;
+	u32 count_ret;
+};
+
+#define RKVENC_SLICE_FIFO_LEN	512
+
+struct rkvenc_task {
+	struct rkvenc_mpp_task mpp_task;
+	const struct rkvenc_hw_info *hw_info;
+
+	/* Class-based register buffers */
+	struct rkvenc_class_msg reg[RKVENC_CLASS_BUTT];
+	struct mpp_request w_reqs[RKVENC_CLASS_BUTT];
+	struct mpp_request r_reqs[RKVENC_CLASS_BUTT];
+	u32 w_req_cnt;
+	u32 r_req_cnt;
+
+	/* Register offset info */
+	struct reg_offset_info off_inf;
+
+	/* Format and flags */
+	u32 fmt;
+	int clk_mode;
+	u32 irq_status;
+	u32 task_split;
+	u32 task_split_done;
+
+	/* Dual-core handshake ID */
+	union rkvenc2_dual_core_handshake_id dchs_id;
+
+	/* Slice split info */
+	DECLARE_KFIFO(slice_info, union rkvenc2_slice_len_info, RKVENC_SLICE_FIFO_LEN);
+	u32 slice_wr_cnt;
+	u32 slice_rd_cnt;
+	u32 last_slice_found;
+
+	/* Bitstream buffer tracking */
+	struct rkvenc_dma_buffer *bs_buf;
+	u32 offset_bs;
+};
+
+/* Forward declaration */
+struct rkvenc_task_msgs;
+
+/* ---- Session ---- */
+struct rkvenc_session {
+	int pid;
+	int index;
+
+	struct rkvenc_dev *mpp;
+	struct rkvenc_service *srv;
+	enum MPP_DEVICE_TYPE device_type;
+	struct rkvenc_dma_session *dma;
+
+	struct mutex pending_lock;
+	struct list_head pending_list;
+	struct list_head service_link;
+	struct list_head session_link;
+
+	atomic_t task_count;
+	atomic_t release_request;
+
+	/* FD translation table (from userspace) */
+	u16 trans_table[MPP_MAX_REG_TRANS_NUM];
+	u32 trans_count;
+	u32 msg_flags;
+
+	/* Session private data */
+	void *priv;
+
+};
+
+/* ---- Task messages ---- */
+struct rkvenc_task_msgs {
+	struct list_head list;
+	struct list_head list_session;
+
+	struct rkvenc_session *session;
+	struct rkvenc_taskqueue *queue;
+	struct rkvenc_mpp_task *task;
+	struct rkvenc_dev *mpp;
+
+	u32 flags;
+	u32 req_cnt;
+	u32 set_cnt;
+	u32 poll_cnt;
+	struct mpp_request *poll_req;
+	struct mpp_request reqs[MPP_MAX_MSG_NUM];
+
+	int ext_fd;
+};
+
+/* ---- Session private data ---- */
+struct rkvenc2_rcb_info_elem {
+	u32 index;
+	u32 size;
+};
+
+struct rkvenc2_rcb_info {
+	u32 cnt;
+	struct rkvenc2_rcb_info_elem elem[20];
+};
+
+enum {
+	ENC_INFO_BASE = 0,
+	ENC_INFO_WIDTH,
+	ENC_INFO_HEIGHT,
+	ENC_INFO_FORMAT,
+	ENC_INFO_FPS_IN,
+	ENC_INFO_FPS_OUT,
+	ENC_INFO_RC_MODE,
+	ENC_INFO_BITRATE,
+	ENC_INFO_GOP_SIZE,
+	ENC_INFO_FPS_CALC,
+	ENC_INFO_PROFILE,
+	ENC_INFO_BUTT,
+};
+
+enum {
+	CODEC_INFO_FLAG_NULL = 0,
+	CODEC_INFO_FLAG_NUMBER,
+	CODEC_INFO_FLAG_STRING,
+	CODEC_INFO_FLAG_BUTT,
+};
+
+struct codec_info_elem {
+	u32 type;
+	u32 flag;
+	u64 data;
+};
+
+struct rkvenc2_codec_info {
+	u32 flag;
+	u64 val;
+};
+
+struct rkvenc2_session_priv {
+	struct rw_semaphore rw_sem;
+	struct rkvenc2_rcb_info rcb_inf;
+	struct rkvenc2_codec_info codec_info[ENC_INFO_BUTT];
+};
+
+/* ---- CCU (Core Coordination Unit) ---- */
+struct rkvenc_ccu {
+	struct mutex lock;
+	struct list_head core_list;
+	int core_num;
+	struct rkvenc_dev *main_core;
+	spinlock_t lock_dchs;
+	union rkvenc2_dual_core_handshake_id dchs[RKVENC_MAX_CORE_NUM];
+};
+
+/* ---- Per-core encoder device ---- */
+struct rkvenc_dev {
+	struct device *dev;
+	void __iomem *reg_base;
+	resource_size_t io_base;
+	int irq;
+	u32 irq_status;
+
+	/* Framework */
+	struct rkvenc_service *srv;
+	struct rkvenc_taskqueue *queue;
+	struct rkvenc_iommu_info *iommu_info;
+	struct rkvenc_reset_group *reset_group;
+	struct rkvenc_grf_info *grf_info;
+	struct rkvenc_mpp_task *cur_task;
+	iommu_fault_handler_t fault_handler;
+
+	/* HW variant data */
+	const struct rkvenc_hw_info *hw_info;
+	const struct rkvenc_trans_info *trans_info;
+
+	s32 core_id;
+	u32 task_capacity;
+	u32 session_max_buffers;
+	u32 msgs_cap;
+	bool auto_freq_en;
+
+	/* Clocks and resets */
+	struct rkvenc_clk_info aclk_info;
+	struct rkvenc_clk_info hclk_info;
+	struct rkvenc_clk_info core_clk_info;
+	struct reset_control *rst_a;
+	struct reset_control *rst_h;
+	struct reset_control *rst_core;
+
+	/* Multi-core */
+	struct rkvenc_ccu *ccu;
+	struct list_head core_link;
+	struct list_head queue_link;
+
+	/* Work */
+	struct kthread_work work;
+
+	/* State */
+	atomic_t reset_request;
+	atomic_t session_index;
+	atomic_t task_count;
+	atomic_t task_index;
+
+	/* SRAM RCB */
+	dma_addr_t sram_iova;
+	u32 sram_size;
+	u32 sram_used;
+	int sram_enabled;
+	struct page *rcb_page;
+
+	/* Bitstream overflow flag */
+	u32 bs_overflow;
+
+	/* PM */
+	u32 disable;
+};
+
+/* ---- Helper macros ---- */
+static inline u32 rkvenc_read(struct rkvenc_dev *mpp, u32 reg)
+{
+	u32 val = readl(mpp->reg_base + reg);
+	return val;
+}
+
+static inline u32 rkvenc_read_relaxed(struct rkvenc_dev *mpp, u32 reg)
+{
+	return readl_relaxed(mpp->reg_base + reg);
+}
+
+static inline void rkvenc_write(struct rkvenc_dev *mpp, u32 reg, u32 val)
+{
+	writel(val, mpp->reg_base + reg);
+}
+
+static inline void rkvenc_write_relaxed(struct rkvenc_dev *mpp, u32 reg, u32 val)
+{
+	writel_relaxed(val, mpp->reg_base + reg);
+}
+
+static inline void rkvenc_clk_safe_enable(struct clk *clk)
+{
+	if (clk)
+		clk_prepare_enable(clk);
+}
+
+static inline void rkvenc_clk_safe_disable(struct clk *clk)
+{
+	if (clk)
+		clk_disable_unprepare(clk);
+}
+
+static inline void rkvenc_safe_reset(struct reset_control *rst)
+{
+	if (rst)
+		reset_control_assert(rst);
+}
+
+static inline void rkvenc_safe_unreset(struct reset_control *rst)
+{
+	if (rst)
+		reset_control_deassert(rst);
+}
+
+/* ---- Extern declarations ---- */
+
+/* rkvenc_service.c */
+int rkvenc_service_probe(struct platform_device *pdev);
+int rkvenc_service_remove(struct platform_device *pdev);
+void rkvenc_task_worker_default(struct kthread_work *work);
+extern const struct file_operations rkvenc_fops;
+
+/* rkvenc_iommu.c */
+struct rkvenc_iommu_info *rkvenc_iommu_probe(struct device *dev);
+int rkvenc_iommu_remove(struct rkvenc_iommu_info *info);
+int rkvenc_iommu_attach(struct rkvenc_iommu_info *info);
+int rkvenc_iommu_detach(struct rkvenc_iommu_info *info);
+int rkvenc_iommu_flush_tlb(struct rkvenc_iommu_info *info);
+int rkvenc_iommu_dev_activate(struct rkvenc_iommu_info *info, struct rkvenc_dev *dev);
+int rkvenc_iommu_dev_deactivate(struct rkvenc_iommu_info *info, struct rkvenc_dev *dev);
+
+struct rkvenc_dma_session *rkvenc_dma_session_create(struct device *dev, u32 max_buffers);
+int rkvenc_dma_session_destroy(struct rkvenc_dma_session *dma);
+struct rkvenc_dma_buffer *rkvenc_dma_import_fd(struct rkvenc_iommu_info *iommu_info,
+					       struct rkvenc_dma_session *dma,
+					       int fd, int static_use);
+struct rkvenc_dma_buffer *rkvenc_dma_find_buffer_fd(struct rkvenc_dma_session *dma, int fd);
+int rkvenc_dma_release(struct rkvenc_dma_session *dma, struct rkvenc_dma_buffer *buffer);
+int rkvenc_dma_release_fd(struct rkvenc_dma_session *dma, int fd);
+void rkvenc_dma_buf_sync(struct rkvenc_dma_buffer *buffer, u32 offset, u32 length,
+			 enum dma_data_direction dir, bool for_cpu);
+
+static inline int rkvenc_iommu_down_read(struct rkvenc_iommu_info *info)
+{
+	if (info)
+		down_read(info->rw_sem);
+	return 0;
+}
+
+static inline int rkvenc_iommu_up_read(struct rkvenc_iommu_info *info)
+{
+	if (info)
+		up_read(info->rw_sem);
+	return 0;
+}
+
+static inline int rkvenc_iommu_down_write(struct rkvenc_iommu_info *info)
+{
+	if (info)
+		down_write(info->rw_sem);
+	return 0;
+}
+
+static inline int rkvenc_iommu_up_write(struct rkvenc_iommu_info *info)
+{
+	if (info)
+		up_write(info->rw_sem);
+	return 0;
+}
+
+/* rkvenc_task.c */
+void rkvenc_free_task_callback(struct kref *ref);
+int rkvenc_task_init(struct rkvenc_session *session, struct rkvenc_mpp_task *task);
+int rkvenc_task_finish(struct rkvenc_session *session, struct rkvenc_mpp_task *task);
+int rkvenc_task_finalize(struct rkvenc_session *session, struct rkvenc_mpp_task *task);
+int rkvenc_translate_reg_address(struct rkvenc_session *session,
+				  struct rkvenc_mpp_task *task, int fmt, u32 reg_class,
+				  u32 *reg, struct reg_offset_info *off_inf);
+int rkvenc_extract_reg_offset_info(struct reg_offset_info *off_inf,
+				   struct mpp_request *req);
+int rkvenc_query_reg_offset_info(struct reg_offset_info *off_inf, u32 index);
+void rkvenc_task_timeout_work(struct work_struct *work_s);
+
+/* rkvenc_hw.c */
+int rkvenc_hw_probe(struct rkvenc_dev *enc, struct platform_device *pdev);
+int rkvenc_hw_remove(struct rkvenc_dev *enc);
+irqreturn_t rkvenc_hw_irq(int irq, void *param);
+int rkvenc_hw_run(struct rkvenc_dev *mpp, struct rkvenc_mpp_task *mpp_task);
+int rkvenc_hw_finish(struct rkvenc_dev *mpp, struct rkvenc_mpp_task *mpp_task);
+int rkvenc_hw_reset(struct rkvenc_dev *enc);
+void rkvenc_hw_clk_on(struct rkvenc_dev *enc);
+void rkvenc_hw_clk_off(struct rkvenc_dev *enc);
+
+/* HW info for VEPU580 */
+extern struct rkvenc_hw_info rkvenc_v2_hw_info;
+extern const struct rkvenc_trans_info trans_rkvenc_v2[];
+
+#endif /* __RKVENC_HW_H__ */
diff -ruN aa/drivers/media/platform/rockchip/rkvenc/rkvenc_iommu.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_iommu.c
--- aa/drivers/media/platform/rockchip/rkvenc/rkvenc_iommu.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_iommu.c	2026-02-08 03:09:11
@@ -0,0 +1,463 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - IOMMU and DMA buffer helpers
+ * Ported from Rockchip BSP mpp_iommu.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ */
+
+#include <linux/dma-buf.h>
+#include <linux/dma-mapping.h>
+#include <linux/iommu.h>
+#include <linux/interrupt.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/kref.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/pm_runtime.h>
+
+#include "rkvenc_hw.h"
+
+/* ---- DMA buffer find ---- */
+struct rkvenc_dma_buffer *
+rkvenc_dma_find_buffer_fd(struct rkvenc_dma_session *dma, int fd)
+{
+	struct dma_buf *dmabuf;
+	struct rkvenc_dma_buffer *out = NULL;
+	struct rkvenc_dma_buffer *buffer = NULL, *n;
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf))
+		return NULL;
+
+	mutex_lock(&dma->list_mutex);
+	list_for_each_entry_safe(buffer, n, &dma->used_list, link) {
+		if (buffer->dmabuf == dmabuf) {
+			out = buffer;
+			list_move_tail(&buffer->link, &buffer->dma->used_list);
+			break;
+		}
+	}
+	if (!out) {
+		list_for_each_entry_safe(buffer, n, &dma->static_list, link) {
+			if (buffer->dmabuf == dmabuf) {
+				out = buffer;
+				list_move_tail(&buffer->link, &buffer->dma->static_list);
+				break;
+			}
+		}
+	}
+	mutex_unlock(&dma->list_mutex);
+	dma_buf_put(dmabuf);
+
+	return out;
+}
+
+/* ---- DMA buffer release ---- */
+static void rkvenc_dma_release_buffer(struct kref *ref)
+{
+	struct rkvenc_dma_buffer *buffer =
+		container_of(ref, struct rkvenc_dma_buffer, ref);
+
+	buffer->dma->buffer_count--;
+	list_move_tail(&buffer->link, &buffer->dma->unused_list);
+
+	dma_buf_unmap_attachment(buffer->attach, buffer->sgt, buffer->dir);
+	dma_buf_detach(buffer->dmabuf, buffer->attach);
+	dma_buf_put(buffer->dmabuf);
+	buffer->dma = NULL;
+	buffer->dmabuf = NULL;
+	buffer->attach = NULL;
+	buffer->sgt = NULL;
+	buffer->iova = 0;
+	buffer->size = 0;
+	buffer->vaddr = NULL;
+}
+
+static int rkvenc_dma_remove_extra_buffer(struct rkvenc_dma_session *dma)
+{
+	struct rkvenc_dma_buffer *n;
+	struct rkvenc_dma_buffer *removable = NULL, *buffer = NULL;
+
+	if (dma->buffer_count > dma->max_buffers) {
+		mutex_lock(&dma->list_mutex);
+		list_for_each_entry_safe(buffer, n, &dma->used_list, link) {
+			if (kref_read(&buffer->ref) == 1) {
+				removable = buffer;
+				break;
+			}
+		}
+		if (removable)
+			kref_put(&removable->ref, rkvenc_dma_release_buffer);
+		mutex_unlock(&dma->list_mutex);
+	}
+
+	return 0;
+}
+
+int rkvenc_dma_release(struct rkvenc_dma_session *dma,
+		       struct rkvenc_dma_buffer *buffer)
+{
+	mutex_lock(&dma->list_mutex);
+	kref_put(&buffer->ref, rkvenc_dma_release_buffer);
+	mutex_unlock(&dma->list_mutex);
+
+	return 0;
+}
+
+int rkvenc_dma_release_fd(struct rkvenc_dma_session *dma, int fd)
+{
+	struct rkvenc_dma_buffer *buffer;
+
+	buffer = rkvenc_dma_find_buffer_fd(dma, fd);
+	if (IS_ERR_OR_NULL(buffer)) {
+		dev_err(dma->dev, "can not find %d buffer in list\n", fd);
+		return -EINVAL;
+	}
+
+	mutex_lock(&dma->list_mutex);
+	kref_put(&buffer->ref, rkvenc_dma_release_buffer);
+	mutex_unlock(&dma->list_mutex);
+
+	return 0;
+}
+
+/* ---- DMA buffer import ---- */
+struct rkvenc_dma_buffer *
+rkvenc_dma_import_fd(struct rkvenc_iommu_info *iommu_info,
+		     struct rkvenc_dma_session *dma,
+		     int fd, int static_use)
+{
+	int ret = 0;
+	struct sg_table *sgt;
+	struct dma_buf *dmabuf;
+	struct rkvenc_dma_buffer *buffer;
+	struct dma_buf_attachment *attach;
+
+	if (!dma) {
+		rkvenc_err("dma session is null\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	rkvenc_dma_remove_extra_buffer(dma);
+
+	/* Check whether in dma session */
+	buffer = rkvenc_dma_find_buffer_fd(dma, fd);
+	if (!IS_ERR_OR_NULL(buffer)) {
+		if (kref_get_unless_zero(&buffer->ref))
+			return buffer;
+	}
+
+	dmabuf = dma_buf_get(fd);
+	if (IS_ERR(dmabuf)) {
+		ret = PTR_ERR(dmabuf);
+		rkvenc_err("dma_buf_get fd %d failed(%d)\n", fd, ret);
+		return ERR_PTR(ret);
+	}
+
+	mutex_lock(&dma->list_mutex);
+	buffer = list_first_entry_or_null(&dma->unused_list,
+					  struct rkvenc_dma_buffer, link);
+	if (!buffer) {
+		ret = -ENOMEM;
+		mutex_unlock(&dma->list_mutex);
+		goto fail;
+	}
+	list_del_init(&buffer->link);
+	mutex_unlock(&dma->list_mutex);
+
+	buffer->dmabuf = dmabuf;
+	buffer->dir = DMA_BIDIRECTIONAL;
+
+	attach = dma_buf_attach(buffer->dmabuf, dma->dev);
+	if (IS_ERR(attach)) {
+		ret = PTR_ERR(attach);
+		rkvenc_err("dma_buf_attach fd %d failed(%d)\n", fd, ret);
+		goto fail_attach;
+	}
+
+	sgt = dma_buf_map_attachment(attach, buffer->dir);
+	if (IS_ERR(sgt)) {
+		ret = PTR_ERR(sgt);
+		rkvenc_err("dma_buf_map_attachment fd %d failed(%d)\n", fd, ret);
+		goto fail_map;
+	}
+	buffer->iova = sg_dma_address(sgt->sgl);
+	buffer->size = sg_dma_len(sgt->sgl);
+	buffer->attach = attach;
+	buffer->sgt = sgt;
+	buffer->dma = dma;
+
+	kref_init(&buffer->ref);
+
+	if (!static_use)
+		kref_get(&buffer->ref);
+
+	mutex_lock(&dma->list_mutex);
+	dma->buffer_count++;
+	if (static_use)
+		list_add_tail(&buffer->link, &dma->static_list);
+	else
+		list_add_tail(&buffer->link, &dma->used_list);
+	mutex_unlock(&dma->list_mutex);
+
+	return buffer;
+
+fail_map:
+	dma_buf_detach(buffer->dmabuf, attach);
+fail_attach:
+	mutex_lock(&dma->list_mutex);
+	list_add_tail(&buffer->link, &dma->unused_list);
+	mutex_unlock(&dma->list_mutex);
+fail:
+	dma_buf_put(dmabuf);
+	return ERR_PTR(ret);
+}
+
+/* ---- DMA buffer sync ---- */
+void rkvenc_dma_buf_sync(struct rkvenc_dma_buffer *buffer, u32 offset, u32 length,
+			 enum dma_data_direction dir, bool for_cpu)
+{
+	struct device *dev = buffer->dma->dev;
+	struct sg_table *sgt = buffer->sgt;
+	struct scatterlist *sg = sgt->sgl;
+	dma_addr_t sg_dma_addr = sg_dma_address(sg);
+	unsigned int len = 0;
+	int i;
+
+	for_each_sgtable_sg(sgt, sg, i) {
+		unsigned int sg_offset, sg_left, size = 0;
+
+		len += sg->length;
+		if (len <= offset) {
+			sg_dma_addr += sg->length;
+			continue;
+		}
+
+		sg_left = len - offset;
+		sg_offset = sg->length - sg_left;
+
+		size = (length < sg_left) ? length : sg_left;
+
+		if (for_cpu)
+			dma_sync_single_range_for_cpu(dev, sg_dma_addr,
+						      sg_offset, size, dir);
+		else
+			dma_sync_single_range_for_device(dev, sg_dma_addr,
+							 sg_offset, size, dir);
+
+		offset += size;
+		length -= size;
+		sg_dma_addr += sg->length;
+
+		if (length == 0)
+			break;
+	}
+}
+
+/* ---- DMA session management ---- */
+int rkvenc_dma_session_destroy(struct rkvenc_dma_session *dma)
+{
+	struct rkvenc_dma_buffer *n, *buffer = NULL;
+
+	if (!dma)
+		return -EINVAL;
+
+	mutex_lock(&dma->list_mutex);
+	list_for_each_entry_safe(buffer, n, &dma->used_list, link)
+		kref_put(&buffer->ref, rkvenc_dma_release_buffer);
+	list_for_each_entry_safe(buffer, n, &dma->static_list, link)
+		kref_put(&buffer->ref, rkvenc_dma_release_buffer);
+	mutex_unlock(&dma->list_mutex);
+
+	kfree(dma);
+	return 0;
+}
+
+struct rkvenc_dma_session *
+rkvenc_dma_session_create(struct device *dev, u32 max_buffers)
+{
+	int i;
+	struct rkvenc_dma_session *dma;
+	struct rkvenc_dma_buffer *buffer;
+
+	dma = kzalloc(sizeof(*dma), GFP_KERNEL);
+	if (!dma)
+		return NULL;
+
+	mutex_init(&dma->list_mutex);
+	INIT_LIST_HEAD(&dma->unused_list);
+	INIT_LIST_HEAD(&dma->used_list);
+	INIT_LIST_HEAD(&dma->static_list);
+
+	if (max_buffers > MPP_SESSION_MAX_BUFFERS)
+		dma->max_buffers = MPP_SESSION_MAX_BUFFERS;
+	else
+		dma->max_buffers = max_buffers;
+
+	for (i = 0; i < ARRAY_SIZE(dma->dma_bufs); i++) {
+		buffer = &dma->dma_bufs[i];
+		buffer->dma = dma;
+		INIT_LIST_HEAD(&buffer->link);
+		list_add_tail(&buffer->link, &dma->unused_list);
+	}
+	dma->dev = dev;
+
+	return dma;
+}
+
+/* ---- IOMMU helpers ---- */
+int rkvenc_iommu_detach(struct rkvenc_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	iommu_detach_group(info->domain, info->group);
+	return 0;
+}
+
+int rkvenc_iommu_attach(struct rkvenc_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	if (info->domain == iommu_get_domain_for_dev(info->dev))
+		return 0;
+
+	return iommu_attach_group(info->domain, info->group);
+}
+
+int rkvenc_iommu_flush_tlb(struct rkvenc_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	if (info->domain && info->domain->ops)
+		iommu_flush_iotlb_all(info->domain);
+
+	return 0;
+}
+
+static int rkvenc_iommu_fault_handler(struct iommu_domain *iommu,
+				      struct device *iommu_dev,
+				      unsigned long iova,
+				      int status, void *arg)
+{
+	dev_err(iommu_dev, "IOMMU fault addr 0x%08lx status %x\n",
+		iova, status);
+
+	return 0;
+}
+
+int rkvenc_iommu_dev_activate(struct rkvenc_iommu_info *info, struct rkvenc_dev *dev)
+{
+	unsigned long flags;
+
+	if (!info)
+		return 0;
+
+	spin_lock_irqsave(&info->dev_lock, flags);
+
+	if (info->dev_active || !dev) {
+		dev_err(info->dev, "can not activate\n");
+		spin_unlock_irqrestore(&info->dev_lock, flags);
+		return -EINVAL;
+	}
+
+	info->dev_active = dev;
+	if (info->domain && info->domain->cookie_type == IOMMU_COOKIE_NONE)
+		iommu_set_fault_handler(info->domain, rkvenc_iommu_fault_handler, dev);
+
+	spin_unlock_irqrestore(&info->dev_lock, flags);
+
+	return 0;
+}
+
+int rkvenc_iommu_dev_deactivate(struct rkvenc_iommu_info *info, struct rkvenc_dev *dev)
+{
+	unsigned long flags;
+
+	if (!info)
+		return 0;
+
+	spin_lock_irqsave(&info->dev_lock, flags);
+	info->dev_active = NULL;
+	spin_unlock_irqrestore(&info->dev_lock, flags);
+
+	return 0;
+}
+
+struct rkvenc_iommu_info *rkvenc_iommu_probe(struct device *dev)
+{
+	int ret = 0;
+	struct device_node *np;
+	struct platform_device *pdev;
+	struct rkvenc_iommu_info *info;
+	struct iommu_domain *domain;
+	struct iommu_group *group;
+
+	np = of_parse_phandle(dev->of_node, "iommus", 0);
+	if (!np || !of_device_is_available(np)) {
+		rkvenc_err("failed to get IOMMU device node\n");
+		return ERR_PTR(-ENODEV);
+	}
+
+	pdev = of_find_device_by_node(np);
+	of_node_put(np);
+	if (!pdev) {
+		rkvenc_err("failed to get IOMMU platform device\n");
+		return ERR_PTR(-ENODEV);
+	}
+
+	group = iommu_group_get(dev);
+	if (!group) {
+		ret = -EINVAL;
+		goto err_put_pdev;
+	}
+
+	domain = iommu_get_domain_for_dev(dev);
+	if (!domain) {
+		ret = -EINVAL;
+		goto err_put_group;
+	}
+
+	info = devm_kzalloc(dev, sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		ret = -ENOMEM;
+		goto err_put_group;
+	}
+
+	init_rwsem(&info->rw_sem_self);
+	info->rw_sem = &info->rw_sem_self;
+	spin_lock_init(&info->dev_lock);
+	info->dev = dev;
+	info->pdev = pdev;
+	info->group = group;
+	info->domain = domain;
+	info->dev_active = NULL;
+	info->irq = platform_get_irq(pdev, 0);
+	info->got_irq = (info->irq < 0) ? false : true;
+
+	return info;
+
+err_put_group:
+	if (group)
+		iommu_group_put(group);
+err_put_pdev:
+	if (pdev)
+		platform_device_put(pdev);
+
+	return ERR_PTR(ret);
+}
+
+int rkvenc_iommu_remove(struct rkvenc_iommu_info *info)
+{
+	if (!info)
+		return 0;
+
+	iommu_group_put(info->group);
+	platform_device_put(info->pdev);
+
+	return 0;
+}
diff -ruN aa/drivers/media/platform/rockchip/rkvenc/rkvenc_service.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_service.c
--- aa/drivers/media/platform/rockchip/rkvenc/rkvenc_service.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_service.c	2026-02-09 17:54:57
@@ -0,0 +1,1206 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - MPP service char device and ioctl dispatch
+ * Ported from Rockchip BSP mpp_service.c / mpp_common.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ */
+
+#include <linux/cdev.h>
+#include <linux/delay.h>
+#include <linux/dma-buf.h>
+#include <linux/device/class.h>
+#include <linux/fs.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/poll.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/wait.h>
+
+#include "rkvenc_hw.h"
+
+/* ---- MSG v1 structure (from userspace) ---- */
+struct mpp_msg_v1 {
+	__u32 cmd;
+	__u32 flags;
+	__u32 size;
+	__u32 offset;
+	__u64 data_ptr;
+};
+
+/* ---- Session helpers ---- */
+static struct rkvenc_session *rkvenc_session_init(void)
+{
+	struct rkvenc_session *session;
+
+	session = kzalloc(sizeof(*session), GFP_KERNEL);
+	if (!session)
+		return NULL;
+
+	session->pid = current->pid;
+	mutex_init(&session->pending_lock);
+	INIT_LIST_HEAD(&session->pending_list);
+	INIT_LIST_HEAD(&session->service_link);
+	INIT_LIST_HEAD(&session->session_link);
+	atomic_set(&session->task_count, 0);
+	atomic_set(&session->release_request, 0);
+
+	return session;
+}
+
+static void rkvenc_session_deinit(struct rkvenc_session *session)
+{
+	if (!session)
+		return;
+
+	if (session->dma) {
+		rkvenc_dma_session_destroy(session->dma);
+		session->dma = NULL;
+	}
+
+	if (session->priv) {
+		kfree(session->priv);
+		session->priv = NULL;
+	}
+
+	kfree(session);
+}
+
+/* ---- Attach session to encoder device ---- */
+static int rkvenc_session_attach_device(struct rkvenc_session *session,
+					struct rkvenc_dev *mpp)
+{
+	session->mpp = mpp;
+	session->dma = rkvenc_dma_session_create(mpp->dev,
+						 mpp->session_max_buffers);
+	if (!session->dma) {
+		rkvenc_err("failed to create dma session\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+/* ---- Class msg alloc/free ---- */
+static int rkvenc_alloc_class_msg(struct rkvenc_task *task, u32 class)
+{
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	u32 base_s = hw->reg_msg[class].base_s;
+	u32 base_e = hw->reg_msg[class].base_e;
+	/*
+	 * BSP treats base_e as the last valid dword (inclusive), unlike the usual
+	 * half-open [base_s, base_e) convention. Keep this to match BSP IOCTL layout.
+	 */
+	u32 size = base_e - base_s + sizeof(u32);
+
+	if (task->reg[class].data)
+		return 0;
+
+	task->reg[class].data = kzalloc(size, GFP_KERNEL);
+	if (!task->reg[class].data)
+		return -ENOMEM;
+
+	task->reg[class].size = size;
+	return 0;
+}
+
+static void rkvenc_free_class_msg(struct rkvenc_task *task)
+{
+	int i;
+
+	for (i = 0; i < RKVENC_CLASS_BUTT; i++) {
+		kfree(task->reg[i].data);
+		task->reg[i].data = NULL;
+		task->reg[i].size = 0;
+		task->reg[i].valid = 0;
+	}
+}
+
+/* ---- Check if request overlaps a class ---- */
+static bool req_over_class(struct mpp_request *req,
+			   struct rkvenc_task *task, u32 class)
+{
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	/* BSP overlap check is inclusive; use last dword of request (offset+size-4). */
+	u32 req_e = req->offset + req->size - sizeof(u32);
+	u32 base_s = hw->reg_msg[class].base_s;
+	u32 base_e = hw->reg_msg[class].base_e;
+
+	return (req->offset <= base_e && req_e >= base_s);
+}
+
+/* ---- Update request to fit within class boundaries ---- */
+static void rkvenc_update_req(struct rkvenc_task *task, u32 class,
+			      struct mpp_request *src, struct mpp_request *dst)
+{
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	u32 base_s = hw->reg_msg[class].base_s;
+	u32 base_e = hw->reg_msg[class].base_e;
+	u32 req_s = src->offset;
+	/* Clamp to BSP's inclusive end to avoid truncating the last register. */
+	u32 req_e = src->offset + src->size - sizeof(u32);
+	u32 s = max(req_s, base_s);
+	u32 e = min(req_e, base_e);
+
+	dst->cmd = src->cmd;
+	dst->flags = src->flags;
+	dst->offset = s;
+	dst->size = e - s + sizeof(u32);
+	/* Adjust data pointer for the offset */
+	dst->data = (void __user *)((u8 __user *)src->data + (s - req_s));
+}
+
+/* ---- Extract RCB info ---- */
+static int rkvenc2_extract_rcb_info(struct rkvenc2_rcb_info *rcb_inf,
+				    struct mpp_request *req)
+{
+	int max_size = ARRAY_SIZE(rcb_inf->elem);
+	int cnt = req->size / sizeof(rcb_inf->elem[0]);
+
+	if (req->size > sizeof(rcb_inf->elem)) {
+		rkvenc_err("count %d, max_size %d\n", cnt, max_size);
+		return -EINVAL;
+	}
+	if (copy_from_user(rcb_inf->elem, req->data, req->size)) {
+		rkvenc_err("copy_from_user failed\n");
+		return -EINVAL;
+	}
+	rcb_inf->cnt = cnt;
+
+	return 0;
+}
+
+/* ---- Extract task messages from userspace ---- */
+static int rkvenc_extract_task_msg(struct rkvenc_session *session,
+				   struct rkvenc_task *task,
+				   struct rkvenc_task_msgs *msgs)
+{
+	int ret;
+	u32 i, j;
+	struct mpp_request *req;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+
+	for (i = 0; i < msgs->req_cnt; i++) {
+		req = &msgs->reqs[i];
+		if (!req->size)
+			continue;
+
+		switch (req->cmd) {
+		case MPP_CMD_SET_REG_WRITE: {
+			void *data;
+			struct mpp_request *wreq;
+
+			for (j = 0; j < hw->reg_class; j++) {
+				if (!req_over_class(req, task, j))
+					continue;
+
+				ret = rkvenc_alloc_class_msg(task, j);
+				if (ret) {
+					rkvenc_err("alloc class msg %d fail.\n", j);
+					goto fail;
+				}
+				wreq = &task->w_reqs[task->w_req_cnt];
+				rkvenc_update_req(task, j, req, wreq);
+				data = (u8 *)task->reg[j].data + (wreq->offset - hw->reg_msg[j].base_s);
+				if (!data) {
+					rkvenc_err("get class reg fail, offset %08x\n", wreq->offset);
+					ret = -EINVAL;
+					goto fail;
+				}
+				if (copy_from_user(data, wreq->data, wreq->size)) {
+					rkvenc_err("copy_from_user fail, offset %08x\n", wreq->offset);
+					ret = -EIO;
+					goto fail;
+				}
+				task->reg[j].valid = 1;
+				task->w_req_cnt++;
+			}
+		} break;
+		case MPP_CMD_SET_REG_READ: {
+			struct mpp_request *rreq;
+
+			for (j = 0; j < hw->reg_class; j++) {
+				if (!req_over_class(req, task, j))
+					continue;
+
+				ret = rkvenc_alloc_class_msg(task, j);
+				if (ret) {
+					rkvenc_err("alloc class msg reg %d fail.\n", j);
+					goto fail;
+				}
+				rreq = &task->r_reqs[task->r_req_cnt];
+				rkvenc_update_req(task, j, req, rreq);
+				task->reg[j].valid = 1;
+				task->r_req_cnt++;
+			}
+		} break;
+		case MPP_CMD_SET_REG_ADDR_OFFSET: {
+			rkvenc_extract_reg_offset_info(&task->off_inf, req);
+		} break;
+		case MPP_CMD_SET_RCB_INFO: {
+			struct rkvenc2_session_priv *priv = session->priv;
+
+			if (priv)
+				rkvenc2_extract_rcb_info(&priv->rcb_inf, req);
+		} break;
+		default:
+			break;
+		}
+	}
+
+	return 0;
+
+fail:
+	rkvenc_free_class_msg(task);
+	return ret;
+}
+
+/* ---- Get task format from register data ---- */
+static int rkvenc_task_get_format(struct rkvenc_dev *mpp,
+				  struct rkvenc_task *task)
+{
+	u32 offset, val;
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	u32 class = hw->fmt_reg.class;
+	u32 *class_reg = task->reg[class].data;
+	u32 class_size = task->reg[class].size;
+	u32 class_base = hw->reg_msg[class].base_s;
+	u32 bitpos = hw->fmt_reg.bitpos;
+	u32 bitlen = hw->fmt_reg.bitlen;
+
+	if (!class_reg || !class_size)
+		return -EINVAL;
+
+	offset = hw->fmt_reg.base - class_base;
+	val = class_reg[offset / sizeof(u32)];
+	task->fmt = (val >> bitpos) & ((1 << bitlen) - 1);
+
+	return 0;
+}
+
+/* ---- Setup task DCHS ID ---- */
+static void rkvenc2_setup_task_id(u32 session_id, struct rkvenc_task *task)
+{
+	const struct rkvenc_hw_info *hw = task->hw_info;
+	u32 val;
+
+	/* always enable tx */
+	val = task->reg[RKVENC_CLASS_PIC].data[hw->dcsh_class_ofst] | DCHS_TXE;
+	if (hw->dcsh_class_ofst)
+		task->reg[RKVENC_CLASS_PIC].data[hw->dcsh_class_ofst] = val;
+	task->dchs_id.val[0] = (((u64)session_id << 32) | val);
+
+	task->dchs_id.txid_orig = task->dchs_id.txid;
+	task->dchs_id.rxid_orig = task->dchs_id.rxid;
+	task->dchs_id.txid_map = task->dchs_id.txid;
+	task->dchs_id.rxid_map = task->dchs_id.rxid;
+
+	task->dchs_id.txe_orig = task->dchs_id.txe;
+	task->dchs_id.rxe_orig = task->dchs_id.rxe;
+	task->dchs_id.txe_map = task->dchs_id.txe;
+	task->dchs_id.rxe_map = task->dchs_id.rxe;
+}
+
+/* ---- Check for slice split task ---- */
+static void rkvenc2_check_split_task(struct rkvenc_dev *mpp, struct rkvenc_task *task)
+{
+	u32 slen_fifo_en = 0;
+	u32 sli_split_en = 0;
+
+	if (task->reg[RKVENC_CLASS_PIC].valid) {
+		u32 *reg = task->reg[RKVENC_CLASS_PIC].data;
+
+		slen_fifo_en = (reg[RKVENC2_REG_ENC_PIC] & RKVENC2_BIT_SLEN_FIFO) ? 1 : 0;
+		sli_split_en = (reg[RKVENC2_REG_SLI_SPLIT] & RKVENC2_BIT_SLI_SPLIT) ? 1 : 0;
+
+		/* H.264 bug: external line buffer + slice flush = bad */
+		if (sli_split_en && slen_fifo_en &&
+		    (reg[RKVENC2_REG_ENC_PIC] & RKVENC2_BIT_ENC_STND) == RKVENC2_BIT_VAL_H264 &&
+		    reg[RKVENC2_REG_EXT_LINE_BUF_BASE])
+			reg[RKVENC2_REG_SLI_SPLIT] &= ~RKVENC2_BIT_SLI_FLUSH;
+	}
+
+	task->task_split = sli_split_en && slen_fifo_en;
+
+	if (task->task_split)
+		INIT_KFIFO(task->slice_info);
+}
+
+/* ---- Set RCB buffer addresses from SRAM ---- */
+static void rkvenc2_set_rcbbuf(struct rkvenc_dev *enc,
+			       struct rkvenc_session *session,
+			       struct rkvenc_task *task)
+{
+	struct rkvenc2_session_priv *priv = session->priv;
+
+	if (priv && enc->sram_iova) {
+		int i;
+		u32 *reg;
+		u32 reg_idx, rcb_size, rcb_offset;
+		struct rkvenc2_rcb_info *rcb_inf = &priv->rcb_inf;
+
+		rcb_offset = 0;
+		for (i = 0; i < rcb_inf->cnt; i++) {
+			reg_idx = rcb_inf->elem[i].index;
+			rcb_size = rcb_inf->elem[i].size;
+
+			if (rcb_offset > enc->sram_size ||
+			    (rcb_offset + rcb_size) > enc->sram_used)
+				continue;
+
+			/* Get class reg for the RCB register index */
+			{
+				const struct rkvenc_hw_info *hw = task->hw_info;
+				int c;
+
+				for (c = 0; c < hw->reg_class; c++) {
+					u32 bs = hw->reg_msg[c].base_s;
+					u32 be = hw->reg_msg[c].base_e;
+					u32 addr = reg_idx * sizeof(u32);
+
+					if (addr >= bs && addr < be) {
+						reg = (u32 *)((u8 *)task->reg[c].data + (addr - bs));
+						*reg = enc->sram_iova + rcb_offset;
+						break;
+					}
+				}
+			}
+
+			rcb_offset += rcb_size;
+		}
+	}
+}
+
+/* ---- Alloc task from session ---- */
+static struct rkvenc_task *rkvenc_alloc_task(struct rkvenc_session *session,
+					     struct rkvenc_task_msgs *msgs)
+{
+	int ret;
+	struct rkvenc_task *task;
+	struct rkvenc_mpp_task *mpp_task;
+	struct rkvenc_dev *mpp = session->mpp;
+
+	task = kzalloc(sizeof(*task), GFP_KERNEL);
+	if (!task)
+		return NULL;
+
+	mpp_task = &task->mpp_task;
+	rkvenc_task_init(session, mpp_task);
+	mpp_task->hw_info = &mpp->hw_info->hw;
+	task->hw_info = mpp->hw_info;
+
+	/* extract reqs for current task */
+	ret = rkvenc_extract_task_msg(session, task, msgs);
+	if (ret)
+		goto free_task;
+	mpp_task->reg = task->reg[0].data;
+
+	/* get format */
+	ret = rkvenc_task_get_format(mpp, task);
+	if (ret)
+		goto free_task;
+
+	/* process fd in register */
+	if (!(msgs->flags & MPP_FLAGS_REG_FD_NO_TRANS)) {
+		u32 i, j;
+		int cnt;
+		u32 off;
+		const u16 *tbl;
+		const struct rkvenc_hw_info *hw = task->hw_info;
+		int fd_bs = -1;
+
+		for (i = 0; i < hw->fd_class; i++) {
+			u32 class = hw->fd_reg[i].class;
+			u32 fmt = hw->fd_reg[i].base_fmt + task->fmt;
+			u32 *reg = task->reg[class].data;
+			u32 ss = hw->reg_msg[class].base_s / sizeof(u32);
+			u32 mem_count_before;
+
+			if (!reg)
+				continue;
+
+			if (fmt == RKVENC_FMT_JPEGE && class == RKVENC_CLASS_PIC && fd_bs == -1) {
+				int bs_index = mpp->trans_info[fmt].table[2];
+
+				fd_bs = reg[bs_index];
+				task->offset_bs = rkvenc_query_reg_offset_info(&task->off_inf,
+									       bs_index + ss);
+			}
+
+			mem_count_before = mpp_task->mem_count;
+			ret = rkvenc_translate_reg_address(session, mpp_task, fmt, class, reg, NULL);
+			if (ret)
+				goto fail;
+
+			cnt = mpp->trans_info[fmt].count;
+			tbl = mpp->trans_info[fmt].table;
+			for (j = 0; j < cnt; j++) {
+				off = rkvenc_query_reg_offset_info(&task->off_inf, tbl[j] + ss);
+				reg[tbl[j]] += off;
+			}
+
+			/* Guardrail: ensure translated regs fall inside their mapped buffers */
+			{
+				u32 mc;
+
+				for (mc = mem_count_before; mc < mpp_task->mem_count; mc++) {
+					struct rkvenc_mem_region *mr = &mpp_task->mem_regions[mc];
+					u32 reg_size;
+					u32 reg_val;
+					unsigned long reg_off;
+					bool allow_end;
+					dma_addr_t end;
+					dma_addr_t reg_iova;
+
+					if (mr->reg_class != class)
+						continue;
+					if (!mr->len)
+						continue;
+
+					reg_size = task->reg[class].size / sizeof(u32);
+					if (mr->reg_idx >= reg_size) {
+						dev_err(mpp->dev,
+							"guardrail: class %u reg_idx %u out of range (%u dwords) fd %d\n",
+							class, mr->reg_idx, reg_size, mr->fd);
+						ret = -EINVAL;
+						goto fail;
+					}
+
+					reg_val = reg[mr->reg_idx];
+					reg_iova = (dma_addr_t)reg_val;
+					end = mr->iova + mr->len;
+					reg_off = rkvenc_query_reg_offset_info(&task->off_inf,
+									   mr->reg_idx + ss);
+					/* BSP encodes BSBT as base+size (end pointer); allow end-of-buffer when offset==len. */
+					allow_end = (reg_iova == end) && (reg_off == mr->len);
+					if (reg_iova < mr->iova || (!allow_end && reg_iova >= end)) {
+						dev_err(mpp->dev,
+							"guardrail: class %u reg[%u]=%#08x outside iova [%pad..%pad) fd %d\n",
+							class, mr->reg_idx, reg_val, &mr->iova, &end, mr->fd);
+						ret = -EINVAL;
+						goto fail;
+					}
+				}
+			}
+		}
+
+		if (fd_bs >= 0) {
+			struct rkvenc_dma_buffer *bs_buf =
+				rkvenc_dma_find_buffer_fd(session->dma, fd_bs);
+
+			if (bs_buf && task->offset_bs > 0)
+				rkvenc_dma_buf_sync(bs_buf, 0, task->offset_bs,
+						   DMA_TO_DEVICE, false);
+			task->bs_buf = bs_buf;
+		}
+	}
+
+	rkvenc2_setup_task_id(session->index, task);
+	task->clk_mode = CLK_MODE_NORMAL;
+	rkvenc2_check_split_task(mpp, task);
+
+	/* Init wait queue and reference count */
+	init_waitqueue_head(&mpp_task->wait);
+	kref_init(&mpp_task->ref);
+	atomic_set(&mpp_task->abort_request, 0);
+	mpp_task->task_index = atomic_inc_return(&mpp->task_index);
+	mpp_task->task_id = atomic_inc_return(&mpp->queue->task_id);
+	atomic_inc(&session->task_count);
+	atomic_inc(&mpp->task_count);
+
+	return task;
+
+fail:
+	rkvenc_task_finalize(session, mpp_task);
+	rkvenc_free_class_msg(task);
+free_task:
+	kfree(task);
+	return NULL;
+}
+
+/* ---- Result: copy status regs back to userspace ---- */
+static int rkvenc_result(struct rkvenc_dev *mpp,
+			 struct rkvenc_mpp_task *mpp_task)
+{
+	u32 i;
+	struct rkvenc_task *task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+
+	for (i = 0; i < task->r_req_cnt; i++) {
+		struct mpp_request *req = &task->r_reqs[i];
+		const struct rkvenc_hw_info *hw = task->hw_info;
+		u32 class_base;
+		u32 *reg;
+		int c;
+
+		/* Find class for this read request offset */
+		for (c = 0; c < hw->reg_class; c++) {
+			if (req->offset >= hw->reg_msg[c].base_s &&
+			    req->offset < hw->reg_msg[c].base_e) {
+				class_base = hw->reg_msg[c].base_s;
+				reg = (u32 *)((u8 *)task->reg[c].data + (req->offset - class_base));
+				break;
+			}
+		}
+
+		if (c == hw->reg_class) {
+			rkvenc_err("read request offset %x not in any class\n", req->offset);
+			return -EINVAL;
+		}
+
+		if (copy_to_user(req->data, reg, req->size)) {
+			rkvenc_err("copy_to_user reg fail\n");
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+/* ---- Task ISR bottom half (called from kthread worker) ---- */
+void rkvenc_task_worker_default(struct kthread_work *work)
+{
+	struct rkvenc_dev *enc = container_of(work, struct rkvenc_dev, work);
+	struct rkvenc_taskqueue *queue = enc->queue;
+	struct rkvenc_mpp_task *mpp_task;
+	struct rkvenc_task *task;
+	struct rkvenc_session *session;
+
+	/* Process finished tasks (ISR bottom half) */
+	mpp_task = enc->cur_task;
+	if (mpp_task && test_bit(TASK_STATE_IRQ, &mpp_task->state)) {
+		enc->cur_task = NULL;
+
+		task = container_of(mpp_task, struct rkvenc_task, mpp_task);
+		session = mpp_task->session;
+
+		task->irq_status = enc->irq_status;
+
+		/* Update DCHS state */
+		if (enc->ccu) {
+			unsigned long flags;
+
+			spin_lock_irqsave(&enc->ccu->lock_dchs, flags);
+			enc->ccu->dchs[enc->core_id].val[0] = 0;
+			enc->ccu->dchs[enc->core_id].val[1] = 0;
+			spin_unlock_irqrestore(&enc->ccu->lock_dchs, flags);
+		}
+
+		/* Check for errors */
+		if (task->irq_status & enc->hw_info->err_mask)
+			atomic_inc(&enc->reset_request);
+
+		rkvenc_task_finish(session, mpp_task);
+
+		set_bit(enc->core_id, &queue->core_idle);
+	}
+
+	/* Process timed-out tasks */
+	if (mpp_task && test_bit(TASK_STATE_TIMEOUT, &mpp_task->state)) {
+		enc->cur_task = NULL;
+
+		session = mpp_task->session;
+
+		rkvenc_err("task %d timeout, reset\n", mpp_task->task_index);
+		atomic_inc(&enc->reset_request);
+
+		rkvenc_task_finish(session, mpp_task);
+		set_bit(enc->core_id, &queue->core_idle);
+	}
+
+	/* Trigger pending tasks if core is idle */
+	{
+		struct rkvenc_mpp_task *pending_task = NULL;
+		unsigned long flags;
+		unsigned long core_idle;
+		s32 core_id;
+
+		spin_lock_irqsave(&queue->running_lock, flags);
+		core_idle = queue->core_idle;
+		core_id = find_first_bit(&core_idle, queue->core_id_max + 1);
+
+		if (core_id <= queue->core_id_max && queue->cores[core_id]) {
+			mutex_lock(&queue->pending_lock);
+			pending_task = list_first_entry_or_null(&queue->pending_list,
+								struct rkvenc_mpp_task,
+								queue_link);
+			if (pending_task) {
+				list_del_init(&pending_task->queue_link);
+				clear_bit(core_id, &queue->core_idle);
+				pending_task->mpp = queue->cores[core_id];
+				pending_task->core_id = core_id;
+			}
+			mutex_unlock(&queue->pending_lock);
+		}
+		spin_unlock_irqrestore(&queue->running_lock, flags);
+
+		if (pending_task) {
+			struct rkvenc_dev *target = pending_task->mpp;
+			struct rkvenc_task *enc_task = container_of(pending_task,
+								   struct rkvenc_task,
+								   mpp_task);
+			int run_ret;
+
+			/* Set RCB buffers if SRAM available */
+			rkvenc2_set_rcbbuf(target, pending_task->session, enc_task);
+
+			/* Run on hardware */
+			set_bit(TASK_STATE_RUNNING, &pending_task->state);
+			run_ret = rkvenc_hw_run(target, pending_task);
+			if (run_ret) {
+				dev_err(target->dev,
+					"hw_run failed: %d, failing task %d\n",
+					run_ret, pending_task->task_index);
+				set_bit(TASK_STATE_DONE, &pending_task->state);
+				set_bit(target->core_id, &queue->core_idle);
+				rkvenc_task_finish(pending_task->session,
+						  pending_task);
+				wake_up(&pending_task->wait);
+			}
+		}
+	}
+}
+
+/* ---- Process ioctl requests ---- */
+static int rkvenc_check_cmd(unsigned int cmd)
+{
+	if (cmd >= MPP_CMD_BUTT)
+		return -EINVAL;
+	return 0;
+}
+
+static int rkvenc_process_request(struct rkvenc_session *session,
+				  struct mpp_request *req,
+				  struct rkvenc_task_msgs *msgs)
+{
+	rkvenc_dbg(DEBUG_IOCTL, "cmd %x, size %d, offset %x\n",
+		   req->cmd, req->size, req->offset);
+
+	switch (req->cmd) {
+	case MPP_CMD_QUERY_HW_SUPPORT: {
+		u32 val = session->srv->hw_support;
+
+		if (put_user(val, (u32 __user *)req->data))
+			return -EFAULT;
+	} break;
+	case MPP_CMD_QUERY_HW_ID: {
+		struct rkvenc_dev *mpp = session->mpp;
+		u32 val = mpp ? mpp->hw_info->hw.hw_id : 0;
+
+		if (put_user(val, (u32 __user *)req->data))
+			return -EFAULT;
+	} break;
+	case MPP_CMD_INIT_CLIENT_TYPE: {
+		u32 client_type;
+
+		if (get_user(client_type, (u32 __user *)req->data))
+			return -EFAULT;
+
+		session->device_type = client_type;
+
+		/* Attach to the encoder device */
+		if (!session->mpp) {
+			struct rkvenc_dev *dev =
+				session->srv->sub_devices[MPP_DEVICE_RKVENC];
+
+			if (dev)
+				rkvenc_session_attach_device(session, dev);
+		}
+
+		/* Init session private data */
+		if (!session->priv) {
+			struct rkvenc2_session_priv *priv;
+
+			priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+			if (priv) {
+				init_rwsem(&priv->rw_sem);
+				session->priv = priv;
+			}
+		}
+	} break;
+	case MPP_CMD_INIT_TRANS_TABLE: {
+		int cnt = req->size / sizeof(u16);
+
+		if (cnt > MPP_MAX_REG_TRANS_NUM) {
+			rkvenc_err("trans count %d too large\n", cnt);
+			return -EINVAL;
+		}
+		if (copy_from_user(session->trans_table, req->data, req->size)) {
+			rkvenc_err("copy_from_user trans_table failed\n");
+			return -EFAULT;
+		}
+		session->trans_count = cnt;
+	} break;
+	case MPP_CMD_SET_REG_WRITE:
+	case MPP_CMD_SET_REG_READ:
+	case MPP_CMD_SET_REG_ADDR_OFFSET:
+	case MPP_CMD_SET_RCB_INFO: {
+		msgs->set_cnt++;
+	} break;
+	case MPP_CMD_POLL_HW_FINISH: {
+		msgs->poll_cnt++;
+		msgs->poll_req = req;
+	} break;
+	case MPP_CMD_TRANS_FD_TO_IOVA: {
+		/* Static FD->IOVA translation for buffer pre-import */
+		if (session->mpp && session->dma) {
+			u32 fd;
+
+			if (get_user(fd, (u32 __user *)req->data))
+				return -EFAULT;
+
+			rkvenc_iommu_down_read(session->mpp->iommu_info);
+			rkvenc_dma_import_fd(session->mpp->iommu_info, session->dma, fd, 1);
+			rkvenc_iommu_up_read(session->mpp->iommu_info);
+		}
+	} break;
+	case MPP_CMD_RELEASE_FD: {
+		if (session->dma) {
+			u32 fd;
+
+			if (get_user(fd, (u32 __user *)req->data))
+				return -EFAULT;
+			rkvenc_dma_release_fd(session->dma, fd);
+		}
+	} break;
+	case MPP_CMD_SEND_CODEC_INFO: {
+		/* Codec info is optional, just store it */
+		if (session->priv) {
+			int ci;
+			int cnt;
+			struct codec_info_elem elem;
+			struct rkvenc2_session_priv *priv = session->priv;
+
+			cnt = req->size / sizeof(elem);
+			cnt = (cnt > ENC_INFO_BUTT) ? ENC_INFO_BUTT : cnt;
+			for (ci = 0; ci < cnt; ci++) {
+				if (copy_from_user(&elem,
+						   (void __user *)req->data + ci * sizeof(elem),
+						   sizeof(elem)))
+					continue;
+				if (elem.type > ENC_INFO_BASE && elem.type < ENC_INFO_BUTT) {
+					priv->codec_info[elem.type].flag = elem.flag;
+					priv->codec_info[elem.type].val = elem.data;
+				}
+			}
+		}
+	} break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+/* ---- Collect messages from one ioctl call ---- */
+static int rkvenc_collect_msgs(struct rkvenc_session *session,
+			       unsigned int cmd, void __user *msg,
+			       struct rkvenc_task_msgs *msgs)
+{
+	struct mpp_msg_v1 msg_v1;
+	struct mpp_request *req;
+	int last = 1;
+	int ret;
+
+	if (cmd != MPP_IOC_CFG_V1) {
+		rkvenc_err("unknown ioctl cmd %x\n", cmd);
+		return -EINVAL;
+	}
+
+next:
+	if (copy_from_user(&msg_v1, msg, sizeof(msg_v1)))
+		return -EFAULT;
+
+	msg += sizeof(msg_v1);
+
+	if (rkvenc_check_cmd(msg_v1.cmd)) {
+		rkvenc_err("cmd %x not supported\n", msg_v1.cmd);
+		return -EFAULT;
+	}
+
+	if (msg_v1.flags & MPP_FLAGS_MULTI_MSG)
+		last = (msg_v1.flags & MPP_FLAGS_LAST_MSG) ? 1 : 0;
+	else
+		last = 1;
+
+	if (msgs->req_cnt >= MPP_MAX_MSG_NUM) {
+		rkvenc_err("message count %d more than %d\n",
+			   msgs->req_cnt, MPP_MAX_MSG_NUM);
+		return -EINVAL;
+	}
+
+	req = &msgs->reqs[msgs->req_cnt++];
+	req->cmd = msg_v1.cmd;
+	req->flags = msg_v1.flags;
+	req->size = msg_v1.size;
+	req->offset = msg_v1.offset;
+	req->data = (void __user *)(unsigned long)msg_v1.data_ptr;
+
+	/* Update session flags */
+	session->msg_flags = msg_v1.flags;
+	msgs->flags = msg_v1.flags;
+
+	ret = rkvenc_process_request(session, req, msgs);
+	if (ret) {
+		rkvenc_err("process cmd %x ret %d\n", req->cmd, ret);
+		return ret;
+	}
+
+	if (!last)
+		goto next;
+
+	return 0;
+}
+
+/* ---- Wait for task result ---- */
+static int rkvenc_wait_result(struct rkvenc_session *session,
+			      struct rkvenc_task_msgs *msgs)
+{
+	struct rkvenc_mpp_task *task;
+	struct rkvenc_task *enc_task;
+	union rkvenc2_slice_len_info slice_info;
+	int ret = 0;
+
+	mutex_lock(&session->pending_lock);
+	task = list_first_entry_or_null(&session->pending_list,
+					struct rkvenc_mpp_task,
+					pending_link);
+	mutex_unlock(&session->pending_lock);
+
+	if (!task) {
+		rkvenc_err("session %p pending list is empty!\n", session);
+		return -EIO;
+	}
+
+	enc_task = container_of(task, struct rkvenc_task, mpp_task);
+
+	if (!enc_task->task_split || enc_task->task_split_done) {
+task_done_ret:
+		ret = wait_event_interruptible(task->wait,
+					       test_bit(TASK_STATE_DONE, &task->state));
+		if (ret == -ERESTARTSYS)
+			rkvenc_err("wait task break by signal\n");
+
+		/* Copy results to userspace */
+		rkvenc_result(task->mpp, task);
+
+		/* Pop from pending list */
+		mutex_lock(&session->pending_lock);
+		list_del_init(&task->pending_link);
+		mutex_unlock(&session->pending_lock);
+
+		kref_put(&task->ref, rkvenc_free_task_callback);
+
+		return ret;
+	}
+
+	/* Slice split mode: wait for all slices */
+	do {
+		ret = wait_event_interruptible(task->wait,
+					       kfifo_out(&enc_task->slice_info,
+							 &slice_info, 1));
+		if (ret == -ERESTARTSYS) {
+			rkvenc_err("wait task break by signal in slice mode\n");
+			return 0;
+		}
+
+		enc_task->slice_rd_cnt++;
+
+		if (slice_info.last)
+			goto task_done_ret;
+	} while (1);
+}
+
+/* ---- Main ioctl handler ---- */
+static long rkvenc_dev_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct rkvenc_session *session = filp->private_data;
+	struct rkvenc_task_msgs msgs;
+	struct rkvenc_task *task;
+	int ret = 0;
+
+	if (!session || !session->srv) {
+		rkvenc_err("session %p\n", session);
+		return -EINVAL;
+	}
+
+	if (atomic_read(&session->release_request) > 0)
+		return -EBUSY;
+	if (atomic_read(&session->srv->shutdown_request) > 0)
+		return -EBUSY;
+
+	/* Init msgs */
+	memset(&msgs, 0, sizeof(msgs));
+	INIT_LIST_HEAD(&msgs.list);
+	msgs.session = session;
+
+	/* Phase 1: Collect all messages from the ioctl */
+	ret = rkvenc_collect_msgs(session, cmd, (void __user *)arg, &msgs);
+	if (ret) {
+		rkvenc_err("collect msgs failed %d\n", ret);
+		return ret;
+	}
+
+	/* Phase 2: If we have SET requests, create a task and submit */
+	if (msgs.set_cnt && session->mpp) {
+		struct rkvenc_taskqueue *queue = session->mpp->queue;
+
+		task = rkvenc_alloc_task(session, &msgs);
+		if (!task) {
+			rkvenc_err("alloc task failed\n");
+			return -ENOMEM;
+		}
+
+		msgs.task = &task->mpp_task;
+		msgs.mpp = session->mpp;
+		msgs.queue = queue;
+
+		/* Add to pending list */
+		kref_get(&task->mpp_task.ref);
+		mutex_lock(&session->pending_lock);
+		list_add_tail(&task->mpp_task.pending_link, &session->pending_list);
+		mutex_unlock(&session->pending_lock);
+
+		/* Add to task queue and trigger worker */
+		set_bit(TASK_STATE_PENDING, &task->mpp_task.state);
+		mutex_lock(&queue->pending_lock);
+		list_add_tail(&task->mpp_task.queue_link, &queue->pending_list);
+		mutex_unlock(&queue->pending_lock);
+
+		/* Trigger the worker to run the task */
+		kthread_queue_work(&queue->worker, &session->mpp->work);
+	}
+
+	/* Phase 3: If we have POLL requests, wait for result */
+	if (msgs.poll_cnt) {
+		ret = rkvenc_wait_result(session, &msgs);
+		if (ret)
+			rkvenc_err("wait result ret %d\n", ret);
+	}
+
+	return ret;
+}
+
+/* ---- File operations ---- */
+static int rkvenc_dev_open(struct inode *inode, struct file *filp)
+{
+	struct rkvenc_service *srv = container_of(inode->i_cdev,
+						  struct rkvenc_service,
+						  mpp_cdev);
+	struct rkvenc_session *session;
+
+	session = rkvenc_session_init();
+	if (!session)
+		return -ENOMEM;
+
+	session->srv = srv;
+
+	if (!srv->sub_devices[MPP_DEVICE_RKVENC]) {
+		rkvenc_session_deinit(session);
+		return -ENODEV;
+	}
+
+	mutex_lock(&srv->session_lock);
+	list_add_tail(&session->service_link, &srv->session_list);
+	session->index = atomic_inc_return(&srv->sub_devices[MPP_DEVICE_RKVENC]->session_index);
+	mutex_unlock(&srv->session_lock);
+
+	filp->private_data = session;
+
+	return nonseekable_open(inode, filp);
+}
+
+static int rkvenc_dev_release(struct inode *inode, struct file *filp)
+{
+	struct rkvenc_session *session = filp->private_data;
+
+	if (!session) {
+		rkvenc_err("session is null\n");
+		return -EINVAL;
+	}
+
+	atomic_inc(&session->release_request);
+
+	/* Remove from service list */
+	if (session->srv) {
+		mutex_lock(&session->srv->session_lock);
+		list_del_init(&session->service_link);
+		mutex_unlock(&session->srv->session_lock);
+	}
+
+	/* Wait for all tasks to complete */
+	if (atomic_read(&session->task_count) > 0) {
+		struct rkvenc_mpp_task *task, *n;
+
+		mutex_lock(&session->pending_lock);
+		list_for_each_entry_safe(task, n, &session->pending_list, pending_link) {
+			/* Wait for task completion with timeout */
+			wait_event_timeout(task->wait,
+					   test_bit(TASK_STATE_DONE, &task->state),
+					   msecs_to_jiffies(2000));
+			list_del_init(&task->pending_link);
+			kref_put(&task->ref, rkvenc_free_task_callback);
+		}
+		mutex_unlock(&session->pending_lock);
+	}
+
+	rkvenc_session_deinit(session);
+	filp->private_data = NULL;
+
+	return 0;
+}
+
+const struct file_operations rkvenc_fops = {
+	.open		= rkvenc_dev_open,
+	.release	= rkvenc_dev_release,
+	.unlocked_ioctl = rkvenc_dev_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl   = rkvenc_dev_ioctl,
+#endif
+};
+
+/* ---- Service probe/remove ---- */
+int rkvenc_service_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct rkvenc_service *srv;
+	struct device *dev = &pdev->dev;
+	u32 taskqueue_cnt = 0;
+	u32 resetgroup_cnt = 0;
+
+	srv = devm_kzalloc(dev, sizeof(*srv), GFP_KERNEL);
+	if (!srv)
+		return -ENOMEM;
+
+	srv->dev = dev;
+	mutex_init(&srv->session_lock);
+	INIT_LIST_HEAD(&srv->session_list);
+	atomic_set(&srv->shutdown_request, 0);
+
+	/* Read DTS properties */
+	of_property_read_u32(dev->of_node, "rockchip,taskqueue-count", &taskqueue_cnt);
+	of_property_read_u32(dev->of_node, "rockchip,resetgroup-count", &resetgroup_cnt);
+	srv->taskqueue_cnt = taskqueue_cnt;
+	srv->reset_group_cnt = resetgroup_cnt;
+
+	/* Create the task queue for RKVENC */
+	{
+		struct rkvenc_taskqueue *queue;
+
+		queue = devm_kzalloc(dev, sizeof(*queue), GFP_KERNEL);
+		if (!queue)
+			return -ENOMEM;
+
+		mutex_init(&queue->session_lock);
+		mutex_init(&queue->pending_lock);
+		spin_lock_init(&queue->running_lock);
+		mutex_init(&queue->dev_lock);
+		INIT_LIST_HEAD(&queue->session_attach);
+		INIT_LIST_HEAD(&queue->session_detach);
+		INIT_LIST_HEAD(&queue->pending_list);
+		INIT_LIST_HEAD(&queue->running_list);
+		INIT_LIST_HEAD(&queue->dev_list);
+		atomic_set(&queue->reset_request, 0);
+		atomic_set(&queue->detach_count, 0);
+		atomic_set(&queue->task_id, 0);
+		queue->core_idle = (unsigned long)-1;
+
+		/* Create kthread worker */
+		kthread_init_worker(&queue->worker);
+		queue->kworker_task = kthread_run(kthread_worker_fn,
+						  &queue->worker,
+						  "rkvenc-worker");
+		if (IS_ERR(queue->kworker_task)) {
+			dev_err(dev, "failed to create kthread worker\n");
+			return PTR_ERR(queue->kworker_task);
+		}
+
+		srv->task_queues[MPP_DEVICE_RKVENC] = queue;
+	}
+
+	/* Create reset group */
+	if (resetgroup_cnt > 0) {
+		struct rkvenc_reset_group *rg;
+
+		rg = devm_kzalloc(dev, sizeof(*rg), GFP_KERNEL);
+		if (!rg)
+			return -ENOMEM;
+
+		init_rwsem(&rg->rw_sem);
+		rg->rw_sem_on = true;
+		srv->reset_groups[0] = rg;
+	}
+
+	/* Allocate char device */
+	ret = alloc_chrdev_region(&srv->dev_id, 0, 1, MPP_SERVICE_NAME);
+	if (ret) {
+		dev_err(dev, "alloc_chrdev_region failed: %d\n", ret);
+		return ret;
+	}
+
+	cdev_init(&srv->mpp_cdev, &rkvenc_fops);
+	srv->mpp_cdev.owner = THIS_MODULE;
+
+	ret = cdev_add(&srv->mpp_cdev, srv->dev_id, 1);
+	if (ret) {
+		dev_err(dev, "cdev_add failed: %d\n", ret);
+		goto err_cdev;
+	}
+
+	srv->cls = class_create(MPP_CLASS_NAME);
+	if (IS_ERR(srv->cls)) {
+		ret = PTR_ERR(srv->cls);
+		dev_err(dev, "class_create failed: %d\n", ret);
+		goto err_class;
+	}
+
+	srv->child_dev = device_create(srv->cls, dev, srv->dev_id,
+				       NULL, MPP_SERVICE_NAME);
+	if (IS_ERR(srv->child_dev)) {
+		ret = PTR_ERR(srv->child_dev);
+		dev_err(dev, "device_create failed: %d\n", ret);
+		goto err_device;
+	}
+
+	platform_set_drvdata(pdev, srv);
+	dev_info(dev, "mpp_service probe success\n");
+
+	return 0;
+
+err_device:
+	class_destroy(srv->cls);
+err_class:
+	cdev_del(&srv->mpp_cdev);
+err_cdev:
+	unregister_chrdev_region(srv->dev_id, 1);
+	return ret;
+}
+
+int rkvenc_service_remove(struct platform_device *pdev)
+{
+	struct rkvenc_service *srv = platform_get_drvdata(pdev);
+
+	if (!srv)
+		return 0;
+
+	/* Stop kthread worker */
+	if (srv->task_queues[MPP_DEVICE_RKVENC]) {
+		struct rkvenc_taskqueue *queue = srv->task_queues[MPP_DEVICE_RKVENC];
+
+		if (queue->kworker_task) {
+			kthread_flush_worker(&queue->worker);
+			kthread_stop(queue->kworker_task);
+		}
+	}
+
+	device_destroy(srv->cls, srv->dev_id);
+	class_destroy(srv->cls);
+	cdev_del(&srv->mpp_cdev);
+	unregister_chrdev_region(srv->dev_id, 1);
+
+	return 0;
+}
diff -ruN aa/drivers/media/platform/rockchip/rkvenc/rkvenc_task.c bb/drivers/media/platform/rockchip/rkvenc/rkvenc_task.c
--- aa/drivers/media/platform/rockchip/rkvenc/rkvenc_task.c	1969-12-31 16:00:00
+++ bb/drivers/media/platform/rockchip/rkvenc/rkvenc_task.c	2026-02-07 21:27:09
@@ -0,0 +1,300 @@
+// SPDX-License-Identifier: (GPL-2.0+ OR MIT)
+/*
+ * Rockchip VEPU580 encoder driver - Task lifecycle management
+ * Ported from Rockchip BSP mpp_common.c
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ */
+
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+
+#include "rkvenc_hw.h"
+
+/* ---- Task init/finish/finalize ---- */
+int rkvenc_task_init(struct rkvenc_session *session, struct rkvenc_mpp_task *task)
+{
+	INIT_LIST_HEAD(&task->pending_link);
+	INIT_LIST_HEAD(&task->queue_link);
+	INIT_LIST_HEAD(&task->mem_region_list);
+	task->state = 0;
+	task->mem_count = 0;
+	task->session = session;
+
+	return 0;
+}
+
+static struct rkvenc_dev *rkvenc_get_task_used_device(struct rkvenc_mpp_task *task,
+						     struct rkvenc_session *session)
+{
+	if (task->mpp)
+		return task->mpp;
+	return session->mpp;
+}
+
+void rkvenc_free_task_callback(struct kref *ref)
+{
+	struct rkvenc_mpp_task *task = container_of(ref, struct rkvenc_mpp_task, ref);
+	struct rkvenc_session *session;
+	struct rkvenc_dev *mpp;
+	struct rkvenc_task *enc_task = container_of(task, struct rkvenc_task, mpp_task);
+
+	if (!task->session) {
+		rkvenc_err("task %p, task->session is null.\n", task);
+		return;
+	}
+	session = task->session;
+	mpp = rkvenc_get_task_used_device(task, session);
+
+	/* Release memory regions and free class register buffers */
+	rkvenc_task_finalize(session, task);
+
+	/* Free class register buffers */
+	{
+		int i;
+
+		for (i = 0; i < RKVENC_CLASS_BUTT; i++) {
+			kfree(enc_task->reg[i].data);
+			enc_task->reg[i].data = NULL;
+			enc_task->reg[i].size = 0;
+			enc_task->reg[i].valid = 0;
+		}
+	}
+
+	atomic_dec(&session->task_count);
+	atomic_dec(&mpp->task_count);
+
+	kfree(enc_task);
+}
+
+int rkvenc_task_finish(struct rkvenc_session *session,
+		       struct rkvenc_mpp_task *task)
+{
+	struct rkvenc_dev *mpp = rkvenc_get_task_used_device(task, session);
+	struct rkvenc_taskqueue *queue = mpp->queue;
+
+	/* Read status registers back from HW */
+	rkvenc_hw_finish(mpp, task);
+
+	/* Handle reset if needed */
+	if (mpp->reset_group) {
+		up_read(&mpp->reset_group->rw_sem);
+		if (atomic_read(&mpp->reset_request) > 0)
+			rkvenc_hw_reset(mpp);
+	}
+
+	/* Power off encoder and its IOMMU */
+	rkvenc_hw_clk_off(mpp);
+	pm_relax(mpp->dev);
+	pm_runtime_mark_last_busy(mpp->dev);
+	pm_runtime_put_autosuspend(mpp->dev);
+	if (mpp->iommu_info && mpp->iommu_info->pdev)
+		pm_runtime_put_sync(&mpp->iommu_info->pdev->dev);
+
+	set_bit(TASK_STATE_FINISH, &task->state);
+	set_bit(TASK_STATE_DONE, &task->state);
+
+	/* Wake up the GET thread */
+	wake_up(&task->wait);
+
+	/* Pop from running queue */
+	{
+		unsigned long flags;
+
+		spin_lock_irqsave(&queue->running_lock, flags);
+		list_del_init(&task->queue_link);
+		spin_unlock_irqrestore(&queue->running_lock, flags);
+		kref_put(&task->ref, rkvenc_free_task_callback);
+	}
+
+	return 0;
+}
+
+int rkvenc_task_finalize(struct rkvenc_session *session,
+			 struct rkvenc_mpp_task *task)
+{
+	struct rkvenc_mem_region *mem_region = NULL, *n;
+	struct rkvenc_dev *mpp = rkvenc_get_task_used_device(task, session);
+
+	list_for_each_entry_safe(mem_region, n, &task->mem_region_list, reg_link) {
+		if (!mem_region->is_dup) {
+			rkvenc_iommu_down_read(mpp->iommu_info);
+			rkvenc_dma_release(session->dma, mem_region->hdl);
+			rkvenc_iommu_up_read(mpp->iommu_info);
+		}
+		list_del_init(&mem_region->reg_link);
+	}
+
+	return 0;
+}
+
+/* ---- Memory region attach ---- */
+static struct rkvenc_mem_region *
+rkvenc_task_attach_fd(struct rkvenc_mpp_task *task, int fd)
+{
+	struct rkvenc_mem_region *mem_region = NULL, *loop = NULL, *n;
+	struct rkvenc_dma_buffer *buffer = NULL;
+	struct rkvenc_dev *mpp = task->session->mpp;
+	struct rkvenc_dma_session *dma = task->session->dma;
+	u32 mem_num = ARRAY_SIZE(task->mem_regions);
+	bool found = false;
+
+	if (fd <= 0 || !dma || !mpp)
+		return ERR_PTR(-EINVAL);
+
+	if (task->mem_count > mem_num) {
+		rkvenc_err("mem_count %d must less than %d\n", task->mem_count, mem_num);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	/* find fd whether had import */
+	list_for_each_entry_safe_reverse(loop, n, &task->mem_region_list, reg_link) {
+		if (loop->fd == fd) {
+			found = true;
+			break;
+		}
+	}
+
+	mem_region = &task->mem_regions[task->mem_count];
+	if (found) {
+		memcpy(mem_region, loop, sizeof(*loop));
+		mem_region->reg_class = 0;
+		mem_region->is_dup = true;
+	} else {
+		rkvenc_iommu_down_read(mpp->iommu_info);
+		buffer = rkvenc_dma_import_fd(mpp->iommu_info, dma, fd, 0);
+		rkvenc_iommu_up_read(mpp->iommu_info);
+		if (IS_ERR(buffer)) {
+			rkvenc_err("can't import dma-buf %d\n", fd);
+			return ERR_CAST(buffer);
+		}
+
+		mem_region->hdl = buffer;
+		mem_region->iova = buffer->iova;
+		mem_region->len = buffer->size;
+		mem_region->fd = fd;
+		mem_region->reg_class = 0;
+		mem_region->is_dup = false;
+	}
+	task->mem_count++;
+	INIT_LIST_HEAD(&mem_region->reg_link);
+	list_add_tail(&mem_region->reg_link, &task->mem_region_list);
+
+	return mem_region;
+}
+
+/* ---- FD translation ---- */
+int rkvenc_translate_reg_address(struct rkvenc_session *session,
+				 struct rkvenc_mpp_task *task, int fmt, u32 reg_class,
+				 u32 *reg, struct reg_offset_info *off_inf)
+{
+	int i;
+	int cnt;
+	const u16 *tbl;
+	struct rkvenc_dev *mpp = rkvenc_get_task_used_device(task, session);
+
+	if (session->trans_count > 0) {
+		cnt = session->trans_count;
+		tbl = session->trans_table;
+	} else {
+		cnt = mpp->trans_info[fmt].count;
+		tbl = mpp->trans_info[fmt].table;
+	}
+
+	for (i = 0; i < cnt; i++) {
+		int usr_fd;
+		u32 offset;
+		struct rkvenc_mem_region *mem_region;
+
+		if (session->msg_flags & MPP_FLAGS_REG_NO_OFFSET) {
+			usr_fd = reg[tbl[i]];
+			offset = 0;
+		} else {
+			usr_fd = reg[tbl[i]] & 0x3ff;
+			offset = reg[tbl[i]] >> 10;
+		}
+
+		if (usr_fd == 0)
+			continue;
+
+		mem_region = rkvenc_task_attach_fd(task, usr_fd);
+		if (IS_ERR(mem_region)) {
+			rkvenc_err("reg[%3d]: 0x%08x fd %d failed\n",
+				   tbl[i], reg[tbl[i]], usr_fd);
+			return PTR_ERR(mem_region);
+		}
+		mem_region->reg_class = reg_class;
+		mem_region->reg_idx = tbl[i];
+		reg[tbl[i]] = mem_region->iova + offset;
+	}
+
+	return 0;
+}
+
+int rkvenc_extract_reg_offset_info(struct reg_offset_info *off_inf,
+				   struct mpp_request *req)
+{
+	int max_size = ARRAY_SIZE(off_inf->elem);
+	int cnt = req->size / sizeof(off_inf->elem[0]);
+
+	if ((cnt + off_inf->cnt) > max_size) {
+		rkvenc_err("count %d, total %d, max_size %d\n",
+			   cnt, off_inf->cnt, max_size);
+		return -EINVAL;
+	}
+	if (copy_from_user(&off_inf->elem[off_inf->cnt], req->data, req->size)) {
+		rkvenc_err("copy_from_user failed\n");
+		return -EINVAL;
+	}
+	off_inf->cnt += cnt;
+
+	return 0;
+}
+
+int rkvenc_query_reg_offset_info(struct reg_offset_info *off_inf, u32 index)
+{
+	if (off_inf) {
+		int i;
+
+		for (i = 0; i < off_inf->cnt; i++) {
+			if (off_inf->elem[i].index == index)
+				return off_inf->elem[i].offset;
+		}
+	}
+
+	return 0;
+}
+
+/* ---- Task timeout ---- */
+void rkvenc_task_timeout_work(struct work_struct *work_s)
+{
+	struct rkvenc_mpp_task *task = container_of(to_delayed_work(work_s),
+						   struct rkvenc_mpp_task,
+						   timeout_work);
+	struct rkvenc_dev *mpp;
+	struct rkvenc_session *session = task->session;
+
+	if (!session) {
+		rkvenc_err("task %p, task->session is null.\n", task);
+		return;
+	}
+
+	mpp = rkvenc_get_task_used_device(task, session);
+	if (!mpp) {
+		rkvenc_err("mpp is null\n");
+		return;
+	}
+
+	disable_irq(mpp->irq);
+	if (test_and_set_bit(TASK_STATE_HANDLE, &task->state)) {
+		enable_irq(mpp->irq);
+		return;
+	}
+	rkvenc_err("task %d processing time out!\n", task->task_index);
+	set_bit(TASK_STATE_TIMEOUT, &task->state);
+	enable_irq(mpp->irq);
+
+	kthread_queue_work(&mpp->queue->worker, &mpp->work);
+}
Binary files aa/drivers/media/platform/synopsys/.DS_Store and bb/drivers/media/platform/synopsys/.DS_Store differ
Binary files aa/include/.DS_Store and bb/include/.DS_Store differ
Binary files aa/include/uapi/.DS_Store and bb/include/uapi/.DS_Store differ
diff -ruN aa/include/uapi/linux/rkvenc.h bb/include/uapi/linux/rkvenc.h
--- aa/include/uapi/linux/rkvenc.h	1969-12-31 16:00:00
+++ bb/include/uapi/linux/rkvenc.h	2026-02-07 01:32:43
@@ -0,0 +1,84 @@
+/* SPDX-License-Identifier: ((GPL-2.0+ WITH Linux-syscall-note) OR MIT) */
+/*
+ * Rockchip MPP service driver - UAPI definitions
+ * Ported from the Rockchip BSP kernel rk-mpp.h
+ *
+ * Copyright (C) 2023 Rockchip Electronics Co., Ltd.
+ */
+
+#ifndef _UAPI_RKVENC_H
+#define _UAPI_RKVENC_H
+
+#include <linux/types.h>
+
+/* Use 'v' as magic number */
+#define MPP_IOC_MAGIC			'v'
+
+#define MPP_IOC_CFG_V1			_IOW(MPP_IOC_MAGIC, 1, unsigned int)
+#define MPP_IOC_CFG_V2			_IOW(MPP_IOC_MAGIC, 2, unsigned int)
+
+/**
+ * Command type: keep the same as user space
+ */
+enum MPP_DEV_COMMAND_TYPE {
+	MPP_CMD_QUERY_BASE		= 0,
+	MPP_CMD_QUERY_HW_SUPPORT	= MPP_CMD_QUERY_BASE + 0,
+	MPP_CMD_QUERY_HW_ID		= MPP_CMD_QUERY_BASE + 1,
+	MPP_CMD_QUERY_CMD_SUPPORT	= MPP_CMD_QUERY_BASE + 2,
+	MPP_CMD_QUERY_BUTT,
+
+	MPP_CMD_INIT_BASE		= 0x100,
+	MPP_CMD_INIT_CLIENT_TYPE	= MPP_CMD_INIT_BASE + 0,
+	MPP_CMD_INIT_DRIVER_DATA	= MPP_CMD_INIT_BASE + 1,
+	MPP_CMD_INIT_TRANS_TABLE	= MPP_CMD_INIT_BASE + 2,
+	MPP_CMD_INIT_BUTT,
+
+	MPP_CMD_SEND_BASE		= 0x200,
+	MPP_CMD_SET_REG_WRITE		= MPP_CMD_SEND_BASE + 0,
+	MPP_CMD_SET_REG_READ		= MPP_CMD_SEND_BASE + 1,
+	MPP_CMD_SET_REG_ADDR_OFFSET	= MPP_CMD_SEND_BASE + 2,
+	MPP_CMD_SET_RCB_INFO		= MPP_CMD_SEND_BASE + 3,
+	MPP_CMD_SET_SESSION_FD		= MPP_CMD_SEND_BASE + 4,
+	MPP_CMD_SEND_BUTT,
+
+	MPP_CMD_POLL_BASE		= 0x300,
+	MPP_CMD_POLL_HW_FINISH		= MPP_CMD_POLL_BASE + 0,
+	MPP_CMD_POLL_HW_IRQ		= MPP_CMD_POLL_BASE + 1,
+	MPP_CMD_POLL_BUTT,
+
+	MPP_CMD_CONTROL_BASE		= 0x400,
+	MPP_CMD_RESET_SESSION		= MPP_CMD_CONTROL_BASE + 0,
+	MPP_CMD_TRANS_FD_TO_IOVA	= MPP_CMD_CONTROL_BASE + 1,
+	MPP_CMD_RELEASE_FD		= MPP_CMD_CONTROL_BASE + 2,
+	MPP_CMD_SEND_CODEC_INFO		= MPP_CMD_CONTROL_BASE + 3,
+	MPP_CMD_CONTROL_BUTT,
+
+	MPP_CMD_BUTT,
+};
+
+/* define flags for mpp_request */
+#define MPP_FLAGS_MULTI_MSG		(0x00000001)
+#define MPP_FLAGS_LAST_MSG		(0x00000002)
+#define MPP_FLAGS_REG_FD_NO_TRANS	(0x00000004)
+#define MPP_FLAGS_SCL_FD_NO_TRANS	(0x00000008)
+#define MPP_FLAGS_REG_NO_OFFSET		(0x00000010)
+#define MPP_FLAGS_SECURE_MODE		(0x00010000)
+
+/* data common struct for parse out */
+struct mpp_request {
+	__u32 cmd;
+	__u32 flags;
+	__u32 size;
+	__u32 offset;
+	void __user *data;
+};
+
+#define MPP_BAT_MSG_DONE		(0x00000001)
+
+struct mpp_bat_msg {
+	__u64 flag;
+	__u32 fd;
+	__s32 ret;
+};
+
+#endif /* _UAPI_RKVENC_H */
